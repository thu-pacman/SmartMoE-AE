--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
using world size: 32, data-parallel-size: 32, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 32
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-1.3B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-1-gpus-32-mp-1-pp-1-ep-32-mlc-0.01-cap-1.2-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 32
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.2
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [32]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 8
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /GPUFS/thu_wgchen_2/zms/Auto-Megatron/deepspeed/output/tensorboard/gpt-1.3B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-1-gpus-32-mp-1-pp-1-ep-32-mlc-0.01-cap-1.2-drop-true_ln101_2023.01.05-05.28.08
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 200
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 32
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2023-01-05 05:28:27,176] [INFO] [comm.py:656:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: è¿›å…¥ç›®å½•â€œ/GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/dataâ€
make: å¯¹â€œdefaultâ€æ— éœ€åšä»»ä½•äº‹ã€‚
make: ç¦»å¼€ç›®å½•â€œ/GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/dataâ€
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.249 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
> setting tensorboard ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 10.942 seconds
time to initialize megatron (seconds): 9.237
[after megatron is initialized] datetime: 2023-01-05 05:28:41 
building GPT model ...
[2023-01-05 05:28:41,329] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2023-01-05 05:28:41,329] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2023-01-05 05:28:41,330] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 14.35 GB, percent = 5.7%
[2023-01-05 05:28:41,363] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2023-01-05 05:28:41,369] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2023-01-05 05:28:41,374] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2023-01-05 05:28:41,379] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2023-01-05 05:28:41,411] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2023-01-05 05:28:41,411] [INFO] [utils.py:832:see_memory_usage] MA 0.94 GB         Max_MA 1.0 GB         CA 1.03 GB         Max_CA 1 GB 
[2023-01-05 05:28:41,412] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 14.35 GB, percent = 5.7%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 508252160
> learning rate decay style: cosine
DeepSpeed is enabled.
[2023-01-05 05:28:41,414] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_32
[2023-01-05 05:28:41,422] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 32
[2023-01-05 05:28:41,527] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [0]
[2023-01-05 05:28:41,538] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [1]
[2023-01-05 05:28:41,549] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [2]
[2023-01-05 05:28:41,560] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [3]
[2023-01-05 05:28:41,571] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [4]
[2023-01-05 05:28:41,582] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [5]
[2023-01-05 05:28:41,593] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [6]
[2023-01-05 05:28:41,604] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [7]
[2023-01-05 05:28:41,615] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [8]
[2023-01-05 05:28:41,616] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [9]
[2023-01-05 05:28:41,627] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [10]
[2023-01-05 05:28:41,638] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [11]
[2023-01-05 05:28:41,649] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [12]
[2023-01-05 05:28:41,660] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [13]
[2023-01-05 05:28:41,671] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [14]
[2023-01-05 05:28:41,683] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [15]
[2023-01-05 05:28:41,694] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [16]
[2023-01-05 05:28:41,705] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [17]
[2023-01-05 05:28:41,716] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [18]
[2023-01-05 05:28:41,727] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [19]
[2023-01-05 05:28:41,738] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [20]
[2023-01-05 05:28:41,749] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [21]
[2023-01-05 05:28:41,760] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [22]
[2023-01-05 05:28:41,771] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [23]
[2023-01-05 05:28:41,782] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [24]
[2023-01-05 05:28:41,783] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [25]
[2023-01-05 05:28:41,794] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [26]
[2023-01-05 05:28:41,805] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [27]
[2023-01-05 05:28:41,816] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [28]
[2023-01-05 05:28:41,827] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [29]
[2023-01-05 05:28:41,838] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [30]
[2023-01-05 05:28:41,850] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [31]
[2023-01-05 05:28:41,861] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_32 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[2023-01-05 05:28:42,171] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-01-05 05:28:42,172] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-01-05 05:28:42,172] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-01-05 05:28:42,174] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-01-05 05:28:42,174] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-01-05 05:28:42,306] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-01-05 05:28:42,306] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-01-05 05:28:42,307] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x2b24de5ddc18>
[2023-01-05 05:28:42,307] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:28:42,308] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2023-01-05 05:28:42,310] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-01-05 05:28:42,310] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-01-05 05:28:42,310] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2023-01-05 05:28:42,310] [INFO] [config.py:1024:print]   amp_params ................... False
[2023-01-05 05:28:42,311] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2b24dd97a7f0>
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 424592, 'difficulty_step': 8}}
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2023-01-05 05:28:42,312] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   dump_state ................... False
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2023-01-05 05:28:42,313] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   global_rank .................. 0
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2023-01-05 05:28:42,314] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x2b24dd97ab38>
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   pld_params ................... False
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2023-01-05 05:28:42,315] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   train_batch_size ............. 256
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  1
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   world_size ................... 32
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2023-01-05 05:28:42,316] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2023-01-05 05:28:42,316] [INFO] [config.py:1016:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 1, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 4.245920e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Emitting ninja build file /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4066586494445801 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-05 05:28:42 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: 25600000
    test:       25600000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.060462 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.055 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-05 05:28:45 
done with setup ...
training ...
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40654921531677246 seconds
time (ms) | model-and-optimizer-setup: 1522.07 | train/valid/test-data-iterators-setup: 2251.31
[before the start of training step] datetime: 2023-01-05 05:28:45 
[2023-01-05 05:28:56,687] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 6787.904296875 | max allocated: 10609.82421875 | reserved: 12316.0 | max reserved: 12316.0
 iteration        1/     200 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 11590.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.093235E+01 | moe loss: 6.177481E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.087 | TFLOPs: 2.18 |
time (ms) | forward-compute: 10000.25 | backward-compute: 1445.57 | backward-embedding-all-reduce: 0.02 | optimizer: 124.82 | batch-generator: 4769.89
[2023-01-05 05:29:02,472] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 5785.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.556746E+00 | moe loss: 8.636104E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 44.246 | TFLOPs: 4.36 |
time (ms) | forward-compute: 4975.56 | backward-compute: 729.69 | backward-embedding-all-reduce: 0.01 | optimizer: 54.70 | batch-generator: 3403.70
[2023-01-05 05:29:08,697] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:29:08,723] [INFO] [timer.py:207:stop] 0/3, RunningAvgSamplesPerSec=78.43320734252903, CurrSamplesPerSec=78.43320734252903, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration        3/     200 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 6224.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.010596E+01 | moe loss: 8.773109E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 41.127 | TFLOPs: 4.05 |
time (ms) | forward-compute: 5418.19 | backward-compute: 724.60 | backward-embedding-all-reduce: 0.01 | optimizer: 57.56 | batch-generator: 3672.80
[2023-01-05 05:29:15,109] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:29:15,136] [INFO] [timer.py:207:stop] 0/4, RunningAvgSamplesPerSec=70.53356851927711, CurrSamplesPerSec=64.07959716349167, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration        4/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 6413.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.604538E+00 | moe loss: 8.339281E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 39.914 | TFLOPs: 3.93 |
time (ms) | forward-compute: 5607.75 | backward-compute: 725.37 | backward-embedding-all-reduce: 0.01 | optimizer: 55.19 | batch-generator: 3882.22
[2023-01-05 05:29:22,897] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:29:22,924] [INFO] [timer.py:207:stop] 0/5, RunningAvgSamplesPerSec=61.73441028734139, CurrSamplesPerSec=49.40718400419942, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration        5/     200 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 7788.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.248882E+00 | moe loss: 7.230515E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.870 | TFLOPs: 3.24 |
time (ms) | forward-compute: 6909.20 | backward-compute: 799.31 | backward-embedding-all-reduce: 0.01 | optimizer: 53.76 | batch-generator: 5039.92
[2023-01-05 05:29:30,937] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:29:30,963] [INFO] [timer.py:207:stop] 0/6, RunningAvgSamplesPerSec=59.20796775753525, CurrSamplesPerSec=52.733687228385676, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration        6/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 8042.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.856219E+00 | moe loss: 7.175751E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.832 | TFLOPs: 3.14 |
time (ms) | forward-compute: 7232.25 | backward-compute: 724.03 | backward-embedding-all-reduce: 0.01 | optimizer: 55.85 | batch-generator: 4583.33
[2023-01-05 05:29:37,794] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:29:37,821] [INFO] [timer.py:207:stop] 0/7, RunningAvgSamplesPerSec=58.900827530700525, CurrSamplesPerSec=57.70348453776671, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration        7/     200 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 6852.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.568626E+00 | moe loss: 6.865977E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 37.359 | TFLOPs: 3.68 |
time (ms) | forward-compute: 6047.43 | backward-compute: 726.51 | backward-embedding-all-reduce: 0.01 | optimizer: 52.92 | batch-generator: 4192.06
[2023-01-05 05:29:44,743] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:29:44,770] [INFO] [timer.py:207:stop] 0/8, RunningAvgSamplesPerSec=59.10290730722076, CurrSamplesPerSec=60.134468359648025, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration        8/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 6950.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.298789E+00 | moe loss: 6.363283E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 36.832 | TFLOPs: 3.63 |
time (ms) | forward-compute: 6148.72 | backward-compute: 723.00 | backward-embedding-all-reduce: 0.01 | optimizer: 54.01 | batch-generator: 4360.76
[2023-01-05 05:29:51,037] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:29:51,064] [INFO] [timer.py:207:stop] 0/9, RunningAvgSamplesPerSec=58.05990441959568, CurrSamplesPerSec=52.500927250445244, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration        9/     200 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 6295.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.266798E+00 | moe loss: 6.122668E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 40.665 | TFLOPs: 4.01 |
time (ms) | forward-compute: 5486.94 | backward-compute: 725.54 | backward-embedding-all-reduce: 0.01 | optimizer: 56.36 | batch-generator: 3426.29
[2023-01-05 05:29:57,412] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:29:57,439] [INFO] [timer.py:207:stop] 0/10, RunningAvgSamplesPerSec=57.24838438364147, CurrSamplesPerSec=52.14633390147976, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       10/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 6375.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.176681E+00 | moe loss: 6.096414E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 40.154 | TFLOPs: 3.96 |
time (ms) | forward-compute: 5574.99 | backward-compute: 719.30 | backward-embedding-all-reduce: 0.01 | optimizer: 54.11 | batch-generator: 4313.68
[2023-01-05 05:30:04,412] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:04,439] [INFO] [timer.py:207:stop] 0/11, RunningAvgSamplesPerSec=58.09135575529563, CurrSamplesPerSec=65.84816790144292, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       11/     200 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 6998.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.990498E+00 | moe loss: 5.839959E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 36.579 | TFLOPs: 3.61 |
time (ms) | forward-compute: 6197.12 | backward-compute: 722.31 | backward-embedding-all-reduce: 0.01 | optimizer: 53.59 | batch-generator: 3885.11
[2023-01-05 05:30:11,326] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:11,353] [INFO] [timer.py:207:stop] 0/12, RunningAvgSamplesPerSec=55.85121868989349, CurrSamplesPerSec=41.461541930864136, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       12/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 6912.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.842208E+00 | moe loss: 5.957554E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 37.033 | TFLOPs: 3.65 |
time (ms) | forward-compute: 6086.02 | backward-compute: 745.57 | backward-embedding-all-reduce: 0.01 | optimizer: 55.24 | batch-generator: 3628.62
[2023-01-05 05:30:17,807] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:17,833] [INFO] [timer.py:207:stop] 0/13, RunningAvgSamplesPerSec=54.502562504007685, CurrSamplesPerSec=43.90152833678688, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       13/     200 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 6481.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.767994E+00 | moe loss: 5.579990E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 39.496 | TFLOPs: 3.89 |
time (ms) | forward-compute: 5673.22 | backward-compute: 729.86 | backward-embedding-all-reduce: 0.01 | optimizer: 53.20 | batch-generator: 3223.05
[2023-01-05 05:30:25,935] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:25,962] [INFO] [timer.py:207:stop] 0/14, RunningAvgSamplesPerSec=56.04969612263824, CurrSamplesPerSec=81.49729248563057, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       14/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 8130.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.694166E+00 | moe loss: 5.595566E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.488 | TFLOPs: 3.10 |
time (ms) | forward-compute: 7331.02 | backward-compute: 719.47 | backward-embedding-all-reduce: 0.01 | optimizer: 53.07 | batch-generator: 4868.85
[2023-01-05 05:30:32,488] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:32,515] [INFO] [timer.py:207:stop] 0/15, RunningAvgSamplesPerSec=55.13739120762303, CurrSamplesPerSec=46.12770186881873, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       15/     200 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 6551.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.623111E+00 | moe loss: 5.388168E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 39.076 | TFLOPs: 3.85 |
time (ms) | forward-compute: 5750.21 | backward-compute: 722.71 | backward-embedding-all-reduce: 0.01 | optimizer: 52.68 | batch-generator: 4014.55
[2023-01-05 05:30:39,466] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:39,493] [INFO] [timer.py:207:stop] 0/16, RunningAvgSamplesPerSec=56.03350544730707, CurrSamplesPerSec=71.04368980345411, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       16/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 6981.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.559436E+00 | moe loss: 5.467414E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 36.667 | TFLOPs: 3.61 |
time (ms) | forward-compute: 6169.10 | backward-compute: 724.83 | backward-embedding-all-reduce: 0.01 | optimizer: 59.39 | batch-generator: 3867.74
[2023-01-05 05:30:45,006] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:45,033] [INFO] [timer.py:207:stop] 0/17, RunningAvgSamplesPerSec=56.56296978570945, CurrSamplesPerSec=65.18626043473785, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       17/     200 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 5538.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.490608E+00 | moe loss: 5.289654E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 46.220 | TFLOPs: 4.56 |
time (ms) | forward-compute: 4733.84 | backward-compute: 720.81 | backward-embedding-all-reduce: 0.01 | optimizer: 54.68 | batch-generator: 3356.12
[2023-01-05 05:30:51,450] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:51,477] [INFO] [timer.py:207:stop] 0/18, RunningAvgSamplesPerSec=55.00925233211548, CurrSamplesPerSec=38.957507095309055, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       18/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 6443.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.441587E+00 | moe loss: 5.027266E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 39.732 | TFLOPs: 3.92 |
time (ms) | forward-compute: 5639.95 | backward-compute: 720.25 | backward-embedding-all-reduce: 0.01 | optimizer: 53.39 | batch-generator: 3124.57
[2023-01-05 05:30:59,492] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:30:59,519] [INFO] [timer.py:207:stop] 0/19, RunningAvgSamplesPerSec=56.14779127218215, CurrSamplesPerSec=83.94746658494024, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       19/     200 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 8040.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.410142E+00 | moe loss: 5.114457E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.841 | TFLOPs: 3.14 |
time (ms) | forward-compute: 7240.85 | backward-compute: 718.36 | backward-embedding-all-reduce: 0.01 | optimizer: 53.74 | batch-generator: 4805.40
[2023-01-05 05:31:05,250] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:05,277] [INFO] [timer.py:207:stop] 0/20, RunningAvgSamplesPerSec=57.016396420902424, CurrSamplesPerSec=77.3617301377514, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       20/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 5751.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.375504E+00 | moe loss: 5.063179E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 44.513 | TFLOPs: 4.39 |
time (ms) | forward-compute: 4963.50 | backward-compute: 716.07 | backward-embedding-all-reduce: 0.01 | optimizer: 53.24 | batch-generator: 3042.28
[2023-01-05 05:31:11,217] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:11,244] [INFO] [timer.py:207:stop] 0/21, RunningAvgSamplesPerSec=57.80541551465333, CurrSamplesPerSec=76.98071721410201, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       21/     200 | consumed samples:         5376 | consumed tokens:      5505024 | elapsed time per iteration (ms): 5973.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.354187E+00 | moe loss: 5.040375E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 42.856 | TFLOPs: 4.23 |
time (ms) | forward-compute: 5174.14 | backward-compute: 719.97 | backward-embedding-all-reduce: 0.01 | optimizer: 53.06 | batch-generator: 3424.07
[2023-01-05 05:31:17,421] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:17,448] [INFO] [timer.py:207:stop] 0/22, RunningAvgSamplesPerSec=58.387437570507274, CurrSamplesPerSec=72.19950682738528, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       22/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 6204.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.325738E+00 | moe loss: 4.870624E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 41.263 | TFLOPs: 4.07 |
time (ms) | forward-compute: 5399.10 | backward-compute: 721.00 | backward-embedding-all-reduce: 0.01 | optimizer: 58.95 | batch-generator: 3262.99
[2023-01-05 05:31:23,122] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:23,149] [INFO] [timer.py:207:stop] 0/23, RunningAvgSamplesPerSec=58.04084319063696, CurrSamplesPerSec=51.881370476828096, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       23/     200 | consumed samples:         5888 | consumed tokens:      6029312 | elapsed time per iteration (ms): 5700.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.294005E+00 | moe loss: 4.839783E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 44.910 | TFLOPs: 4.43 |
time (ms) | forward-compute: 4900.72 | backward-compute: 721.14 | backward-embedding-all-reduce: 0.01 | optimizer: 53.57 | batch-generator: 3316.16
[2023-01-05 05:31:29,746] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:29,773] [INFO] [timer.py:207:stop] 0/24, RunningAvgSamplesPerSec=57.683276366477344, CurrSamplesPerSec=51.07549879558419, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       24/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 6626.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.280046E+00 | moe loss: 4.709582E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.635 | TFLOPs: 3.81 |
time (ms) | forward-compute: 5819.84 | backward-compute: 721.30 | backward-embedding-all-reduce: 0.01 | optimizer: 58.33 | batch-generator: 3098.95
[2023-01-05 05:31:34,664] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:34,691] [INFO] [timer.py:207:stop] 0/25, RunningAvgSamplesPerSec=58.487923373017175, CurrSamplesPerSec=84.38436970309678, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       25/     200 | consumed samples:         6400 | consumed tokens:      6553600 | elapsed time per iteration (ms): 4916.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.259913E+00 | moe loss: 4.668459E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 52.067 | TFLOPs: 5.13 |
time (ms) | forward-compute: 4115.66 | backward-compute: 719.68 | backward-embedding-all-reduce: 0.01 | optimizer: 55.84 | batch-generator: 2591.49
[2023-01-05 05:31:39,413] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:39,440] [INFO] [timer.py:207:stop] 0/26, RunningAvgSamplesPerSec=57.18040003739098, CurrSamplesPerSec=37.76339881969796, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       26/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 4750.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.255847E+00 | moe loss: 4.622635E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 53.886 | TFLOPs: 5.31 |
time (ms) | forward-compute: 3943.68 | backward-compute: 727.24 | backward-embedding-all-reduce: 0.02 | optimizer: 53.22 | batch-generator: 2499.46
[2023-01-05 05:31:45,150] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:45,177] [INFO] [timer.py:207:stop] 0/27, RunningAvgSamplesPerSec=57.20001014633359, CurrSamplesPerSec=57.67472144038846, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       27/     200 | consumed samples:         6912 | consumed tokens:      7077888 | elapsed time per iteration (ms): 5731.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.239575E+00 | moe loss: 4.657816E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 44.662 | TFLOPs: 4.40 |
time (ms) | forward-compute: 4930.85 | backward-compute: 724.65 | backward-embedding-all-reduce: 0.01 | optimizer: 52.98 | batch-generator: 3044.24
[2023-01-05 05:31:49,800] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:49,827] [INFO] [timer.py:207:stop] 0/28, RunningAvgSamplesPerSec=57.34007851818972, CurrSamplesPerSec=61.07927105921222, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       28/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 4653.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.234394E+00 | moe loss: 4.522306E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.010 | TFLOPs: 5.42 |
time (ms) | forward-compute: 3848.51 | backward-compute: 726.15 | backward-embedding-all-reduce: 0.01 | optimizer: 54.31 | batch-generator: 2598.36
[2023-01-05 05:31:54,763] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:54,790] [INFO] [timer.py:207:stop] 0/29, RunningAvgSamplesPerSec=58.10259209088217, CurrSamplesPerSec=88.80798504623426, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       29/     200 | consumed samples:         7424 | consumed tokens:      7602176 | elapsed time per iteration (ms): 4960.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.215879E+00 | moe loss: 4.520994E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 51.612 | TFLOPs: 5.09 |
time (ms) | forward-compute: 4163.07 | backward-compute: 718.28 | backward-embedding-all-reduce: 0.01 | optimizer: 53.78 | batch-generator: 2604.24
[2023-01-05 05:31:59,697] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:31:59,724] [INFO] [timer.py:207:stop] 0/30, RunningAvgSamplesPerSec=58.641468438334265, CurrSamplesPerSec=78.23173469072529, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       30/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 4929.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.189310E+00 | moe loss: 4.484960E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 51.930 | TFLOPs: 5.12 |
time (ms) | forward-compute: 4133.87 | backward-compute: 722.77 | backward-embedding-all-reduce: 0.01 | optimizer: 54.13 | batch-generator: 2701.78
[2023-01-05 05:32:04,334] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:04,361] [INFO] [timer.py:207:stop] 0/31, RunningAvgSamplesPerSec=58.85575933566101, CurrSamplesPerSec=65.5642361602995, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       31/     200 | consumed samples:         7936 | consumed tokens:      8126464 | elapsed time per iteration (ms): 4642.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.178823E+00 | moe loss: 4.429556E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.138 | TFLOPs: 5.44 |
time (ms) | forward-compute: 3846.14 | backward-compute: 719.17 | backward-embedding-all-reduce: 0.01 | optimizer: 53.74 | batch-generator: 2172.47
[2023-01-05 05:32:09,290] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:09,316] [INFO] [timer.py:207:stop] 0/32, RunningAvgSamplesPerSec=58.71951148552729, CurrSamplesPerSec=55.0254645696343, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       32/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 4955.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.149850E+00 | moe loss: 4.400143E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 51.660 | TFLOPs: 5.09 |
time (ms) | forward-compute: 4156.84 | backward-compute: 719.79 | backward-embedding-all-reduce: 0.01 | optimizer: 54.24 | batch-generator: 2638.62
[2023-01-05 05:32:13,816] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:13,843] [INFO] [timer.py:207:stop] 0/33, RunningAvgSamplesPerSec=59.29684175218648, CurrSamplesPerSec=84.10422533446126, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       33/     200 | consumed samples:         8448 | consumed tokens:      8650752 | elapsed time per iteration (ms): 4527.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.146419E+00 | moe loss: 4.466808E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.538 | TFLOPs: 5.57 |
time (ms) | forward-compute: 3724.32 | backward-compute: 724.83 | backward-embedding-all-reduce: 0.01 | optimizer: 53.80 | batch-generator: 2176.16
[2023-01-05 05:32:18,190] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:18,217] [INFO] [timer.py:207:stop] 0/34, RunningAvgSamplesPerSec=59.56112141562568, CurrSamplesPerSec=69.10955094508473, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       34/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 4373.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.128359E+00 | moe loss: 4.405273E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.534 | TFLOPs: 5.77 |
time (ms) | forward-compute: 3575.74 | backward-compute: 720.48 | backward-embedding-all-reduce: 0.01 | optimizer: 53.29 | batch-generator: 1966.77
[2023-01-05 05:32:24,083] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:24,109] [INFO] [timer.py:207:stop] 0/35, RunningAvgSamplesPerSec=60.00213560206078, CurrSamplesPerSec=78.63364724051101, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       35/     200 | consumed samples:         8960 | consumed tokens:      9175040 | elapsed time per iteration (ms): 5891.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.108449E+00 | moe loss: 4.246927E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 43.452 | TFLOPs: 4.28 |
time (ms) | forward-compute: 5088.24 | backward-compute: 724.91 | backward-embedding-all-reduce: 0.01 | optimizer: 53.70 | batch-generator: 2283.40
[2023-01-05 05:32:28,816] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:28,843] [INFO] [timer.py:207:stop] 0/36, RunningAvgSamplesPerSec=60.5666029497115, CurrSamplesPerSec=87.83445849551985, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       36/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 4733.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.085634E+00 | moe loss: 4.251869E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.085 | TFLOPs: 5.33 |
time (ms) | forward-compute: 3930.90 | backward-compute: 722.72 | backward-embedding-all-reduce: 0.01 | optimizer: 53.73 | batch-generator: 2267.37
[2023-01-05 05:32:33,007] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:33,034] [INFO] [timer.py:207:stop] 0/37, RunningAvgSamplesPerSec=60.53918377592001, CurrSamplesPerSec=59.621479376891244, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       37/     200 | consumed samples:         9472 | consumed tokens:      9699328 | elapsed time per iteration (ms): 4192.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.086021E+00 | moe loss: 4.247390E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.063 | TFLOPs: 6.02 |
time (ms) | forward-compute: 3392.03 | backward-compute: 721.35 | backward-embedding-all-reduce: 0.01 | optimizer: 52.87 | batch-generator: 2105.09
[2023-01-05 05:32:37,678] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:37,705] [INFO] [timer.py:207:stop] 0/38, RunningAvgSamplesPerSec=60.475184756499004, CurrSamplesPerSec=58.317424743743544, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       38/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 4669.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.075389E+00 | moe loss: 4.282045E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.821 | TFLOPs: 5.40 |
time (ms) | forward-compute: 3868.43 | backward-compute: 723.51 | backward-embedding-all-reduce: 0.01 | optimizer: 53.72 | batch-generator: 2104.29
[2023-01-05 05:32:42,529] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:42,556] [INFO] [timer.py:207:stop] 0/39, RunningAvgSamplesPerSec=60.35711412090313, CurrSamplesPerSec=56.39345838254504, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       39/     200 | consumed samples:         9984 | consumed tokens:     10223616 | elapsed time per iteration (ms): 4852.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.072628E+00 | moe loss: 4.287828E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 52.754 | TFLOPs: 5.20 |
time (ms) | forward-compute: 4043.22 | backward-compute: 723.09 | backward-embedding-all-reduce: 0.01 | optimizer: 58.71 | batch-generator: 2311.61
[2023-01-05 05:32:46,748] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:46,775] [INFO] [timer.py:207:stop] 0/40, RunningAvgSamplesPerSec=60.83620100791111, CurrSamplesPerSec=86.1323027914218, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       40/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 4219.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.032098E+00 | moe loss: 4.270916E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 60.675 | TFLOPs: 5.98 |
time (ms) | forward-compute: 3408.60 | backward-compute: 727.54 | backward-embedding-all-reduce: 0.03 | optimizer: 53.12 | batch-generator: 2129.55
[2023-01-05 05:32:51,145] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:51,172] [INFO] [timer.py:207:stop] 0/41, RunningAvgSamplesPerSec=60.90579465513277, CurrSamplesPerSec=63.673699736087926, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       41/     200 | consumed samples:        10496 | consumed tokens:     10747904 | elapsed time per iteration (ms): 4394.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.018394E+00 | moe loss: 4.260577E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.256 | TFLOPs: 5.74 |
time (ms) | forward-compute: 3586.74 | backward-compute: 727.93 | backward-embedding-all-reduce: 0.01 | optimizer: 52.85 | batch-generator: 1856.54
[2023-01-05 05:32:56,039] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:56,066] [INFO] [timer.py:207:stop] 0/42, RunningAvgSamplesPerSec=59.39966838606804, CurrSamplesPerSec=30.23772408049021, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       42/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 4894.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.033540E+00 | moe loss: 4.151121E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 52.308 | TFLOPs: 5.16 |
time (ms) | forward-compute: 4095.65 | backward-compute: 721.62 | backward-embedding-all-reduce: 0.01 | optimizer: 52.60 | batch-generator: 1997.61
[2023-01-05 05:32:59,533] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:32:59,560] [INFO] [timer.py:207:stop] 0/43, RunningAvgSamplesPerSec=58.92727911938023, CurrSamplesPerSec=44.7059218529123, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       43/     200 | consumed samples:        11008 | consumed tokens:     11272192 | elapsed time per iteration (ms): 3496.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.995998E+00 | moe loss: 4.200725E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 73.208 | TFLOPs: 7.22 |
time (ms) | forward-compute: 2686.74 | backward-compute: 727.11 | backward-embedding-all-reduce: 0.01 | optimizer: 55.61 | batch-generator: 190.22
[2023-01-05 05:33:01,323] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:01,350] [INFO] [timer.py:207:stop] 0/44, RunningAvgSamplesPerSec=59.02952768269337, CurrSamplesPerSec=63.550632321029404, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       44/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 1788.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.984436E+00 | moe loss: 4.153793E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 143.126 | TFLOPs: 14.11 |
time (ms) | forward-compute: 986.42 | backward-compute: 720.99 | backward-embedding-all-reduce: 0.01 | optimizer: 54.02 | batch-generator: 225.40
[2023-01-05 05:33:03,248] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:03,275] [INFO] [timer.py:207:stop] 0/45, RunningAvgSamplesPerSec=59.420933832936164, CurrSamplesPerSec=82.35621044570792, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       45/     200 | consumed samples:        11520 | consumed tokens:     11796480 | elapsed time per iteration (ms): 1924.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.970892E+00 | moe loss: 4.181653E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 133.041 | TFLOPs: 13.12 |
time (ms) | forward-compute: 1125.77 | backward-compute: 720.21 | backward-embedding-all-reduce: 0.01 | optimizer: 53.55 | batch-generator: 129.28
[2023-01-05 05:33:04,723] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:04,750] [INFO] [timer.py:207:stop] 0/46, RunningAvgSamplesPerSec=59.70560990776778, CurrSamplesPerSec=75.19655418927307, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       46/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 1478.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.952073E+00 | moe loss: 4.227897E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 173.170 | TFLOPs: 17.07 |
time (ms) | forward-compute: 674.44 | backward-compute: 721.70 | backward-embedding-all-reduce: 0.01 | optimizer: 54.28 | batch-generator: 127.49
[2023-01-05 05:33:06,512] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:06,539] [INFO] [timer.py:207:stop] 0/47, RunningAvgSamplesPerSec=59.93885674631742, CurrSamplesPerSec=72.38040098407086, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       47/     200 | consumed samples:        12032 | consumed tokens:     12320768 | elapsed time per iteration (ms): 1786.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.936277E+00 | moe loss: 4.168020E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 143.267 | TFLOPs: 14.12 |
time (ms) | forward-compute: 977.36 | backward-compute: 727.75 | backward-embedding-all-reduce: 0.01 | optimizer: 53.63 | batch-generator: 135.71
[2023-01-05 05:33:08,231] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:08,258] [INFO] [timer.py:207:stop] 0/48, RunningAvgSamplesPerSec=60.24637904703719, CurrSamplesPerSec=78.33126714729177, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       48/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 1714.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.920592E+00 | moe loss: 4.126063E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.362 | TFLOPs: 14.73 |
time (ms) | forward-compute: 909.16 | backward-compute: 720.17 | backward-embedding-all-reduce: 0.01 | optimizer: 64.32 | batch-generator: 128.80
[2023-01-05 05:33:10,025] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:10,052] [INFO] [timer.py:207:stop] 0/49, RunningAvgSamplesPerSec=60.41223636595411, CurrSamplesPerSec=69.17198770529822, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       49/     200 | consumed samples:        12544 | consumed tokens:     12845056 | elapsed time per iteration (ms): 1801.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.913174E+00 | moe loss: 4.045043E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 142.142 | TFLOPs: 14.01 |
time (ms) | forward-compute: 993.17 | backward-compute: 720.21 | backward-embedding-all-reduce: 0.01 | optimizer: 57.93 | batch-generator: 436.58
[2023-01-05 05:33:12,245] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:12,272] [INFO] [timer.py:207:stop] 0/50, RunningAvgSamplesPerSec=59.80765844267116, CurrSamplesPerSec=40.67567494669978, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       50/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 2218.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.885045E+00 | moe loss: 4.040965E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 115.380 | TFLOPs: 11.37 |
time (ms) | forward-compute: 1325.97 | backward-compute: 810.45 | backward-embedding-all-reduce: 0.01 | optimizer: 54.06 | batch-generator: 153.32
[2023-01-05 05:33:15,516] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:15,543] [INFO] [timer.py:207:stop] 0/51, RunningAvgSamplesPerSec=59.03313583658647, CurrSamplesPerSec=36.404010530272906, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       51/     200 | consumed samples:        13056 | consumed tokens:     13369344 | elapsed time per iteration (ms): 3272.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.876609E+00 | moe loss: 4.091070E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.230 | TFLOPs: 7.71 |
time (ms) | forward-compute: 2455.37 | backward-compute: 726.61 | backward-embedding-all-reduce: 0.01 | optimizer: 59.93 | batch-generator: 141.05
[2023-01-05 05:33:17,566] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:17,593] [INFO] [timer.py:207:stop] 0/52, RunningAvgSamplesPerSec=59.41040018127782, CurrSamplesPerSec=86.49632278071503, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       52/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 2042.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.853360E+00 | moe loss: 4.111008E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 125.353 | TFLOPs: 12.36 |
time (ms) | forward-compute: 1244.56 | backward-compute: 719.34 | backward-embedding-all-reduce: 0.03 | optimizer: 54.80 | batch-generator: 236.65
[2023-01-05 05:33:19,626] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:19,653] [INFO] [timer.py:207:stop] 0/53, RunningAvgSamplesPerSec=59.6903167672886, CurrSamplesPerSec=78.08563501358174, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       53/     200 | consumed samples:        13568 | consumed tokens:     13893632 | elapsed time per iteration (ms): 2059.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.857147E+00 | moe loss: 4.077799E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 124.316 | TFLOPs: 12.26 |
time (ms) | forward-compute: 1249.86 | backward-compute: 736.26 | backward-embedding-all-reduce: 0.01 | optimizer: 53.17 | batch-generator: 134.66
[2023-01-05 05:33:21,208] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:21,235] [INFO] [timer.py:207:stop] 0/54, RunningAvgSamplesPerSec=60.04714120734529, CurrSamplesPerSec=86.38314682744767, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       54/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 1586.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.834848E+00 | moe loss: 4.082250E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 161.401 | TFLOPs: 15.91 |
time (ms) | forward-compute: 785.83 | backward-compute: 723.78 | backward-embedding-all-reduce: 0.01 | optimizer: 53.16 | batch-generator: 132.15
[2023-01-05 05:33:23,072] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:23,099] [INFO] [timer.py:207:stop] 0/55, RunningAvgSamplesPerSec=60.21292991727127, CurrSamplesPerSec=70.3069514542764, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       55/     200 | consumed samples:        14080 | consumed tokens:     14417920 | elapsed time per iteration (ms): 1865.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.824961E+00 | moe loss: 4.060562E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 137.254 | TFLOPs: 13.53 |
time (ms) | forward-compute: 1065.26 | backward-compute: 721.19 | backward-embedding-all-reduce: 0.01 | optimizer: 53.43 | batch-generator: 340.20
[2023-01-05 05:33:24,945] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:24,972] [INFO] [timer.py:207:stop] 0/56, RunningAvgSamplesPerSec=60.43652804669005, CurrSamplesPerSec=75.24592018594836, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       56/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 1874.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.842299E+00 | moe loss: 5.757244E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 136.563 | TFLOPs: 13.46 |
time (ms) | forward-compute: 1069.91 | backward-compute: 723.17 | backward-embedding-all-reduce: 0.01 | optimizer: 54.49 | batch-generator: 193.18
[2023-01-05 05:33:26,816] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:26,843] [INFO] [timer.py:207:stop] 0/57, RunningAvgSamplesPerSec=60.68438173232481, CurrSamplesPerSec=77.94610001568007, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       57/     200 | consumed samples:        14592 | consumed tokens:     14942208 | elapsed time per iteration (ms): 1871.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.801805E+00 | moe loss: 4.111475E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 136.800 | TFLOPs: 13.49 |
time (ms) | forward-compute: 1059.91 | backward-compute: 726.98 | backward-embedding-all-reduce: 0.01 | optimizer: 56.76 | batch-generator: 214.81
[2023-01-05 05:33:29,816] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:29,843] [INFO] [timer.py:207:stop] 0/58, RunningAvgSamplesPerSec=60.88731744520776, CurrSamplesPerSec=74.61008759690526, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       58/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 2992.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.794318E+00 | moe loss: 4.080208E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 85.540 | TFLOPs: 8.43 |
time (ms) | forward-compute: 2184.81 | backward-compute: 729.87 | backward-embedding-all-reduce: 0.01 | optimizer: 56.86 | batch-generator: 702.34
[2023-01-05 05:33:31,518] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:31,545] [INFO] [timer.py:207:stop] 0/59, RunningAvgSamplesPerSec=61.030272422284234, CurrSamplesPerSec=70.26929585015665, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       59/     200 | consumed samples:        15104 | consumed tokens:     15466496 | elapsed time per iteration (ms): 1701.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.777614E+00 | moe loss: 4.151731E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 150.497 | TFLOPs: 14.84 |
time (ms) | forward-compute: 895.68 | backward-compute: 724.67 | backward-embedding-all-reduce: 0.01 | optimizer: 59.99 | batch-generator: 132.84
[2023-01-05 05:33:33,158] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:33,185] [INFO] [timer.py:207:stop] 0/60, RunningAvgSamplesPerSec=60.968071058786535, CurrSamplesPerSec=57.62066995144104, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       60/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 1646.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.767134E+00 | moe loss: 4.219985E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 155.485 | TFLOPs: 15.33 |
time (ms) | forward-compute: 842.14 | backward-compute: 724.40 | backward-embedding-all-reduce: 0.02 | optimizer: 54.12 | batch-generator: 156.82
[2023-01-05 05:33:34,840] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:34,867] [INFO] [timer.py:207:stop] 0/61, RunningAvgSamplesPerSec=61.14388428038063, CurrSamplesPerSec=73.42442959981312, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       61/     200 | consumed samples:        15616 | consumed tokens:     15990784 | elapsed time per iteration (ms): 1679.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.769290E+00 | moe loss: 4.102383E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 152.392 | TFLOPs: 15.02 |
time (ms) | forward-compute: 876.95 | backward-compute: 724.88 | backward-embedding-all-reduce: 0.01 | optimizer: 54.03 | batch-generator: 136.02
[2023-01-05 05:33:36,419] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:36,446] [INFO] [timer.py:207:stop] 0/62, RunningAvgSamplesPerSec=61.229462526736164, CurrSamplesPerSec=66.74075536379101, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       62/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 1573.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.746953E+00 | moe loss: 4.098157E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 162.683 | TFLOPs: 16.04 |
time (ms) | forward-compute: 771.29 | backward-compute: 730.52 | backward-embedding-all-reduce: 0.01 | optimizer: 54.08 | batch-generator: 131.56
[2023-01-05 05:33:37,863] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:37,890] [INFO] [timer.py:207:stop] 0/63, RunningAvgSamplesPerSec=61.48320192398335, CurrSamplesPerSec=81.82969964784954, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       63/     200 | consumed samples:        16128 | consumed tokens:     16515072 | elapsed time per iteration (ms): 1445.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.718298E+00 | moe loss: 4.069164E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 177.049 | TFLOPs: 17.45 |
time (ms) | forward-compute: 646.66 | backward-compute: 720.33 | backward-embedding-all-reduce: 0.01 | optimizer: 57.30 | batch-generator: 127.00
[2023-01-05 05:33:39,503] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:39,530] [INFO] [timer.py:207:stop] 0/64, RunningAvgSamplesPerSec=61.77131890891418, CurrSamplesPerSec=86.4966014934482, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       64/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 1646.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.713225E+00 | moe loss: 4.007529E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 155.505 | TFLOPs: 15.33 |
time (ms) | forward-compute: 843.02 | backward-compute: 724.25 | backward-embedding-all-reduce: 0.01 | optimizer: 53.42 | batch-generator: 206.44
[2023-01-05 05:33:41,439] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:41,466] [INFO] [timer.py:207:stop] 0/65, RunningAvgSamplesPerSec=61.69693742885616, CurrSamplesPerSec=57.41082628589467, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       65/     200 | consumed samples:        16640 | consumed tokens:     17039360 | elapsed time per iteration (ms): 1935.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.711094E+00 | moe loss: 4.050191E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 132.267 | TFLOPs: 13.04 |
time (ms) | forward-compute: 1133.59 | backward-compute: 721.69 | backward-embedding-all-reduce: 0.01 | optimizer: 53.93 | batch-generator: 195.49
[2023-01-05 05:33:42,940] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:42,967] [INFO] [timer.py:207:stop] 0/66, RunningAvgSamplesPerSec=61.9791155342586, CurrSamplesPerSec=87.06617192973351, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       66/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 1502.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.682030E+00 | moe loss: 4.069914E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 170.430 | TFLOPs: 16.80 |
time (ms) | forward-compute: 694.34 | backward-compute: 726.63 | backward-embedding-all-reduce: 0.01 | optimizer: 54.24 | batch-generator: 130.50
[2023-01-05 05:33:44,448] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:44,475] [INFO] [timer.py:207:stop] 0/67, RunningAvgSamplesPerSec=62.225082896678245, CurrSamplesPerSec=83.41022466826753, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       67/     200 | consumed samples:        17152 | consumed tokens:     17563648 | elapsed time per iteration (ms): 1506.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.677104E+00 | moe loss: 4.065681E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 169.895 | TFLOPs: 16.75 |
time (ms) | forward-compute: 691.49 | backward-compute: 726.95 | backward-embedding-all-reduce: 0.01 | optimizer: 62.17 | batch-generator: 132.32
[2023-01-05 05:33:46,290] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:46,317] [INFO] [timer.py:207:stop] 0/68, RunningAvgSamplesPerSec=62.29557952985821, CurrSamplesPerSec=67.24773056361386, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       68/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 1841.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.660941E+00 | moe loss: 4.047054E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 139.040 | TFLOPs: 13.71 |
time (ms) | forward-compute: 1040.51 | backward-compute: 721.61 | backward-embedding-all-reduce: 0.01 | optimizer: 53.54 | batch-generator: 254.97
[2023-01-05 05:33:47,878] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:47,904] [INFO] [timer.py:207:stop] 0/69, RunningAvgSamplesPerSec=62.36929426314278, CurrSamplesPerSec=67.6528606525068, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       69/     200 | consumed samples:        17664 | consumed tokens:     18087936 | elapsed time per iteration (ms): 1588.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.645988E+00 | moe loss: 4.082489E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 161.209 | TFLOPs: 15.89 |
time (ms) | forward-compute: 784.86 | backward-compute: 723.67 | backward-embedding-all-reduce: 0.01 | optimizer: 53.74 | batch-generator: 127.20
[2023-01-05 05:33:49,389] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:49,416] [INFO] [timer.py:207:stop] 0/70, RunningAvgSamplesPerSec=62.58190353344258, CurrSamplesPerSec=81.10611714358056, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       70/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 1511.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.649876E+00 | moe loss: 4.280146E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 169.340 | TFLOPs: 16.69 |
time (ms) | forward-compute: 703.08 | backward-compute: 726.82 | backward-embedding-all-reduce: 0.01 | optimizer: 54.15 | batch-generator: 131.19
[2023-01-05 05:33:51,109] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:51,136] [INFO] [timer.py:207:stop] 0/71, RunningAvgSamplesPerSec=62.607121604750155, CurrSamplesPerSec=64.37097338885197, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       71/     200 | consumed samples:        18176 | consumed tokens:     18612224 | elapsed time per iteration (ms): 1720.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.659377E+00 | moe loss: 3.994897E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 148.790 | TFLOPs: 14.67 |
time (ms) | forward-compute: 917.61 | backward-compute: 723.25 | backward-embedding-all-reduce: 0.01 | optimizer: 53.22 | batch-generator: 113.03
[2023-01-05 05:33:52,613] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:52,640] [INFO] [timer.py:207:stop] 0/72, RunningAvgSamplesPerSec=62.800197196251325, CurrSamplesPerSec=79.77573497507784, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       72/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 1502.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.638409E+00 | moe loss: 4.133021E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 170.342 | TFLOPs: 16.79 |
time (ms) | forward-compute: 696.34 | backward-compute: 727.58 | backward-embedding-all-reduce: 0.01 | optimizer: 53.55 | batch-generator: 130.61
[2023-01-05 05:33:54,329] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:54,356] [INFO] [timer.py:207:stop] 0/73, RunningAvgSamplesPerSec=63.05339110640089, CurrSamplesPerSec=87.84520865062842, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       73/     200 | consumed samples:        18688 | consumed tokens:     19136512 | elapsed time per iteration (ms): 1717.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.643449E+00 | moe loss: 4.156530E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.068 | TFLOPs: 14.70 |
time (ms) | forward-compute: 910.23 | backward-compute: 725.55 | backward-embedding-all-reduce: 0.01 | optimizer: 54.75 | batch-generator: 200.00
[2023-01-05 05:33:55,879] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:55,906] [INFO] [timer.py:207:stop] 0/74, RunningAvgSamplesPerSec=63.07949275052737, CurrSamplesPerSec=64.98961756095483, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       74/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 1549.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.609365E+00 | moe loss: 4.100017E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 165.267 | TFLOPs: 16.29 |
time (ms) | forward-compute: 747.71 | backward-compute: 722.01 | backward-embedding-all-reduce: 0.01 | optimizer: 53.45 | batch-generator: 134.21
[2023-01-05 05:33:57,439] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:57,466] [INFO] [timer.py:207:stop] 0/75, RunningAvgSamplesPerSec=63.21793640889529, CurrSamplesPerSec=75.08264041172522, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       75/     200 | consumed samples:        19200 | consumed tokens:     19660800 | elapsed time per iteration (ms): 1558.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.603262E+00 | moe loss: 4.092265E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 164.232 | TFLOPs: 16.19 |
time (ms) | forward-compute: 755.32 | backward-compute: 725.65 | backward-embedding-all-reduce: 0.01 | optimizer: 52.69 | batch-generator: 128.74
[2023-01-05 05:33:59,934] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:33:59,961] [INFO] [timer.py:207:stop] 0/76, RunningAvgSamplesPerSec=63.433651932052825, CurrSamplesPerSec=84.4761990479745, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       76/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 2496.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.582245E+00 | moe loss: 4.106185E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 102.529 | TFLOPs: 10.11 |
time (ms) | forward-compute: 1684.43 | backward-compute: 723.91 | backward-embedding-all-reduce: 0.01 | optimizer: 61.94 | batch-generator: 199.29
[2023-01-05 05:34:01,711] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:01,738] [INFO] [timer.py:207:stop] 0/77, RunningAvgSamplesPerSec=63.66129344407413, CurrSamplesPerSec=86.68010482903284, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       77/     200 | consumed samples:        19712 | consumed tokens:     20185088 | elapsed time per iteration (ms): 1776.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.597703E+00 | moe loss: 4.167607E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 144.131 | TFLOPs: 14.21 |
time (ms) | forward-compute: 970.17 | backward-compute: 724.24 | backward-embedding-all-reduce: 0.01 | optimizer: 53.44 | batch-generator: 131.45
[2023-01-05 05:34:03,111] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:03,138] [INFO] [timer.py:207:stop] 0/78, RunningAvgSamplesPerSec=63.890392617528036, CurrSamplesPerSec=87.50957816435414, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       78/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 1401.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.580999E+00 | moe loss: 4.179669E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 182.633 | TFLOPs: 18.01 |
time (ms) | forward-compute: 599.31 | backward-compute: 721.54 | backward-embedding-all-reduce: 0.01 | optimizer: 52.67 | batch-generator: 128.26
[2023-01-05 05:34:06,577] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:06,604] [INFO] [timer.py:207:stop] 0/79, RunningAvgSamplesPerSec=63.43343481297327, CurrSamplesPerSec=41.09532058995533, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       79/     200 | consumed samples:        20224 | consumed tokens:     20709376 | elapsed time per iteration (ms): 3464.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.556480E+00 | moe loss: 4.141713E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 73.901 | TFLOPs: 7.29 |
time (ms) | forward-compute: 2661.85 | backward-compute: 724.62 | backward-embedding-all-reduce: 0.01 | optimizer: 52.87 | batch-generator: 118.95
[2023-01-05 05:34:09,005] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:09,032] [INFO] [timer.py:207:stop] 0/80, RunningAvgSamplesPerSec=63.64509089876256, CurrSamplesPerSec=85.65075317398782, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       80/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 2427.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.560463E+00 | moe loss: 4.167578E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 105.471 | TFLOPs: 10.40 |
time (ms) | forward-compute: 1615.31 | backward-compute: 727.95 | backward-embedding-all-reduce: 0.01 | optimizer: 58.48 | batch-generator: 114.76
[2023-01-05 05:34:10,791] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:10,818] [INFO] [timer.py:207:stop] 0/81, RunningAvgSamplesPerSec=63.781765699254194, CurrSamplesPerSec=76.61485172817308, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       81/     200 | consumed samples:        20736 | consumed tokens:     21233664 | elapsed time per iteration (ms): 1786.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.548554E+00 | moe loss: 4.120065E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 143.287 | TFLOPs: 14.13 |
time (ms) | forward-compute: 984.32 | backward-compute: 722.28 | backward-embedding-all-reduce: 0.01 | optimizer: 54.54 | batch-generator: 106.64
[2023-01-05 05:34:12,660] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:12,686] [INFO] [timer.py:207:stop] 0/82, RunningAvgSamplesPerSec=63.835318261718264, CurrSamplesPerSec=68.37033078820782, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       82/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 1868.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.519641E+00 | moe loss: 4.144925E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 137.047 | TFLOPs: 13.51 |
time (ms) | forward-compute: 1064.51 | backward-compute: 722.80 | backward-embedding-all-reduce: 0.01 | optimizer: 53.39 | batch-generator: 123.43
[2023-01-05 05:34:14,935] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:14,961] [INFO] [timer.py:207:stop] 0/83, RunningAvgSamplesPerSec=64.04585893603965, CurrSamplesPerSec=87.00166396253847, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       83/     200 | consumed samples:        21248 | consumed tokens:     21757952 | elapsed time per iteration (ms): 2275.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.546851E+00 | moe loss: 4.203196E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 112.497 | TFLOPs: 11.09 |
time (ms) | forward-compute: 1473.44 | backward-compute: 723.41 | backward-embedding-all-reduce: 0.01 | optimizer: 53.67 | batch-generator: 117.12
[2023-01-05 05:34:16,655] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:16,681] [INFO] [timer.py:207:stop] 0/84, RunningAvgSamplesPerSec=64.03032129092335, CurrSamplesPerSec=62.796326291903526, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       84/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 1716.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.502062E+00 | moe loss: 4.164724E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.128 | TFLOPs: 14.70 |
time (ms) | forward-compute: 909.75 | backward-compute: 727.15 | backward-embedding-all-reduce: 0.01 | optimizer: 55.06 | batch-generator: 256.04
[2023-01-05 05:34:18,055] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:18,082] [INFO] [timer.py:207:stop] 0/85, RunningAvgSamplesPerSec=64.22728444262329, CurrSamplesPerSec=85.89285929126632, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       85/     200 | consumed samples:        21760 | consumed tokens:     22282240 | elapsed time per iteration (ms): 1403.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.495755E+00 | moe loss: 4.121471E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 182.362 | TFLOPs: 17.98 |
time (ms) | forward-compute: 600.38 | backward-compute: 724.09 | backward-embedding-all-reduce: 0.01 | optimizer: 53.75 | batch-generator: 118.84
[2023-01-05 05:34:19,627] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:19,651] [INFO] [timer.py:207:stop] 0/86, RunningAvgSamplesPerSec=64.28319770681018, CurrSamplesPerSec=69.28979494065457, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       86/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 1565.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.511536E+00 | moe loss: 4.122686E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 163.561 | TFLOPs: 16.12 |
time (ms) | forward-compute: 758.92 | backward-compute: 728.83 | backward-embedding-all-reduce: 0.01 | optimizer: 53.89 | batch-generator: 150.48
[2023-01-05 05:34:21,082] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:21,109] [INFO] [timer.py:207:stop] 0/87, RunningAvgSamplesPerSec=64.44425916413364, CurrSamplesPerSec=81.62273263815194, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       87/     200 | consumed samples:        22272 | consumed tokens:     22806528 | elapsed time per iteration (ms): 1461.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.487639E+00 | moe loss: 4.077937E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 175.167 | TFLOPs: 17.27 |
time (ms) | forward-compute: 651.25 | backward-compute: 729.39 | backward-embedding-all-reduce: 0.01 | optimizer: 56.01 | batch-generator: 123.41
[2023-01-05 05:34:22,696] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:22,723] [INFO] [timer.py:207:stop] 0/88, RunningAvgSamplesPerSec=64.62583749998342, CurrSamplesPerSec=84.97767129887866, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       88/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 1610.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.480692E+00 | moe loss: 4.045282E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 158.971 | TFLOPs: 15.67 |
time (ms) | forward-compute: 812.38 | backward-compute: 722.66 | backward-embedding-all-reduce: 0.01 | optimizer: 53.94 | batch-generator: 109.85
[2023-01-05 05:34:24,181] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:24,207] [INFO] [timer.py:207:stop] 0/89, RunningAvgSamplesPerSec=64.7915887329462, CurrSamplesPerSec=83.12702634072625, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       89/     200 | consumed samples:        22784 | consumed tokens:     23330816 | elapsed time per iteration (ms): 1487.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.475037E+00 | moe loss: 4.017930E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 172.078 | TFLOPs: 16.96 |
time (ms) | forward-compute: 679.20 | backward-compute: 728.07 | backward-embedding-all-reduce: 0.01 | optimizer: 54.22 | batch-generator: 121.59
[2023-01-05 05:34:25,809] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:25,836] [INFO] [timer.py:207:stop] 0/90, RunningAvgSamplesPerSec=64.93938418119922, CurrSamplesPerSec=81.01774790751917, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       90/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 1630.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.475479E+00 | moe loss: 3.983099E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 157.016 | TFLOPs: 15.48 |
time (ms) | forward-compute: 826.06 | backward-compute: 724.11 | backward-embedding-all-reduce: 0.01 | optimizer: 53.45 | batch-generator: 168.36
[2023-01-05 05:34:27,503] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:27,530] [INFO] [timer.py:207:stop] 0/91, RunningAvgSamplesPerSec=64.88903842983771, CurrSamplesPerSec=60.74478476240476, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       91/     200 | consumed samples:        23296 | consumed tokens:     23855104 | elapsed time per iteration (ms): 1693.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.454087E+00 | moe loss: 4.041830E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 151.210 | TFLOPs: 14.91 |
time (ms) | forward-compute: 877.03 | backward-compute: 731.09 | backward-embedding-all-reduce: 0.01 | optimizer: 58.11 | batch-generator: 108.70
[2023-01-05 05:34:28,926] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:28,953] [INFO] [timer.py:207:stop] 0/92, RunningAvgSamplesPerSec=65.07406802814435, CurrSamplesPerSec=87.20510634758925, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       92/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 1422.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.447141E+00 | moe loss: 4.034738E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 179.960 | TFLOPs: 17.74 |
time (ms) | forward-compute: 618.89 | backward-compute: 723.20 | backward-embedding-all-reduce: 0.01 | optimizer: 53.72 | batch-generator: 111.04
[2023-01-05 05:34:30,680] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:30,707] [INFO] [timer.py:207:stop] 0/93, RunningAvgSamplesPerSec=65.1033974813267, CurrSamplesPerSec=67.85588948736641, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       93/     200 | consumed samples:        23808 | consumed tokens:     24379392 | elapsed time per iteration (ms): 1755.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.432334E+00 | moe loss: 4.048976E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 145.849 | TFLOPs: 14.38 |
time (ms) | forward-compute: 949.36 | backward-compute: 725.85 | backward-embedding-all-reduce: 0.01 | optimizer: 53.44 | batch-generator: 112.03
[2023-01-05 05:34:32,372] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:32,399] [INFO] [timer.py:207:stop] 0/94, RunningAvgSamplesPerSec=65.01464134685148, CurrSamplesPerSec=57.83905628780912, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       94/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 1692.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.425313E+00 | moe loss: 4.032679E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 151.301 | TFLOPs: 14.92 |
time (ms) | forward-compute: 884.24 | backward-compute: 726.82 | backward-embedding-all-reduce: 0.01 | optimizer: 53.52 | batch-generator: 232.06
[2023-01-05 05:34:33,887] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:33,914] [INFO] [timer.py:207:stop] 0/95, RunningAvgSamplesPerSec=65.07108122773515, CurrSamplesPerSec=70.7191471828307, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       95/     200 | consumed samples:        24320 | consumed tokens:     24903680 | elapsed time per iteration (ms): 1513.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.423278E+00 | moe loss: 4.051719E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 169.142 | TFLOPs: 16.68 |
time (ms) | forward-compute: 707.21 | backward-compute: 727.69 | backward-embedding-all-reduce: 0.01 | optimizer: 53.41 | batch-generator: 127.62
[2023-01-05 05:34:35,281] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:35,308] [INFO] [timer.py:207:stop] 0/96, RunningAvgSamplesPerSec=65.245076274575, CurrSamplesPerSec=86.83999676495803, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       96/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 1396.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.418651E+00 | moe loss: 4.335186E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 183.343 | TFLOPs: 18.08 |
time (ms) | forward-compute: 591.88 | backward-compute: 721.80 | backward-embedding-all-reduce: 0.01 | optimizer: 55.16 | batch-generator: 107.65
[2023-01-05 05:34:36,908] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:36,935] [INFO] [timer.py:207:stop] 0/97, RunningAvgSamplesPerSec=65.41722349128247, CurrSamplesPerSec=86.99286713821549, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       97/     200 | consumed samples:        24832 | consumed tokens:     25427968 | elapsed time per iteration (ms): 1622.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.419798E+00 | moe loss: 4.085332E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 157.829 | TFLOPs: 15.56 |
time (ms) | forward-compute: 818.96 | backward-compute: 723.13 | backward-embedding-all-reduce: 0.01 | optimizer: 54.53 | batch-generator: 112.94
[2023-01-05 05:34:38,630] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:38,657] [INFO] [timer.py:207:stop] 0/98, RunningAvgSamplesPerSec=65.48308975140982, CurrSamplesPerSec=72.40917975335577, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       98/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 1724.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.393208E+00 | moe loss: 4.082105E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 148.482 | TFLOPs: 14.64 |
time (ms) | forward-compute: 916.55 | backward-compute: 728.84 | backward-embedding-all-reduce: 0.01 | optimizer: 54.42 | batch-generator: 153.19
[2023-01-05 05:34:40,128] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:40,155] [INFO] [timer.py:207:stop] 0/99, RunningAvgSamplesPerSec=65.58865158588567, CurrSamplesPerSec=77.59734839440378, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration       99/     200 | consumed samples:        25344 | consumed tokens:     25952256 | elapsed time per iteration (ms): 1498.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.387695E+00 | moe loss: 4.051504E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 170.802 | TFLOPs: 16.84 |
time (ms) | forward-compute: 695.44 | backward-compute: 724.53 | backward-embedding-all-reduce: 0.01 | optimizer: 54.04 | batch-generator: 122.16
[2023-01-05 05:34:41,904] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:41,931] [INFO] [timer.py:207:stop] 0/100, RunningAvgSamplesPerSec=65.5790659958718, CurrSamplesPerSec=64.66239463035772, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      100/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 1775.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.395403E+00 | moe loss: 4.010664E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 144.173 | TFLOPs: 14.21 |
time (ms) | forward-compute: 971.06 | backward-compute: 723.76 | backward-embedding-all-reduce: 0.01 | optimizer: 55.69 | batch-generator: 191.14
[2023-01-05 05:34:43,461] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:43,488] [INFO] [timer.py:207:stop] 0/101, RunningAvgSamplesPerSec=65.73735566808062, CurrSamplesPerSec=86.10506124391907, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      101/     200 | consumed samples:        25856 | consumed tokens:     26476544 | elapsed time per iteration (ms): 1558.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.393195E+00 | moe loss: 4.007782E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 164.249 | TFLOPs: 16.19 |
time (ms) | forward-compute: 751.13 | backward-compute: 724.99 | backward-embedding-all-reduce: 0.01 | optimizer: 55.80 | batch-generator: 137.93
[2023-01-05 05:34:45,058] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:45,085] [INFO] [timer.py:207:stop] 0/102, RunningAvgSamplesPerSec=65.87941737204656, CurrSamplesPerSec=83.81007473847154, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      102/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 1595.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.382578E+00 | moe loss: 4.018641E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 160.483 | TFLOPs: 15.82 |
time (ms) | forward-compute: 789.91 | backward-compute: 725.27 | backward-embedding-all-reduce: 0.01 | optimizer: 55.41 | batch-generator: 117.53
[2023-01-05 05:34:46,638] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:46,665] [INFO] [timer.py:207:stop] 0/103, RunningAvgSamplesPerSec=65.99796340931306, CurrSamplesPerSec=80.47982184048992, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      103/     200 | consumed samples:        26368 | consumed tokens:     27000832 | elapsed time per iteration (ms): 1580.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.372416E+00 | moe loss: 4.062005E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 162.024 | TFLOPs: 15.97 |
time (ms) | forward-compute: 776.89 | backward-compute: 723.33 | backward-embedding-all-reduce: 0.01 | optimizer: 54.83 | batch-generator: 119.20
[2023-01-05 05:34:48,195] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:48,221] [INFO] [timer.py:207:stop] 0/104, RunningAvgSamplesPerSec=66.13029799786347, CurrSamplesPerSec=82.92390358125871, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      104/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 1554.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.381599E+00 | moe loss: 4.049987E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 164.691 | TFLOPs: 16.24 |
time (ms) | forward-compute: 753.06 | backward-compute: 724.22 | backward-embedding-all-reduce: 0.01 | optimizer: 54.22 | batch-generator: 113.18
[2023-01-05 05:34:49,808] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:49,835] [INFO] [timer.py:207:stop] 0/105, RunningAvgSamplesPerSec=66.28977689741939, CurrSamplesPerSec=87.91535040316506, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      105/     200 | consumed samples:        26880 | consumed tokens:     27525120 | elapsed time per iteration (ms): 1617.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.368646E+00 | moe loss: 4.070369E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 158.291 | TFLOPs: 15.61 |
time (ms) | forward-compute: 810.38 | backward-compute: 727.17 | backward-embedding-all-reduce: 0.01 | optimizer: 53.12 | batch-generator: 113.93
[2023-01-05 05:34:51,420] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:51,447] [INFO] [timer.py:207:stop] 0/106, RunningAvgSamplesPerSec=66.30897469395589, CurrSamplesPerSec=68.34773504543352, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      106/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 1610.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.364086E+00 | moe loss: 4.089269E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 158.955 | TFLOPs: 15.67 |
time (ms) | forward-compute: 809.59 | backward-compute: 723.77 | backward-embedding-all-reduce: 0.01 | optimizer: 53.15 | batch-generator: 105.91
[2023-01-05 05:34:53,693] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:53,720] [INFO] [timer.py:207:stop] 0/107, RunningAvgSamplesPerSec=66.36640813871796, CurrSamplesPerSec=72.93648951200956, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      107/     200 | consumed samples:        27392 | consumed tokens:     28049408 | elapsed time per iteration (ms): 2274.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.369813E+00 | moe loss: 4.103022E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 112.559 | TFLOPs: 11.10 |
time (ms) | forward-compute: 1466.39 | backward-compute: 723.33 | backward-embedding-all-reduce: 0.01 | optimizer: 58.36 | batch-generator: 118.92
[2023-01-05 05:34:55,288] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:55,315] [INFO] [timer.py:207:stop] 0/108, RunningAvgSamplesPerSec=66.51828268226743, CurrSamplesPerSec=87.55684604751322, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      108/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 1594.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.349797E+00 | moe loss: 4.122441E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 160.589 | TFLOPs: 15.83 |
time (ms) | forward-compute: 791.78 | backward-compute: 722.37 | backward-embedding-all-reduce: 0.01 | optimizer: 53.78 | batch-generator: 120.06
[2023-01-05 05:34:56,780] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:56,807] [INFO] [timer.py:207:stop] 0/109, RunningAvgSamplesPerSec=66.54980027455893, CurrSamplesPerSec=70.06899892874371, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      109/     200 | consumed samples:        27904 | consumed tokens:     28573696 | elapsed time per iteration (ms): 1490.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.346394E+00 | moe loss: 4.081011E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 171.734 | TFLOPs: 16.93 |
time (ms) | forward-compute: 687.20 | backward-compute: 723.49 | backward-embedding-all-reduce: 0.01 | optimizer: 53.53 | batch-generator: 110.06
[2023-01-05 05:34:58,268] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:58,295] [INFO] [timer.py:207:stop] 0/110, RunningAvgSamplesPerSec=66.5749749268807, CurrSamplesPerSec=69.3833544333571, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      110/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 1490.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.344400E+00 | moe loss: 4.098173E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 171.740 | TFLOPs: 16.93 |
time (ms) | forward-compute: 684.31 | backward-compute: 723.07 | backward-embedding-all-reduce: 0.01 | optimizer: 54.22 | batch-generator: 118.36
[2023-01-05 05:34:59,840] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:34:59,867] [INFO] [timer.py:207:stop] 0/111, RunningAvgSamplesPerSec=66.60160297285131, CurrSamplesPerSec=69.6084693293835, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      111/     200 | consumed samples:        28416 | consumed tokens:     29097984 | elapsed time per iteration (ms): 1569.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.329866E+00 | moe loss: 4.083670E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 163.130 | TFLOPs: 16.08 |
time (ms) | forward-compute: 765.51 | backward-compute: 724.01 | backward-embedding-all-reduce: 0.01 | optimizer: 53.52 | batch-generator: 111.77
[2023-01-05 05:35:01,314] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:01,341] [INFO] [timer.py:207:stop] 0/112, RunningAvgSamplesPerSec=66.73923843545084, CurrSamplesPerSec=86.143359044564, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      112/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 1474.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.328585E+00 | moe loss: 4.059325E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 173.664 | TFLOPs: 17.12 |
time (ms) | forward-compute: 671.48 | backward-compute: 724.27 | backward-embedding-all-reduce: 0.01 | optimizer: 54.25 | batch-generator: 107.05
[2023-01-05 05:35:03,936] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:03,962] [INFO] [timer.py:207:stop] 0/113, RunningAvgSamplesPerSec=66.26841954522006, CurrSamplesPerSec=37.31316427494557, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      113/     200 | consumed samples:        28928 | consumed tokens:     29622272 | elapsed time per iteration (ms): 2623.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.314495E+00 | moe loss: 4.031350E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 97.577 | TFLOPs: 9.62 |
time (ms) | forward-compute: 1820.04 | backward-compute: 721.46 | backward-embedding-all-reduce: 0.01 | optimizer: 54.58 | batch-generator: 184.07
[2023-01-05 05:35:05,361] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:05,387] [INFO] [timer.py:207:stop] 0/114, RunningAvgSamplesPerSec=66.3885438535278, CurrSamplesPerSec=83.11127513753037, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      114/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 1422.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.324394E+00 | moe loss: 4.124268E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 179.930 | TFLOPs: 17.74 |
time (ms) | forward-compute: 614.32 | backward-compute: 728.54 | backward-embedding-all-reduce: 0.01 | optimizer: 56.19 | batch-generator: 107.73
[2023-01-05 05:35:06,799] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:06,826] [INFO] [timer.py:207:stop] 0/115, RunningAvgSamplesPerSec=66.52221343014189, CurrSamplesPerSec=85.89115533942683, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      115/     200 | consumed samples:        29440 | consumed tokens:     30146560 | elapsed time per iteration (ms): 1439.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.310813E+00 | moe loss: 4.068111E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 177.808 | TFLOPs: 17.53 |
time (ms) | forward-compute: 631.33 | backward-compute: 728.87 | backward-embedding-all-reduce: 0.01 | optimizer: 53.59 | batch-generator: 119.09
[2023-01-05 05:35:08,340] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:08,367] [INFO] [timer.py:207:stop] 0/116, RunningAvgSamplesPerSec=66.66100757568671, CurrSamplesPerSec=87.22601877516271, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      116/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 1537.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.304841E+00 | moe loss: 4.085360E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 166.523 | TFLOPs: 16.42 |
time (ms) | forward-compute: 737.87 | backward-compute: 723.43 | backward-embedding-all-reduce: 0.01 | optimizer: 53.33 | batch-generator: 99.05
[2023-01-05 05:35:10,041] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:10,067] [INFO] [timer.py:207:stop] 0/117, RunningAvgSamplesPerSec=66.7713649424851, CurrSamplesPerSec=82.30444575400108, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      117/     200 | consumed samples:        29952 | consumed tokens:     30670848 | elapsed time per iteration (ms): 1703.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.306631E+00 | moe loss: 4.069398E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 150.249 | TFLOPs: 14.81 |
time (ms) | forward-compute: 893.57 | backward-compute: 724.48 | backward-embedding-all-reduce: 0.02 | optimizer: 60.60 | batch-generator: 333.86
[2023-01-05 05:35:11,755] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:11,782] [INFO] [timer.py:207:stop] 0/118, RunningAvgSamplesPerSec=66.78625214539633, CurrSamplesPerSec=68.5437240473451, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      118/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 1713.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.309340E+00 | moe loss: 4.050286E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.390 | TFLOPs: 14.73 |
time (ms) | forward-compute: 911.63 | backward-compute: 724.02 | backward-embedding-all-reduce: 0.01 | optimizer: 53.35 | batch-generator: 119.35
[2023-01-05 05:35:15,149] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:15,176] [INFO] [timer.py:207:stop] 0/119, RunningAvgSamplesPerSec=66.92076203532369, CurrSamplesPerSec=87.32152718320525, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      119/     200 | consumed samples:        30464 | consumed tokens:     31195136 | elapsed time per iteration (ms): 3394.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.289453E+00 | moe loss: 4.053569E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 75.418 | TFLOPs: 7.44 |
time (ms) | forward-compute: 2593.80 | backward-compute: 722.73 | backward-embedding-all-reduce: 0.01 | optimizer: 53.06 | batch-generator: 549.18
[2023-01-05 05:35:16,582] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:16,609] [INFO] [timer.py:207:stop] 0/120, RunningAvgSamplesPerSec=67.00199524976311, CurrSamplesPerSec=78.09299519227133, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      120/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 1433.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.292234E+00 | moe loss: 4.072234E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 178.646 | TFLOPs: 17.61 |
time (ms) | forward-compute: 631.80 | backward-compute: 723.38 | backward-embedding-all-reduce: 0.01 | optimizer: 52.62 | batch-generator: 110.53
[2023-01-05 05:35:18,037] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:18,064] [INFO] [timer.py:207:stop] 0/121, RunningAvgSamplesPerSec=67.05046473556318, CurrSamplesPerSec=73.308169340259, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      121/     200 | consumed samples:        30976 | consumed tokens:     31719424 | elapsed time per iteration (ms): 1456.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.280012E+00 | moe loss: 4.069423E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 175.711 | TFLOPs: 17.32 |
time (ms) | forward-compute: 648.81 | backward-compute: 722.98 | backward-embedding-all-reduce: 0.02 | optimizer: 56.29 | batch-generator: 112.08
[2023-01-05 05:35:19,637] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:19,664] [INFO] [timer.py:207:stop] 0/122, RunningAvgSamplesPerSec=67.17540795912431, CurrSamplesPerSec=86.31564975137688, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      122/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 1599.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.279056E+00 | moe loss: 4.107221E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 160.031 | TFLOPs: 15.78 |
time (ms) | forward-compute: 790.22 | backward-compute: 724.81 | backward-embedding-all-reduce: 0.01 | optimizer: 56.78 | batch-generator: 216.05
[2023-01-05 05:35:21,167] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:21,194] [INFO] [timer.py:207:stop] 0/123, RunningAvgSamplesPerSec=67.27344821306382, CurrSamplesPerSec=81.55701552902973, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      123/     200 | consumed samples:        31488 | consumed tokens:     32243712 | elapsed time per iteration (ms): 1525.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.272990E+00 | moe loss: 4.103464E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 167.855 | TFLOPs: 16.55 |
time (ms) | forward-compute: 721.13 | backward-compute: 722.75 | backward-embedding-all-reduce: 0.01 | optimizer: 58.75 | batch-generator: 105.99
[2023-01-05 05:35:22,810] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:22,837] [INFO] [timer.py:207:stop] 0/124, RunningAvgSamplesPerSec=67.40144233552633, CurrSamplesPerSec=87.55867385048754, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      124/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 1642.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.262223E+00 | moe loss: 4.074536E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 155.840 | TFLOPs: 15.36 |
time (ms) | forward-compute: 846.56 | backward-compute: 721.28 | backward-embedding-all-reduce: 0.01 | optimizer: 53.55 | batch-generator: 213.20
[2023-01-05 05:35:24,392] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:24,418] [INFO] [timer.py:207:stop] 0/125, RunningAvgSamplesPerSec=67.51854622805484, CurrSamplesPerSec=85.67951285310203, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      125/     200 | consumed samples:        32000 | consumed tokens:     32768000 | elapsed time per iteration (ms): 1583.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.264232E+00 | moe loss: 4.102529E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 161.653 | TFLOPs: 15.94 |
time (ms) | forward-compute: 780.96 | backward-compute: 723.90 | backward-embedding-all-reduce: 0.01 | optimizer: 53.07 | batch-generator: 250.68
[2023-01-05 05:35:26,125] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:26,152] [INFO] [timer.py:207:stop] 0/126, RunningAvgSamplesPerSec=67.53037608170645, CurrSamplesPerSec=69.01775712660772, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      126/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 1736.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.272482E+00 | moe loss: 4.078339E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 147.436 | TFLOPs: 14.54 |
time (ms) | forward-compute: 930.87 | backward-compute: 724.95 | backward-embedding-all-reduce: 0.01 | optimizer: 53.38 | batch-generator: 216.21
[2023-01-05 05:35:28,082] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:28,109] [INFO] [timer.py:207:stop] 0/127, RunningAvgSamplesPerSec=67.62585146177199, CurrSamplesPerSec=82.00183166184516, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      127/     200 | consumed samples:        32512 | consumed tokens:     33292288 | elapsed time per iteration (ms): 1949.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.275312E+00 | moe loss: 4.079678E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 131.340 | TFLOPs: 12.95 |
time (ms) | forward-compute: 1142.56 | backward-compute: 731.62 | backward-embedding-all-reduce: 0.01 | optimizer: 55.75 | batch-generator: 109.66
[2023-01-05 05:35:29,645] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:29,672] [INFO] [timer.py:207:stop] 0/128, RunningAvgSamplesPerSec=67.60604907949354, CurrSamplesPerSec=65.21885421429678, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      128/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 1571.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.254227E+00 | moe loss: 4.032084E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 162.935 | TFLOPs: 16.06 |
time (ms) | forward-compute: 712.44 | backward-compute: 779.29 | backward-embedding-all-reduce: 0.01 | optimizer: 53.69 | batch-generator: 121.78
[2023-01-05 05:35:31,037] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:31,064] [INFO] [timer.py:207:stop] 0/129, RunningAvgSamplesPerSec=67.72677886605548, CurrSamplesPerSec=87.39037960309693, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      129/     200 | consumed samples:        33024 | consumed tokens:     33816576 | elapsed time per iteration (ms): 1389.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.257650E+00 | moe loss: 4.070377E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 184.261 | TFLOPs: 18.17 |
time (ms) | forward-compute: 580.84 | backward-compute: 724.87 | backward-embedding-all-reduce: 0.01 | optimizer: 56.06 | batch-generator: 112.18
[2023-01-05 05:35:32,649] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:32,676] [INFO] [timer.py:207:stop] 0/130, RunningAvgSamplesPerSec=67.79495958395204, CurrSamplesPerSec=77.73325032070866, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      130/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 1613.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.266400E+00 | moe loss: 4.045528E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 158.679 | TFLOPs: 15.64 |
time (ms) | forward-compute: 809.21 | backward-compute: 724.36 | backward-embedding-all-reduce: 0.01 | optimizer: 53.62 | batch-generator: 128.49
[2023-01-05 05:35:35,423] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:35,450] [INFO] [timer.py:207:stop] 0/131, RunningAvgSamplesPerSec=67.90690196380291, CurrSamplesPerSec=86.10550315988553, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      131/     200 | consumed samples:        33536 | consumed tokens:     34340864 | elapsed time per iteration (ms): 2769.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.236478E+00 | moe loss: 4.034009E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 92.434 | TFLOPs: 9.11 |
time (ms) | forward-compute: 1964.85 | backward-compute: 722.96 | backward-embedding-all-reduce: 0.01 | optimizer: 59.76 | batch-generator: 103.35
[2023-01-05 05:35:37,058] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:37,085] [INFO] [timer.py:207:stop] 0/132, RunningAvgSamplesPerSec=68.02458585805091, CurrSamplesPerSec=87.61079833443975, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      132/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 1637.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.236573E+00 | moe loss: 4.170045E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 156.314 | TFLOPs: 15.41 |
time (ms) | forward-compute: 835.40 | backward-compute: 724.66 | backward-embedding-all-reduce: 0.01 | optimizer: 53.56 | batch-generator: 99.79
[2023-01-05 05:35:38,463] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:38,490] [INFO] [timer.py:207:stop] 0/133, RunningAvgSamplesPerSec=68.13585889722339, CurrSamplesPerSec=86.5383171616602, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      133/     200 | consumed samples:        34048 | consumed tokens:     34865152 | elapsed time per iteration (ms): 1405.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.250273E+00 | moe loss: 4.110937E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 182.089 | TFLOPs: 17.95 |
time (ms) | forward-compute: 594.18 | backward-compute: 733.03 | backward-embedding-all-reduce: 0.01 | optimizer: 53.23 | batch-generator: 107.52
[2023-01-05 05:35:40,011] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:40,038] [INFO] [timer.py:207:stop] 0/134, RunningAvgSamplesPerSec=68.2473839204062, CurrSamplesPerSec=86.87529604575433, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      134/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 1547.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.242362E+00 | moe loss: 4.091794E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 165.392 | TFLOPs: 16.31 |
time (ms) | forward-compute: 739.71 | backward-compute: 729.22 | backward-embedding-all-reduce: 0.01 | optimizer: 53.73 | batch-generator: 113.95
[2023-01-05 05:35:42,538] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:42,565] [INFO] [timer.py:207:stop] 0/135, RunningAvgSamplesPerSec=68.12164384482534, CurrSamplesPerSec=54.79544972058828, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      135/     200 | consumed samples:        34560 | consumed tokens:     35389440 | elapsed time per iteration (ms): 2529.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.224011E+00 | moe loss: 4.067849E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 101.224 | TFLOPs: 9.98 |
time (ms) | forward-compute: 1720.66 | backward-compute: 728.52 | backward-embedding-all-reduce: 0.01 | optimizer: 52.97 | batch-generator: 506.07
[2023-01-05 05:35:44,136] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:44,163] [INFO] [timer.py:207:stop] 0/136, RunningAvgSamplesPerSec=68.23505817432462, CurrSamplesPerSec=87.64140463693695, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      136/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 1597.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.230310E+00 | moe loss: 4.026309E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 160.278 | TFLOPs: 15.80 |
time (ms) | forward-compute: 788.53 | backward-compute: 725.33 | backward-embedding-all-reduce: 0.01 | optimizer: 56.74 | batch-generator: 110.36
[2023-01-05 05:35:45,518] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:45,545] [INFO] [timer.py:207:stop] 0/137, RunningAvgSamplesPerSec=68.34572906232506, CurrSamplesPerSec=87.3244246425344, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      137/     200 | consumed samples:        35072 | consumed tokens:     35913728 | elapsed time per iteration (ms): 1381.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.221227E+00 | moe loss: 4.054438E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 185.305 | TFLOPs: 18.27 |
time (ms) | forward-compute: 580.16 | backward-compute: 722.28 | backward-embedding-all-reduce: 0.01 | optimizer: 53.52 | batch-generator: 107.79
[2023-01-05 05:35:47,110] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:47,137] [INFO] [timer.py:207:stop] 0/138, RunningAvgSamplesPerSec=68.38836505313792, CurrSamplesPerSec=74.67746212511106, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      138/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 1591.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.219964E+00 | moe loss: 4.084365E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 160.874 | TFLOPs: 15.86 |
time (ms) | forward-compute: 783.56 | backward-compute: 726.53 | backward-embedding-all-reduce: 0.01 | optimizer: 56.56 | batch-generator: 97.69
[2023-01-05 05:35:49,699] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:49,725] [INFO] [timer.py:207:stop] 0/139, RunningAvgSamplesPerSec=68.49733523401675, CurrSamplesPerSec=87.44743129409665, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      139/     200 | consumed samples:        35584 | consumed tokens:     36438016 | elapsed time per iteration (ms): 2589.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.231684E+00 | moe loss: 4.098033E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 98.880 | TFLOPs: 9.75 |
time (ms) | forward-compute: 1781.24 | backward-compute: 726.93 | backward-embedding-all-reduce: 0.01 | optimizer: 54.76 | batch-generator: 133.72
[2023-01-05 05:35:51,214] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:51,241] [INFO] [timer.py:207:stop] 0/140, RunningAvgSamplesPerSec=68.53872559830721, CurrSamplesPerSec=74.72473418321955, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      140/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 1517.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.231704E+00 | moe loss: 4.124866E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 168.683 | TFLOPs: 16.63 |
time (ms) | forward-compute: 703.15 | backward-compute: 726.92 | backward-embedding-all-reduce: 0.02 | optimizer: 60.05 | batch-generator: 114.36
[2023-01-05 05:35:52,623] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:52,650] [INFO] [timer.py:207:stop] 0/141, RunningAvgSamplesPerSec=68.64189327908542, CurrSamplesPerSec=86.63886756913249, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      141/     200 | consumed samples:        36096 | consumed tokens:     36962304 | elapsed time per iteration (ms): 1407.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.204540E+00 | moe loss: 4.095578E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 181.873 | TFLOPs: 17.93 |
time (ms) | forward-compute: 600.16 | backward-compute: 725.25 | backward-embedding-all-reduce: 0.01 | optimizer: 54.68 | batch-generator: 109.18
[2023-01-05 05:35:55,175] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:55,201] [INFO] [timer.py:207:stop] 0/142, RunningAvgSamplesPerSec=68.68354564947758, CurrSamplesPerSec=75.01038270361623, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      142/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 2545.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.204592E+00 | moe loss: 4.126802E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 100.567 | TFLOPs: 9.91 |
time (ms) | forward-compute: 1745.14 | backward-compute: 723.12 | backward-embedding-all-reduce: 0.01 | optimizer: 55.51 | batch-generator: 302.25
[2023-01-05 05:35:56,654] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:56,681] [INFO] [timer.py:207:stop] 0/143, RunningAvgSamplesPerSec=68.74529579190924, CurrSamplesPerSec=78.64401408842708, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      143/     200 | consumed samples:        36608 | consumed tokens:     37486592 | elapsed time per iteration (ms): 1483.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.218081E+00 | moe loss: 4.212825E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 172.520 | TFLOPs: 17.01 |
time (ms) | forward-compute: 680.05 | backward-compute: 724.77 | backward-embedding-all-reduce: 0.01 | optimizer: 54.46 | batch-generator: 93.49
[2023-01-05 05:35:58,103] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:58,130] [INFO] [timer.py:207:stop] 0/144, RunningAvgSamplesPerSec=68.80063379307425, CurrSamplesPerSec=77.60937341888896, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      144/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 1445.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.203051E+00 | moe loss: 4.068806E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 177.147 | TFLOPs: 17.46 |
time (ms) | forward-compute: 645.53 | backward-compute: 724.13 | backward-embedding-all-reduce: 0.01 | optimizer: 54.37 | batch-generator: 98.25
[2023-01-05 05:35:59,736] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:35:59,763] [INFO] [timer.py:207:stop] 0/145, RunningAvgSamplesPerSec=68.90208806141102, CurrSamplesPerSec=87.15108651752077, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      145/     200 | consumed samples:        37120 | consumed tokens:     38010880 | elapsed time per iteration (ms): 1630.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.204722E+00 | moe loss: 4.036228E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 156.978 | TFLOPs: 15.48 |
time (ms) | forward-compute: 829.04 | backward-compute: 729.84 | backward-embedding-all-reduce: 0.01 | optimizer: 53.49 | batch-generator: 160.67
[2023-01-05 05:36:02,074] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:02,101] [INFO] [timer.py:207:stop] 0/146, RunningAvgSamplesPerSec=68.51850155612365, CurrSamplesPerSec=38.1485145503717, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      146/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 2345.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.215772E+00 | moe loss: 4.049656E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 109.161 | TFLOPs: 10.76 |
time (ms) | forward-compute: 1540.57 | backward-compute: 725.28 | backward-embedding-all-reduce: 0.01 | optimizer: 53.21 | batch-generator: 825.59
[2023-01-05 05:36:03,465] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:03,491] [INFO] [timer.py:207:stop] 0/147, RunningAvgSamplesPerSec=68.61963336357654, CurrSamplesPerSec=87.14056214469915, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      147/     200 | consumed samples:        37632 | consumed tokens:     38535168 | elapsed time per iteration (ms): 1390.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.195087E+00 | moe loss: 4.033202E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 184.123 | TFLOPs: 18.15 |
time (ms) | forward-compute: 581.80 | backward-compute: 727.67 | backward-embedding-all-reduce: 0.01 | optimizer: 54.25 | batch-generator: 116.42
[2023-01-05 05:36:05,043] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:05,070] [INFO] [timer.py:207:stop] 0/148, RunningAvgSamplesPerSec=68.66280172955743, CurrSamplesPerSec=75.55483700999984, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      148/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 1577.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.197294E+00 | moe loss: 4.034453E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 162.276 | TFLOPs: 16.00 |
time (ms) | forward-compute: 774.69 | backward-compute: 722.94 | backward-embedding-all-reduce: 0.01 | optimizer: 54.19 | batch-generator: 105.21
[2023-01-05 05:36:07,251] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:07,278] [INFO] [timer.py:207:stop] 0/149, RunningAvgSamplesPerSec=68.76123530884244, CurrSamplesPerSec=86.9627683940513, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      149/     200 | consumed samples:        38144 | consumed tokens:     39059456 | elapsed time per iteration (ms): 2208.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.188167E+00 | moe loss: 4.069809E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 115.893 | TFLOPs: 11.43 |
time (ms) | forward-compute: 1404.62 | backward-compute: 724.08 | backward-embedding-all-reduce: 0.02 | optimizer: 53.79 | batch-generator: 396.21
[2023-01-05 05:36:08,887] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:08,914] [INFO] [timer.py:207:stop] 0/150, RunningAvgSamplesPerSec=68.76674757815617, CurrSamplesPerSec=69.58677963606048, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      150/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 1633.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.199527E+00 | moe loss: 4.072980E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 156.742 | TFLOPs: 15.45 |
time (ms) | forward-compute: 832.40 | backward-compute: 721.65 | backward-embedding-all-reduce: 0.01 | optimizer: 53.59 | batch-generator: 154.25
[2023-01-05 05:36:10,407] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:10,434] [INFO] [timer.py:207:stop] 0/151, RunningAvgSamplesPerSec=68.84474568158076, CurrSamplesPerSec=82.73293796897123, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      151/     200 | consumed samples:        38656 | consumed tokens:     39583744 | elapsed time per iteration (ms): 1518.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.183433E+00 | moe loss: 4.040916E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 168.627 | TFLOPs: 16.62 |
time (ms) | forward-compute: 718.46 | backward-compute: 724.74 | backward-embedding-all-reduce: 0.01 | optimizer: 53.33 | batch-generator: 132.10
[2023-01-05 05:36:12,136] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:12,163] [INFO] [timer.py:207:stop] 0/152, RunningAvgSamplesPerSec=68.9447540987542, CurrSamplesPerSec=87.98993035159856, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      152/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 1728.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.193006E+00 | moe loss: 4.071794E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 148.068 | TFLOPs: 14.60 |
time (ms) | forward-compute: 928.08 | backward-compute: 726.22 | backward-embedding-all-reduce: 0.01 | optimizer: 53.62 | batch-generator: 251.57
[2023-01-05 05:36:13,825] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:13,852] [INFO] [timer.py:207:stop] 0/153, RunningAvgSamplesPerSec=68.94902418711551, CurrSamplesPerSec=69.5955838049341, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      153/     200 | consumed samples:        39168 | consumed tokens:     40108032 | elapsed time per iteration (ms): 1688.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.166782E+00 | moe loss: 4.017189E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 151.575 | TFLOPs: 14.94 |
time (ms) | forward-compute: 891.42 | backward-compute: 723.74 | backward-embedding-all-reduce: 0.02 | optimizer: 52.98 | batch-generator: 106.54
[2023-01-05 05:36:15,458] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:15,485] [INFO] [timer.py:207:stop] 0/154, RunningAvgSamplesPerSec=68.98226226788354, CurrSamplesPerSec=74.3978439669766, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      154/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 1636.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.176020E+00 | moe loss: 4.053206E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 156.457 | TFLOPs: 15.42 |
time (ms) | forward-compute: 833.68 | backward-compute: 723.26 | backward-embedding-all-reduce: 0.01 | optimizer: 54.70 | batch-generator: 111.39
[2023-01-05 05:36:17,199] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:17,225] [INFO] [timer.py:207:stop] 0/155, RunningAvgSamplesPerSec=69.02827109924577, CurrSamplesPerSec=76.81576495420279, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      155/     200 | consumed samples:        39680 | consumed tokens:     40632320 | elapsed time per iteration (ms): 1741.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.182310E+00 | moe loss: 4.043036E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 147.013 | TFLOPs: 14.49 |
time (ms) | forward-compute: 931.80 | backward-compute: 729.69 | backward-embedding-all-reduce: 0.01 | optimizer: 53.71 | batch-generator: 198.08
[2023-01-05 05:36:18,947] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:18,974] [INFO] [timer.py:207:stop] 0/156, RunningAvgSamplesPerSec=69.08518777996494, CurrSamplesPerSec=79.05884570482077, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      156/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 1749.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.173766E+00 | moe loss: 4.026105E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 146.366 | TFLOPs: 14.43 |
time (ms) | forward-compute: 945.21 | backward-compute: 722.53 | backward-embedding-all-reduce: 0.01 | optimizer: 53.02 | batch-generator: 269.00
[2023-01-05 05:36:20,729] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:20,755] [INFO] [timer.py:207:stop] 0/157, RunningAvgSamplesPerSec=69.05784868909343, CurrSamplesPerSec=65.0910417070805, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      157/     200 | consumed samples:        40192 | consumed tokens:     41156608 | elapsed time per iteration (ms): 1780.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.166573E+00 | moe loss: 4.082254E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 143.806 | TFLOPs: 14.18 |
time (ms) | forward-compute: 970.46 | backward-compute: 730.69 | backward-embedding-all-reduce: 0.01 | optimizer: 53.09 | batch-generator: 284.95
[2023-01-05 05:36:22,376] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:22,402] [INFO] [timer.py:207:stop] 0/158, RunningAvgSamplesPerSec=69.05453073563248, CurrSamplesPerSec=68.54407409544753, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      158/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 1647.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.171882E+00 | moe loss: 4.074230E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 155.406 | TFLOPs: 15.32 |
time (ms) | forward-compute: 841.76 | backward-compute: 726.60 | backward-embedding-all-reduce: 0.01 | optimizer: 53.97 | batch-generator: 193.13
[2023-01-05 05:36:24,539] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:24,566] [INFO] [timer.py:207:stop] 0/159, RunningAvgSamplesPerSec=69.04600870521308, CurrSamplesPerSec=67.74184378835842, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      159/     200 | consumed samples:        40704 | consumed tokens:     41680896 | elapsed time per iteration (ms): 2164.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.165269E+00 | moe loss: 4.100656E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 118.270 | TFLOPs: 11.66 |
time (ms) | forward-compute: 1357.33 | backward-compute: 728.14 | backward-embedding-all-reduce: 0.01 | optimizer: 52.58 | batch-generator: 149.08
[2023-01-05 05:36:25,919] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:25,946] [INFO] [timer.py:207:stop] 0/160, RunningAvgSamplesPerSec=69.13557671309648, CurrSamplesPerSec=86.81707884239349, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      160/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 1381.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.163868E+00 | moe loss: 4.097742E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 185.307 | TFLOPs: 18.27 |
time (ms) | forward-compute: 577.69 | backward-compute: 722.05 | backward-embedding-all-reduce: 0.01 | optimizer: 54.07 | batch-generator: 107.10
[2023-01-05 05:36:27,395] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:27,422] [INFO] [timer.py:207:stop] 0/161, RunningAvgSamplesPerSec=69.16264796123856, CurrSamplesPerSec=73.72376574319065, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      161/     200 | consumed samples:        41216 | consumed tokens:     42205184 | elapsed time per iteration (ms): 1477.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.175773E+00 | moe loss: 4.106697E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 173.226 | TFLOPs: 17.08 |
time (ms) | forward-compute: 667.45 | backward-compute: 723.80 | backward-embedding-all-reduce: 0.02 | optimizer: 55.26 | batch-generator: 112.68
[2023-01-05 05:36:29,048] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:29,075] [INFO] [timer.py:207:stop] 0/162, RunningAvgSamplesPerSec=69.25509676244228, CurrSamplesPerSec=87.94668854340992, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      162/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 1653.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.168481E+00 | moe loss: 4.129970E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 154.802 | TFLOPs: 15.26 |
time (ms) | forward-compute: 843.55 | backward-compute: 723.46 | backward-embedding-all-reduce: 0.01 | optimizer: 54.97 | batch-generator: 206.90
[2023-01-05 05:36:30,766] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:30,792] [INFO] [timer.py:207:stop] 0/163, RunningAvgSamplesPerSec=69.34853324593688, CurrSamplesPerSec=88.43967293503837, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      163/     200 | consumed samples:        41728 | consumed tokens:     42729472 | elapsed time per iteration (ms): 1714.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.166386E+00 | moe loss: 4.160212E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.301 | TFLOPs: 14.72 |
time (ms) | forward-compute: 906.30 | backward-compute: 723.25 | backward-embedding-all-reduce: 0.01 | optimizer: 53.87 | batch-generator: 152.00
[2023-01-05 05:36:32,872] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:32,899] [INFO] [timer.py:207:stop] 0/164, RunningAvgSamplesPerSec=69.43809170478877, CurrSamplesPerSec=87.66544700206789, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      164/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 2105.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.155094E+00 | moe loss: 4.086307E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 121.594 | TFLOPs: 11.99 |
time (ms) | forward-compute: 1297.82 | backward-compute: 726.89 | backward-embedding-all-reduce: 0.01 | optimizer: 53.24 | batch-generator: 501.80
[2023-01-05 05:36:34,346] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:34,373] [INFO] [timer.py:207:stop] 0/165, RunningAvgSamplesPerSec=69.50270030865599, CurrSamplesPerSec=81.83843129985799, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      165/     200 | consumed samples:        42240 | consumed tokens:     43253760 | elapsed time per iteration (ms): 1475.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.169064E+00 | moe loss: 4.096746E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 173.544 | TFLOPs: 17.11 |
time (ms) | forward-compute: 666.15 | backward-compute: 726.88 | backward-embedding-all-reduce: 0.01 | optimizer: 53.44 | batch-generator: 111.58
[2023-01-05 05:36:35,792] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:35,819] [INFO] [timer.py:207:stop] 0/166, RunningAvgSamplesPerSec=69.58816757044653, CurrSamplesPerSec=87.03314409475114, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      166/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 1447.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.137919E+00 | moe loss: 4.106227E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 176.867 | TFLOPs: 17.44 |
time (ms) | forward-compute: 636.80 | backward-compute: 724.15 | backward-embedding-all-reduce: 0.01 | optimizer: 55.52 | batch-generator: 103.74
[2023-01-05 05:36:37,540] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:37,567] [INFO] [timer.py:207:stop] 0/167, RunningAvgSamplesPerSec=69.65143825359621, CurrSamplesPerSec=81.85729802915857, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      167/     200 | consumed samples:        42752 | consumed tokens:     43778048 | elapsed time per iteration (ms): 1747.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.144118E+00 | moe loss: 4.128730E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 146.496 | TFLOPs: 14.44 |
time (ms) | forward-compute: 880.51 | backward-compute: 783.73 | backward-embedding-all-reduce: 0.01 | optimizer: 54.10 | batch-generator: 209.94
[2023-01-05 05:36:39,230] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:39,257] [INFO] [timer.py:207:stop] 0/168, RunningAvgSamplesPerSec=69.68277921314973, CurrSamplesPerSec=75.27128179819643, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      168/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 1688.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.129873E+00 | moe loss: 4.085474E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 151.616 | TFLOPs: 14.95 |
time (ms) | forward-compute: 878.94 | backward-compute: 725.51 | backward-embedding-all-reduce: 0.01 | optimizer: 55.84 | batch-generator: 224.14
[2023-01-05 05:36:41,773] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:41,800] [INFO] [timer.py:207:stop] 0/169, RunningAvgSamplesPerSec=69.7220842486724, CurrSamplesPerSec=76.92481678321357, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      169/     200 | consumed samples:        43264 | consumed tokens:     44302336 | elapsed time per iteration (ms): 2542.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.135129E+00 | moe loss: 4.110020E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 100.695 | TFLOPs: 9.93 |
time (ms) | forward-compute: 1727.84 | backward-compute: 734.42 | backward-embedding-all-reduce: 0.01 | optimizer: 53.39 | batch-generator: 111.92
[2023-01-05 05:36:43,213] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:43,240] [INFO] [timer.py:207:stop] 0/170, RunningAvgSamplesPerSec=69.79734104023805, CurrSamplesPerSec=85.14538017275505, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      170/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 1439.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.146774E+00 | moe loss: 4.062975E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 177.801 | TFLOPs: 17.53 |
time (ms) | forward-compute: 631.10 | backward-compute: 725.75 | backward-embedding-all-reduce: 0.01 | optimizer: 56.48 | batch-generator: 107.27
[2023-01-05 05:36:44,921] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:44,948] [INFO] [timer.py:207:stop] 0/171, RunningAvgSamplesPerSec=69.87268293486242, CurrSamplesPerSec=85.35064370177007, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      171/     200 | consumed samples:        43776 | consumed tokens:     44826624 | elapsed time per iteration (ms): 1709.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.139976E+00 | moe loss: 4.069351E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.742 | TFLOPs: 14.76 |
time (ms) | forward-compute: 901.35 | backward-compute: 729.11 | backward-embedding-all-reduce: 0.01 | optimizer: 52.85 | batch-generator: 173.18
[2023-01-05 05:36:46,421] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:46,448] [INFO] [timer.py:207:stop] 0/172, RunningAvgSamplesPerSec=69.93256554837757, CurrSamplesPerSec=81.77690040681985, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      172/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 1498.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.139161E+00 | moe loss: 4.082074E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 170.806 | TFLOPs: 16.84 |
time (ms) | forward-compute: 694.47 | backward-compute: 723.62 | backward-embedding-all-reduce: 0.01 | optimizer: 52.64 | batch-generator: 105.41
[2023-01-05 05:36:48,139] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:48,166] [INFO] [timer.py:207:stop] 0/173, RunningAvgSamplesPerSec=69.93344730908706, CurrSamplesPerSec=70.08367052075941, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      173/     200 | consumed samples:        44288 | consumed tokens:     45350912 | elapsed time per iteration (ms): 1711.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.120806E+00 | moe loss: 4.034170E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.585 | TFLOPs: 14.75 |
time (ms) | forward-compute: 914.90 | backward-compute: 722.28 | backward-embedding-all-reduce: 0.01 | optimizer: 54.02 | batch-generator: 241.72
[2023-01-05 05:36:49,759] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:49,785] [INFO] [timer.py:207:stop] 0/174, RunningAvgSamplesPerSec=69.99285191178427, CurrSamplesPerSec=81.88741296798393, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      174/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 1628.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.120358E+00 | moe loss: 4.067229E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 157.168 | TFLOPs: 15.49 |
time (ms) | forward-compute: 820.58 | backward-compute: 724.85 | backward-embedding-all-reduce: 0.01 | optimizer: 55.25 | batch-generator: 157.62
[2023-01-05 05:36:51,498] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:51,525] [INFO] [timer.py:207:stop] 0/175, RunningAvgSamplesPerSec=69.9747842278347, CurrSamplesPerSec=67.00002246350803, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      175/     200 | consumed samples:        44800 | consumed tokens:     45875200 | elapsed time per iteration (ms): 1736.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.125119E+00 | moe loss: 4.088486E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 147.398 | TFLOPs: 14.53 |
time (ms) | forward-compute: 931.59 | backward-compute: 722.30 | backward-embedding-all-reduce: 0.01 | optimizer: 53.32 | batch-generator: 264.20
[2023-01-05 05:36:53,084] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:53,111] [INFO] [timer.py:207:stop] 0/176, RunningAvgSamplesPerSec=70.00555023793386, CurrSamplesPerSec=75.76878106279058, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      176/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 1586.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.115522E+00 | moe loss: 4.081919E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 161.331 | TFLOPs: 15.91 |
time (ms) | forward-compute: 780.89 | backward-compute: 724.24 | backward-embedding-all-reduce: 0.01 | optimizer: 53.41 | batch-generator: 166.94
[2023-01-05 05:36:54,836] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:54,863] [INFO] [timer.py:207:stop] 0/177, RunningAvgSamplesPerSec=70.04706436606763, CurrSamplesPerSec=78.10640155121403, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      177/     200 | consumed samples:        45312 | consumed tokens:     46399488 | elapsed time per iteration (ms): 1751.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.120465E+00 | moe loss: 4.070722E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 146.178 | TFLOPs: 14.41 |
time (ms) | forward-compute: 940.13 | backward-compute: 729.89 | backward-embedding-all-reduce: 0.01 | optimizer: 53.47 | batch-generator: 251.21
[2023-01-05 05:36:56,521] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:56,548] [INFO] [timer.py:207:stop] 0/178, RunningAvgSamplesPerSec=70.04062300488519, CurrSamplesPerSec=68.93133969006564, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      178/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 1685.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.117269E+00 | moe loss: 4.091667E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 151.919 | TFLOPs: 14.98 |
time (ms) | forward-compute: 881.51 | backward-compute: 723.76 | backward-embedding-all-reduce: 0.01 | optimizer: 52.63 | batch-generator: 288.92
[2023-01-05 05:36:58,283] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:58,310] [INFO] [timer.py:207:stop] 0/179, RunningAvgSamplesPerSec=69.99614300991385, CurrSamplesPerSec=62.95916263177695, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      179/     200 | consumed samples:        45824 | consumed tokens:     46923776 | elapsed time per iteration (ms): 1756.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.108104E+00 | moe loss: 4.110945E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 145.724 | TFLOPs: 14.37 |
time (ms) | forward-compute: 956.19 | backward-compute: 724.16 | backward-embedding-all-reduce: 0.02 | optimizer: 54.08 | batch-generator: 255.33
[2023-01-05 05:36:59,830] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:36:59,857] [INFO] [timer.py:207:stop] 0/180, RunningAvgSamplesPerSec=70.04797942497014, CurrSamplesPerSec=80.61492776847471, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      180/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 1548.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.104157E+00 | moe loss: 4.081851E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 165.359 | TFLOPs: 16.30 |
time (ms) | forward-compute: 747.46 | backward-compute: 722.42 | backward-embedding-all-reduce: 0.01 | optimizer: 55.48 | batch-generator: 93.03
[2023-01-05 05:37:01,527] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:01,554] [INFO] [timer.py:207:stop] 0/181, RunningAvgSamplesPerSec=70.08758396939479, CurrSamplesPerSec=77.93048613026374, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      181/     200 | consumed samples:        46336 | consumed tokens:     47448064 | elapsed time per iteration (ms): 1700.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.104403E+00 | moe loss: 4.962746E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 150.521 | TFLOPs: 14.84 |
time (ms) | forward-compute: 898.16 | backward-compute: 723.43 | backward-embedding-all-reduce: 0.01 | optimizer: 53.13 | batch-generator: 211.57
[2023-01-05 05:37:03,274] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:03,301] [INFO] [timer.py:207:stop] 0/182, RunningAvgSamplesPerSec=70.12582381517075, CurrSamplesPerSec=77.71574155459591, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      182/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 1742.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.119494E+00 | moe loss: 4.075520E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 146.922 | TFLOPs: 14.48 |
time (ms) | forward-compute: 944.28 | backward-compute: 722.89 | backward-embedding-all-reduce: 0.01 | optimizer: 52.97 | batch-generator: 280.77
[2023-01-05 05:37:04,953] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:04,980] [INFO] [timer.py:207:stop] 0/183, RunningAvgSamplesPerSec=70.17040497182965, CurrSamplesPerSec=79.2376994111681, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      183/     200 | consumed samples:        46848 | consumed tokens:     47972352 | elapsed time per iteration (ms): 1683.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.106787E+00 | moe loss: 4.050373E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 152.085 | TFLOPs: 14.99 |
time (ms) | forward-compute: 880.05 | backward-compute: 723.36 | backward-embedding-all-reduce: 0.01 | optimizer: 53.61 | batch-generator: 186.59
[2023-01-05 05:37:06,669] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:06,696] [INFO] [timer.py:207:stop] 0/184, RunningAvgSamplesPerSec=70.20396127741124, CurrSamplesPerSec=76.85636457859128, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      184/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 1713.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.099042E+00 | moe loss: 4.062403E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.424 | TFLOPs: 14.73 |
time (ms) | forward-compute: 904.39 | backward-compute: 724.93 | backward-embedding-all-reduce: 0.01 | optimizer: 58.00 | batch-generator: 180.88
[2023-01-05 05:37:08,335] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:08,362] [INFO] [timer.py:207:stop] 0/185, RunningAvgSamplesPerSec=70.24355901415683, CurrSamplesPerSec=78.27932345736615, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      185/     200 | consumed samples:        47360 | consumed tokens:     48496640 | elapsed time per iteration (ms): 1664.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.091029E+00 | moe loss: 4.069116E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 153.786 | TFLOPs: 15.16 |
time (ms) | forward-compute: 852.26 | backward-compute: 731.73 | backward-embedding-all-reduce: 0.01 | optimizer: 57.71 | batch-generator: 197.65
[2023-01-05 05:37:10,165] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:10,192] [INFO] [timer.py:207:stop] 0/186, RunningAvgSamplesPerSec=70.28065917765552, CurrSamplesPerSec=77.80038779124195, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      186/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 1831.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.098137E+00 | moe loss: 4.064255E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 139.766 | TFLOPs: 13.78 |
time (ms) | forward-compute: 1030.85 | backward-compute: 720.80 | backward-embedding-all-reduce: 0.01 | optimizer: 56.32 | batch-generator: 168.79
[2023-01-05 05:37:11,833] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:11,860] [INFO] [timer.py:207:stop] 0/187, RunningAvgSamplesPerSec=70.30643028195169, CurrSamplesPerSec=75.39326437240027, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      187/     200 | consumed samples:        47872 | consumed tokens:     49020928 | elapsed time per iteration (ms): 1670.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.102718E+00 | moe loss: 4.085506E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 153.274 | TFLOPs: 15.11 |
time (ms) | forward-compute: 866.20 | backward-compute: 723.01 | backward-embedding-all-reduce: 0.01 | optimizer: 54.33 | batch-generator: 177.80
[2023-01-05 05:37:13,511] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:13,538] [INFO] [timer.py:207:stop] 0/188, RunningAvgSamplesPerSec=70.34152487705933, CurrSamplesPerSec=77.49814970179857, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      188/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 1678.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.096917E+00 | moe loss: 4.165509E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 152.562 | TFLOPs: 15.04 |
time (ms) | forward-compute: 863.53 | backward-compute: 726.62 | backward-embedding-all-reduce: 0.01 | optimizer: 60.48 | batch-generator: 140.98
[2023-01-05 05:37:15,198] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:15,225] [INFO] [timer.py:207:stop] 0/189, RunningAvgSamplesPerSec=70.41793473998891, CurrSamplesPerSec=88.24812990618135, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      189/     200 | consumed samples:        48384 | consumed tokens:     49545216 | elapsed time per iteration (ms): 1687.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.098939E+00 | moe loss: 4.160254E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 151.678 | TFLOPs: 14.95 |
time (ms) | forward-compute: 878.53 | backward-compute: 728.62 | backward-embedding-all-reduce: 0.01 | optimizer: 53.35 | batch-generator: 184.66
[2023-01-05 05:37:16,860] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:16,887] [INFO] [timer.py:207:stop] 0/190, RunningAvgSamplesPerSec=70.45785456286802, CurrSamplesPerSec=78.81280145437975, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      190/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 1661.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.090285E+00 | moe loss: 4.132918E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 154.096 | TFLOPs: 15.19 |
time (ms) | forward-compute: 855.72 | backward-compute: 726.76 | backward-embedding-all-reduce: 0.01 | optimizer: 52.75 | batch-generator: 261.42
[2023-01-05 05:37:18,571] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:18,598] [INFO] [timer.py:207:stop] 0/191, RunningAvgSamplesPerSec=70.53238603862096, CurrSamplesPerSec=88.04106816049958, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      191/     200 | consumed samples:        48896 | consumed tokens:     50069504 | elapsed time per iteration (ms): 1710.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.092052E+00 | moe loss: 4.138189E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.626 | TFLOPs: 14.75 |
time (ms) | forward-compute: 904.39 | backward-compute: 726.87 | backward-embedding-all-reduce: 0.01 | optimizer: 53.28 | batch-generator: 122.09
[2023-01-05 05:37:20,176] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:20,203] [INFO] [timer.py:207:stop] 0/192, RunningAvgSamplesPerSec=70.57650225703722, CurrSamplesPerSec=80.03819380877215, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      192/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 1604.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.094414E+00 | moe loss: 4.096354E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 159.534 | TFLOPs: 15.73 |
time (ms) | forward-compute: 794.69 | backward-compute: 728.12 | backward-embedding-all-reduce: 0.01 | optimizer: 55.39 | batch-generator: 150.93
[2023-01-05 05:37:21,883] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:21,910] [INFO] [timer.py:207:stop] 0/193, RunningAvgSamplesPerSec=70.59987789075312, CurrSamplesPerSec=75.34108274748691, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      193/     200 | consumed samples:        49408 | consumed tokens:     50593792 | elapsed time per iteration (ms): 1703.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.069606E+00 | moe loss: 4.055601E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 150.312 | TFLOPs: 14.82 |
time (ms) | forward-compute: 896.08 | backward-compute: 728.23 | backward-embedding-all-reduce: 0.01 | optimizer: 53.02 | batch-generator: 164.46
[2023-01-05 05:37:23,514] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:23,540] [INFO] [timer.py:207:stop] 0/194, RunningAvgSamplesPerSec=70.62182473862882, CurrSamplesPerSec=75.07965839351871, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      194/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 1628.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.077220E+00 | moe loss: 4.081662E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 157.183 | TFLOPs: 15.50 |
time (ms) | forward-compute: 830.69 | backward-compute: 723.10 | backward-embedding-all-reduce: 0.01 | optimizer: 54.63 | batch-generator: 130.72
[2023-01-05 05:37:25,202] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:25,229] [INFO] [timer.py:207:stop] 0/195, RunningAvgSamplesPerSec=70.62286953861555, CurrSamplesPerSec=70.82404554513856, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      195/     200 | consumed samples:        49920 | consumed tokens:     51118080 | elapsed time per iteration (ms): 1694.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.074096E+00 | moe loss: 4.083041E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 151.100 | TFLOPs: 14.90 |
time (ms) | forward-compute: 883.21 | backward-compute: 723.34 | backward-embedding-all-reduce: 0.01 | optimizer: 59.95 | batch-generator: 204.17
[2023-01-05 05:37:26,914] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:26,941] [INFO] [timer.py:207:stop] 0/196, RunningAvgSamplesPerSec=70.67017586091872, CurrSamplesPerSec=81.1629121179815, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      196/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 1708.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.069749E+00 | moe loss: 4.084827E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 149.803 | TFLOPs: 14.77 |
time (ms) | forward-compute: 904.44 | backward-compute: 726.45 | backward-embedding-all-reduce: 0.02 | optimizer: 54.62 | batch-generator: 260.21
[2023-01-05 05:37:28,591] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:28,618] [INFO] [timer.py:207:stop] 0/197, RunningAvgSamplesPerSec=70.70777077990252, CurrSamplesPerSec=78.84483951731244, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      197/     200 | consumed samples:        50432 | consumed tokens:     51642368 | elapsed time per iteration (ms): 1679.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.071887E+00 | moe loss: 4.104097E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 152.431 | TFLOPs: 15.03 |
time (ms) | forward-compute: 872.03 | backward-compute: 724.46 | backward-embedding-all-reduce: 0.01 | optimizer: 56.20 | batch-generator: 110.66
[2023-01-05 05:37:30,240] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:30,267] [INFO] [timer.py:207:stop] 0/198, RunningAvgSamplesPerSec=70.76643533393222, CurrSamplesPerSec=84.42534690334512, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      198/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 1649.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.069228E+00 | moe loss: 4.121225E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 155.212 | TFLOPs: 15.30 |
time (ms) | forward-compute: 847.65 | backward-compute: 723.00 | backward-embedding-all-reduce: 0.01 | optimizer: 52.68 | batch-generator: 102.10
[2023-01-05 05:37:31,987] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:32,014] [INFO] [timer.py:207:stop] 0/199, RunningAvgSamplesPerSec=70.73433757133016, CurrSamplesPerSec=64.95942169551704, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      199/     200 | consumed samples:        50944 | consumed tokens:     52166656 | elapsed time per iteration (ms): 1747.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.073498E+00 | moe loss: 4.094809E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 146.535 | TFLOPs: 14.45 |
time (ms) | forward-compute: 942.28 | backward-compute: 724.36 | backward-embedding-all-reduce: 0.01 | optimizer: 54.54 | batch-generator: 225.30
[2023-01-05 05:37:33,618] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 05:37:33,645] [INFO] [timer.py:207:stop] 0/200, RunningAvgSamplesPerSec=70.77484267976173, CurrSamplesPerSec=79.77412283829983, MemAllocated=6.63GB, MaxMemAllocated=10.36GB
 iteration      200/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 1630.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.073750E+00 | moe loss: 4.094328E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 157.008 | TFLOPs: 15.48 |
time (ms) | forward-compute: 822.33 | backward-compute: 723.84 | backward-embedding-all-reduce: 0.01 | optimizer: 58.43 | batch-generator: 177.58
[after training is done] datetime: 2023-01-05 05:37:33 
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.421694278717041 seconds
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.41498804092407227 seconds
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4069497585296631 seconds
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40770435333251953 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4113161563873291 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
Loading extension module utils...
Time to load utils op: 0.4614405632019043 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.42952775955200195 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.41245055198669434 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4297366142272949 seconds
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4092695713043213 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4079151153564453 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.41568541526794434 seconds
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40801334381103516 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.415752649307251 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4108867645263672 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4085581302642822 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.41498851776123047 seconds
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40726518630981445 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.415050745010376 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.41435766220092773 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.5087649822235107 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.5098905563354492 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4072592258453369 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4073963165283203 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4296290874481201 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4070136547088623 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40712881088256836 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4067554473876953 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.42618775367736816 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_32
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.41289591789245605 seconds

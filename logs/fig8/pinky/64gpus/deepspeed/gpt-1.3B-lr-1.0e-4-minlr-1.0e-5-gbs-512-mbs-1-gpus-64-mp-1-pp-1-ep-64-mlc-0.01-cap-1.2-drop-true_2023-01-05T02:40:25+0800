--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
using world size: 64, data-parallel-size: 64, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 64
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-1.3B-lr-1.0e-4-minlr-1.0e-5-gbs-512-mbs-1-gpus-64-mp-1-pp-1-ep-64-mlc-0.01-cap-1.2-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1536
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 64
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.2
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [64]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /GPUFS/thu_wgchen_2/zms/Auto-Megatron/deepspeed/output/tensorboard/gpt-1.3B-lr-1.0e-4-minlr-1.0e-5-gbs-512-mbs-1-gpus-64-mp-1-pp-1-ep-64-mlc-0.01-cap-1.2-drop-true_ln101_2023.01.05-02.40.25
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 200
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 64
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2023-01-05 02:40:37,928] [INFO] [comm.py:656:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: è¿›å…¥ç›®å½•â€œ/GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/dataâ€
make: å¯¹â€œdefaultâ€æ— éœ€åšä»»ä½•äº‹ã€‚
make: ç¦»å¼€ç›®å½•â€œ/GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/dataâ€
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.240 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 14.951 seconds
time to initialize megatron (seconds): 196.130
[after megatron is initialized] datetime: 2023-01-05 02:43:16 
building GPT model ...
[2023-01-05 02:43:16,225] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2023-01-05 02:43:16,226] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2023-01-05 02:43:16,226] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 14.36 GB, percent = 5.7%
[2023-01-05 02:43:16,269] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 1 | expert_parallel_size: 64
[2023-01-05 02:43:16,284] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 1 | expert_parallel_size: 64
[2023-01-05 02:43:16,291] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 1 | expert_parallel_size: 64
[2023-01-05 02:43:16,299] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 1 | expert_parallel_size: 64
[2023-01-05 02:43:16,305] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 1 | expert_parallel_size: 64
[2023-01-05 02:43:16,311] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 1 | expert_parallel_size: 64
[2023-01-05 02:43:16,317] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 1 | expert_parallel_size: 64
[2023-01-05 02:43:16,329] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 64 | num_local_experts: 1 | expert_parallel_size: 64
[2023-01-05 02:43:16,385] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2023-01-05 02:43:16,385] [INFO] [utils.py:832:see_memory_usage] MA 0.99 GB         Max_MA 1.02 GB         CA 1.06 GB         Max_CA 1 GB 
[2023-01-05 02:43:16,386] [INFO] [utils.py:837:see_memory_usage] CPU Virtual Memory:  used = 14.37 GB, percent = 5.7%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 532933632
> learning rate decay style: cosine
DeepSpeed is enabled.
[2023-01-05 02:43:16,389] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_64
[2023-01-05 02:43:16,411] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 64
[2023-01-05 02:43:16,662] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [0]
[2023-01-05 02:43:16,694] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [1]
[2023-01-05 02:43:16,726] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [2]
[2023-01-05 02:43:16,737] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [3]
[2023-01-05 02:43:16,749] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [4]
[2023-01-05 02:43:16,760] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [5]
[2023-01-05 02:43:16,771] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [6]
[2023-01-05 02:43:16,782] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [7]
[2023-01-05 02:43:16,794] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [8]
[2023-01-05 02:43:16,805] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [9]
[2023-01-05 02:43:16,816] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [10]
[2023-01-05 02:43:16,828] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [11]
[2023-01-05 02:43:16,839] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [12]
[2023-01-05 02:43:16,850] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [13]
[2023-01-05 02:43:16,861] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [14]
[2023-01-05 02:43:16,872] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [15]
[2023-01-05 02:43:16,883] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [16]
[2023-01-05 02:43:16,895] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [17]
[2023-01-05 02:43:16,906] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [18]
[2023-01-05 02:43:16,917] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [19]
[2023-01-05 02:43:16,929] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [20]
[2023-01-05 02:43:16,940] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [21]
[2023-01-05 02:43:16,951] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [22]
[2023-01-05 02:43:16,962] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [23]
[2023-01-05 02:43:16,974] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [24]
[2023-01-05 02:43:16,985] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [25]
[2023-01-05 02:43:16,996] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [26]
[2023-01-05 02:43:17,007] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [27]
[2023-01-05 02:43:17,019] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [28]
[2023-01-05 02:43:17,030] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [29]
[2023-01-05 02:43:17,041] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [30]
[2023-01-05 02:43:17,053] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [31]
[2023-01-05 02:43:17,064] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [32]
[2023-01-05 02:43:17,075] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [33]
[2023-01-05 02:43:17,086] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [34]
[2023-01-05 02:43:17,098] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [35]
[2023-01-05 02:43:17,109] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [36]
[2023-01-05 02:43:17,121] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [37]
[2023-01-05 02:43:17,132] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [38]
[2023-01-05 02:43:17,143] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [39]
[2023-01-05 02:43:17,155] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [40]
[2023-01-05 02:43:17,166] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [41]
[2023-01-05 02:43:17,177] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [42]
[2023-01-05 02:43:17,188] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [43]
[2023-01-05 02:43:17,200] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [44]
[2023-01-05 02:43:17,211] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [45]
[2023-01-05 02:43:17,223] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [46]
[2023-01-05 02:43:17,224] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [47]
[2023-01-05 02:43:17,235] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [48]
[2023-01-05 02:43:17,246] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [49]
[2023-01-05 02:43:17,258] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [50]
[2023-01-05 02:43:17,269] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [51]
[2023-01-05 02:43:17,281] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [52]
[2023-01-05 02:43:17,292] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [53]
[2023-01-05 02:43:17,303] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [54]
[2023-01-05 02:43:17,315] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [55]
[2023-01-05 02:43:17,326] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [56]
[2023-01-05 02:43:17,337] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [57]
[2023-01-05 02:43:17,349] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [58]
[2023-01-05 02:43:17,360] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [59]
[2023-01-05 02:43:17,371] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [60]
[2023-01-05 02:43:17,383] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [61]
[2023-01-05 02:43:17,394] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [62]
[2023-01-05 02:43:17,406] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_64 with ranks: [63]
[2023-01-05 02:43:17,417] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_64 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[2023-01-05 02:43:17,752] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-01-05 02:43:17,752] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-01-05 02:43:17,752] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-01-05 02:43:17,758] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-01-05 02:43:17,758] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-01-05 02:43:17,894] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-01-05 02:43:17,895] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-01-05 02:43:17,895] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x2b5525273c18>
[2023-01-05 02:43:17,895] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:43:17,896] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2023-01-05 02:43:17,899] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-01-05 02:43:17,899] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-01-05 02:43:17,899] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2023-01-05 02:43:17,899] [INFO] [config.py:1024:print]   amp_params ................... False
[2023-01-05 02:43:17,900] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-01-05 02:43:17,900] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2023-01-05 02:43:17,900] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2023-01-05 02:43:17,900] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2023-01-05 02:43:17,900] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2b552fc15630>
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 212296, 'difficulty_step': 8}}
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   dump_state ................... False
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2023-01-05 02:43:17,901] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2023-01-05 02:43:17,902] [INFO] [config.py:1024:print]   global_rank .................. 0
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x2b552fc15198>
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2023-01-05 02:43:17,903] [INFO] [config.py:1024:print]   pld_params ................... False
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   train_batch_size ............. 512
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  1
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   world_size ................... 64
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2023-01-05 02:43:17,904] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2023-01-05 02:43:17,905] [INFO] [config.py:1016:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 1, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.122960e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Emitting ninja build file /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.559875726699829 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-05 02:43:20 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: 51200000
    test:       51200000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.021076 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.066 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-05 02:43:23 
done with setup ...
training ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
> setting tensorboard ...
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4706101417541504 seconds
time (ms) | model-and-optimizer-setup: 4326.20 | train/valid/test-data-iterators-setup: 2461.30
[before the start of training step] datetime: 2023-01-05 02:43:23 
[2023-01-05 02:43:43,144] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 7119.314453125 | max allocated: 11053.060546875 | reserved: 12862.0 | max reserved: 12862.0
 iteration        1/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 20063.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.101444E+01 | moe loss: 1.354098E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.519 | TFLOPs: 1.37 |
time (ms) | forward-compute: 18057.72 | backward-compute: 1825.50 | backward-embedding-all-reduce: 0.01 | optimizer: 139.92 | batch-generator: 11008.94
[2023-01-05 02:43:57,725] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 16862.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.757054E+00 | moe loss: 2.373689E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.363 | TFLOPs: 1.64 |
time (ms) | forward-compute: 13429.68 | backward-compute: 1058.44 | backward-embedding-all-reduce: 0.01 | optimizer: 69.46 | batch-generator: 11161.63
[2023-01-05 02:44:15,767] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:44:15,794] [INFO] [timer.py:207:stop] 0/3, RunningAvgSamplesPerSec=70.43281756767571, CurrSamplesPerSec=70.43281756767571, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration        3/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 17011.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.277922E+00 | moe loss: 2.442920E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.098 | TFLOPs: 1.62 |
time (ms) | forward-compute: 14604.03 | backward-compute: 1059.88 | backward-embedding-all-reduce: 0.01 | optimizer: 68.62 | batch-generator: 11934.09
[2023-01-05 02:44:33,056] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:44:33,083] [INFO] [timer.py:207:stop] 0/4, RunningAvgSamplesPerSec=62.180946281044385, CurrSamplesPerSec=55.65986609805998, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration        4/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 16347.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.226539E+00 | moe loss: 2.074580E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.321 | TFLOPs: 1.69 |
time (ms) | forward-compute: 14887.61 | backward-compute: 1055.36 | backward-embedding-all-reduce: 0.01 | optimizer: 67.38 | batch-generator: 12046.53
[2023-01-05 02:44:48,229] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:44:48,256] [INFO] [timer.py:207:stop] 0/5, RunningAvgSamplesPerSec=71.25494584428037, CurrSamplesPerSec=100.62238042455986, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration        5/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 14867.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.016802E+00 | moe loss: 1.939731E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 34.438 | TFLOPs: 1.86 |
time (ms) | forward-compute: 13724.92 | backward-compute: 1048.23 | backward-embedding-all-reduce: 0.01 | optimizer: 64.56 | batch-generator: 11282.40
[2023-01-05 02:45:01,487] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:45:01,514] [INFO] [timer.py:207:stop] 0/6, RunningAvgSamplesPerSec=78.42404156509504, CurrSamplesPerSec=112.32892125334818, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration        6/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 13258.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.783896E+00 | moe loss: 1.806314E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.615 | TFLOPs: 2.08 |
time (ms) | forward-compute: 12049.13 | backward-compute: 1117.79 | backward-embedding-all-reduce: 0.01 | optimizer: 63.64 | batch-generator: 9868.36
[2023-01-05 02:45:17,331] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:45:17,359] [INFO] [timer.py:207:stop] 0/7, RunningAvgSamplesPerSec=80.77184090217571, CurrSamplesPerSec=91.76000612564128, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration        7/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 16882.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.651885E+00 | moe loss: 1.641441E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.328 | TFLOPs: 1.63 |
time (ms) | forward-compute: 14723.15 | backward-compute: 1029.35 | backward-embedding-all-reduce: 0.01 | optimizer: 63.10 | batch-generator: 12114.48
[2023-01-05 02:45:31,049] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:45:31,077] [INFO] [timer.py:207:stop] 0/8, RunningAvgSamplesPerSec=82.25979947991748, CurrSamplesPerSec=90.60534399661928, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration        8/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 12680.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.507058E+00 | moe loss: 1.628610E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 40.377 | TFLOPs: 2.18 |
time (ms) | forward-compute: 11543.27 | backward-compute: 1043.66 | backward-embedding-all-reduce: 0.02 | optimizer: 61.06 | batch-generator: 8695.01
[2023-01-05 02:45:45,223] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:45:45,247] [INFO] [timer.py:207:stop] 0/9, RunningAvgSamplesPerSec=85.1060601750993, CurrSamplesPerSec=107.40361098941109, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration        9/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 14169.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.370755E+00 | moe loss: 1.507196E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 36.133 | TFLOPs: 1.95 |
time (ms) | forward-compute: 13056.21 | backward-compute: 1020.49 | backward-embedding-all-reduce: 0.01 | optimizer: 63.67 | batch-generator: 10133.83
[2023-01-05 02:45:56,914] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:45:56,942] [INFO] [timer.py:207:stop] 0/10, RunningAvgSamplesPerSec=85.00524040021287, CurrSamplesPerSec=84.30613531922805, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       10/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 11695.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.255972E+00 | moe loss: 1.466624E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 43.777 | TFLOPs: 2.36 |
time (ms) | forward-compute: 10562.70 | backward-compute: 1030.36 | backward-embedding-all-reduce: 0.01 | optimizer: 74.45 | batch-generator: 8310.91
[2023-01-05 02:46:08,462] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:46:08,490] [INFO] [timer.py:207:stop] 0/11, RunningAvgSamplesPerSec=87.55568467068973, CurrSamplesPerSec=115.20894356537458, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       11/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 11548.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.154457E+00 | moe loss: 1.394071E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 44.335 | TFLOPs: 2.39 |
time (ms) | forward-compute: 10426.46 | backward-compute: 1029.65 | backward-embedding-all-reduce: 0.01 | optimizer: 62.05 | batch-generator: 8855.06
[2023-01-05 02:46:19,995] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:46:20,023] [INFO] [timer.py:207:stop] 0/12, RunningAvgSamplesPerSec=88.03560235241775, CurrSamplesPerSec=92.60389775037439, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       12/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 11532.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.034388E+00 | moe loss: 1.341087E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 44.398 | TFLOPs: 2.39 |
time (ms) | forward-compute: 10419.27 | backward-compute: 1020.82 | backward-embedding-all-reduce: 0.01 | optimizer: 61.93 | batch-generator: 8404.44
[2023-01-05 02:46:32,818] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:46:32,845] [INFO] [timer.py:207:stop] 0/13, RunningAvgSamplesPerSec=86.70706733994662, CurrSamplesPerSec=75.3379110130937, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       13/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 12819.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.945395E+00 | moe loss: 1.277456E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 39.939 | TFLOPs: 2.15 |
time (ms) | forward-compute: 11701.70 | backward-compute: 1025.35 | backward-embedding-all-reduce: 0.01 | optimizer: 65.23 | batch-generator: 9731.96
[2023-01-05 02:46:44,688] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:46:44,715] [INFO] [timer.py:207:stop] 0/14, RunningAvgSamplesPerSec=86.90482782555544, CurrSamplesPerSec=89.14126393187904, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       14/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 11872.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.856167E+00 | moe loss: 1.260953E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 43.124 | TFLOPs: 2.32 |
time (ms) | forward-compute: 10757.16 | backward-compute: 1026.27 | backward-embedding-all-reduce: 0.01 | optimizer: 62.12 | batch-generator: 7978.55
[2023-01-05 02:46:58,510] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:46:58,537] [INFO] [timer.py:207:stop] 0/15, RunningAvgSamplesPerSec=86.25546154554404, CurrSamplesPerSec=79.15771743306911, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       15/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 13823.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.769800E+00 | moe loss: 1.241260E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 37.039 | TFLOPs: 2.00 |
time (ms) | forward-compute: 12625.73 | backward-compute: 1103.57 | backward-embedding-all-reduce: 0.01 | optimizer: 62.80 | batch-generator: 9768.82
[2023-01-05 02:47:11,870] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:47:11,898] [INFO] [timer.py:207:stop] 0/16, RunningAvgSamplesPerSec=87.88108959934803, CurrSamplesPerSec=116.39982481625219, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       16/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 13360.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.695982E+00 | moe loss: 1.213408E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.323 | TFLOPs: 2.06 |
time (ms) | forward-compute: 12244.49 | backward-compute: 1019.88 | backward-embedding-all-reduce: 0.01 | optimizer: 64.40 | batch-generator: 9624.47
[2023-01-05 02:47:23,921] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:47:23,949] [INFO] [timer.py:207:stop] 0/17, RunningAvgSamplesPerSec=88.70463203708395, CurrSamplesPerSec=102.09963364613381, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       17/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 12049.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.631125E+00 | moe loss: 1.167563E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 42.491 | TFLOPs: 2.29 |
time (ms) | forward-compute: 10916.09 | backward-compute: 1041.16 | backward-embedding-all-reduce: 0.01 | optimizer: 63.35 | batch-generator: 8491.48
[2023-01-05 02:47:34,420] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:47:34,448] [INFO] [timer.py:207:stop] 0/18, RunningAvgSamplesPerSec=86.88117091869456, CurrSamplesPerSec=66.4052341390625, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       18/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 10503.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.564563E+00 | moe loss: 1.157750E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 48.748 | TFLOPs: 2.63 |
time (ms) | forward-compute: 9367.61 | backward-compute: 1031.00 | backward-embedding-all-reduce: 0.01 | optimizer: 74.20 | batch-generator: 6942.88
[2023-01-05 02:47:45,921] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:47:45,948] [INFO] [timer.py:207:stop] 0/19, RunningAvgSamplesPerSec=83.98918135186574, CurrSamplesPerSec=54.80220649781759, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       19/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 11498.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.519089E+00 | moe loss: 1.145037E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 44.528 | TFLOPs: 2.40 |
time (ms) | forward-compute: 10388.75 | backward-compute: 1015.01 | backward-embedding-all-reduce: 0.01 | optimizer: 62.75 | batch-generator: 7630.79
[2023-01-05 02:48:01,072] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:48:01,099] [INFO] [timer.py:207:stop] 0/20, RunningAvgSamplesPerSec=80.98346665157786, CurrSamplesPerSec=50.35102370330883, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       20/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 15151.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.475386E+00 | moe loss: 1.139734E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.793 | TFLOPs: 1.82 |
time (ms) | forward-compute: 14041.08 | backward-compute: 1017.65 | backward-embedding-all-reduce: 0.01 | optimizer: 62.10 | batch-generator: 11371.66
[2023-01-05 02:48:12,891] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:48:12,918] [INFO] [timer.py:207:stop] 0/21, RunningAvgSamplesPerSec=82.10639023089412, CurrSamplesPerSec=109.41530299429803, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       21/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 11817.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.438346E+00 | moe loss: 1.122137E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 43.326 | TFLOPs: 2.33 |
time (ms) | forward-compute: 10698.38 | backward-compute: 1020.65 | backward-embedding-all-reduce: 0.01 | optimizer: 68.91 | batch-generator: 8309.87
[2023-01-05 02:48:25,181] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:48:25,209] [INFO] [timer.py:207:stop] 0/22, RunningAvgSamplesPerSec=82.62866791738372, CurrSamplesPerSec=93.98793233902754, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       22/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 12291.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.399851E+00 | moe loss: 1.111050E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 41.655 | TFLOPs: 2.24 |
time (ms) | forward-compute: 11182.20 | backward-compute: 1016.23 | backward-embedding-all-reduce: 0.01 | optimizer: 62.42 | batch-generator: 8751.46
[2023-01-05 02:48:38,161] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:48:38,189] [INFO] [timer.py:207:stop] 0/23, RunningAvgSamplesPerSec=81.99399177052778, CurrSamplesPerSec=71.07531054934435, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       23/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 12979.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.365017E+00 | moe loss: 1.120623E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 39.447 | TFLOPs: 2.13 |
time (ms) | forward-compute: 11800.01 | backward-compute: 1086.72 | backward-embedding-all-reduce: 0.01 | optimizer: 61.78 | batch-generator: 8760.66
[2023-01-05 02:48:50,485] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:48:50,512] [INFO] [timer.py:207:stop] 0/24, RunningAvgSamplesPerSec=82.56271577002555, CurrSamplesPerSec=96.63912215458058, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       24/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 12324.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.338491E+00 | moe loss: 1.100490E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 41.544 | TFLOPs: 2.24 |
time (ms) | forward-compute: 11213.54 | backward-compute: 1015.36 | backward-embedding-all-reduce: 0.01 | optimizer: 64.44 | batch-generator: 8908.88
[2023-01-05 02:49:01,664] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:49:01,692] [INFO] [timer.py:207:stop] 0/25, RunningAvgSamplesPerSec=80.4788839788869, CurrSamplesPerSec=51.74604529494145, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       25/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 11181.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.320362E+00 | moe loss: 1.100101E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 45.788 | TFLOPs: 2.47 |
time (ms) | forward-compute: 10050.63 | backward-compute: 1035.73 | backward-embedding-all-reduce: 0.01 | optimizer: 61.22 | batch-generator: 7518.86
[2023-01-05 02:49:11,671] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:49:11,699] [INFO] [timer.py:207:stop] 0/26, RunningAvgSamplesPerSec=81.34176878413855, CurrSamplesPerSec=107.96670366973953, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       26/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 10004.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.300703E+00 | moe loss: 1.075256E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 51.175 | TFLOPs: 2.76 |
time (ms) | forward-compute: 8882.65 | backward-compute: 1027.31 | backward-embedding-all-reduce: 0.01 | optimizer: 63.82 | batch-generator: 7214.12
[2023-01-05 02:49:21,967] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:49:21,994] [INFO] [timer.py:207:stop] 0/27, RunningAvgSamplesPerSec=81.89916530023368, CurrSamplesPerSec=98.01951367710123, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       27/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 10296.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.296659E+00 | moe loss: 1.069557E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 49.726 | TFLOPs: 2.68 |
time (ms) | forward-compute: 9116.62 | backward-compute: 1085.79 | backward-embedding-all-reduce: 0.01 | optimizer: 61.53 | batch-generator: 7087.74
[2023-01-05 02:49:32,704] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:49:32,732] [INFO] [timer.py:207:stop] 0/28, RunningAvgSamplesPerSec=82.87652735173923, CurrSamplesPerSec=118.11546065325415, MemAllocated=6.95GB, MaxMemAllocated=10.79GB
 iteration       28/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 10736.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.295179E+00 | moe loss: 1.057350E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 47.689 | TFLOPs: 2.57 |
time (ms) | forward-compute: 9593.94 | backward-compute: 1049.03 | backward-embedding-all-reduce: 0.01 | optimizer: 63.23 | batch-generator: 7190.31
[2023-01-05 02:49:44,462] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:49:44,490] [INFO] [timer.py:207:stop] 0/29, RunningAvgSamplesPerSec=83.18136344535247, CurrSamplesPerSec=91.97745136872057, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       29/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 11758.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.278821E+00 | moe loss: 1.057390E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 43.542 | TFLOPs: 2.35 |
time (ms) | forward-compute: 10628.32 | backward-compute: 1036.34 | backward-embedding-all-reduce: 0.01 | optimizer: 61.68 | batch-generator: 8586.27
[2023-01-05 02:49:54,596] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:49:54,624] [INFO] [timer.py:207:stop] 0/30, RunningAvgSamplesPerSec=83.35565891668193, CurrSamplesPerSec=88.35429473851208, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       30/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 10133.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.266124E+00 | moe loss: 1.057000E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 50.524 | TFLOPs: 2.72 |
time (ms) | forward-compute: 9017.80 | backward-compute: 1021.30 | backward-embedding-all-reduce: 0.01 | optimizer: 63.17 | batch-generator: 7493.64
[2023-01-05 02:50:07,225] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:50:07,253] [INFO] [timer.py:207:stop] 0/31, RunningAvgSamplesPerSec=83.94907346692747, CurrSamplesPerSec=104.84904005883895, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       31/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 12628.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.265425E+00 | moe loss: 1.030623E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 40.544 | TFLOPs: 2.18 |
time (ms) | forward-compute: 11499.42 | backward-compute: 1030.92 | backward-embedding-all-reduce: 0.01 | optimizer: 67.43 | batch-generator: 9450.07
[2023-01-05 02:50:18,522] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:50:18,549] [INFO] [timer.py:207:stop] 0/32, RunningAvgSamplesPerSec=84.31140557204051, CurrSamplesPerSec=96.37425610624756, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       32/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 11296.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.234035E+00 | moe loss: 1.023212E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 45.322 | TFLOPs: 2.44 |
time (ms) | forward-compute: 10177.74 | backward-compute: 1022.29 | backward-embedding-all-reduce: 0.01 | optimizer: 65.36 | batch-generator: 7840.11
[2023-01-05 02:50:28,993] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:50:29,020] [INFO] [timer.py:207:stop] 0/33, RunningAvgSamplesPerSec=83.60040931102205, CurrSamplesPerSec=66.72076598445989, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       33/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 10470.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.221972E+00 | moe loss: 1.009370E-01 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 48.901 | TFLOPs: 2.63 |
time (ms) | forward-compute: 9357.59 | backward-compute: 1018.41 | backward-embedding-all-reduce: 0.01 | optimizer: 61.51 | batch-generator: 7138.08
[2023-01-05 02:50:39,107] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:50:39,135] [INFO] [timer.py:207:stop] 0/34, RunningAvgSamplesPerSec=84.4041218374031, CurrSamplesPerSec=120.23823016935943, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       34/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 10115.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.202926E+00 | moe loss: 9.877063E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 50.616 | TFLOPs: 2.73 |
time (ms) | forward-compute: 9012.43 | backward-compute: 1009.21 | backward-embedding-all-reduce: 0.01 | optimizer: 61.23 | batch-generator: 6944.04
[2023-01-05 02:50:51,048] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:50:51,075] [INFO] [timer.py:207:stop] 0/35, RunningAvgSamplesPerSec=85.09028888809803, CurrSamplesPerSec=115.00947760289182, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       35/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 11940.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.184354E+00 | moe loss: 9.689996E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 42.880 | TFLOPs: 2.31 |
time (ms) | forward-compute: 10832.39 | backward-compute: 1013.66 | backward-embedding-all-reduce: 0.01 | optimizer: 61.46 | batch-generator: 7624.20
[2023-01-05 02:51:02,026] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:51:02,052] [INFO] [timer.py:207:stop] 0/36, RunningAvgSamplesPerSec=85.46250743013877, CurrSamplesPerSec=99.88080480495677, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       36/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 10976.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.176672E+00 | moe loss: 9.659894E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 46.646 | TFLOPs: 2.51 |
time (ms) | forward-compute: 9868.48 | backward-compute: 1014.22 | backward-embedding-all-reduce: 0.01 | optimizer: 61.70 | batch-generator: 7654.43
[2023-01-05 02:51:12,747] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:51:12,774] [INFO] [timer.py:207:stop] 0/37, RunningAvgSamplesPerSec=85.41210995201199, CurrSamplesPerSec=83.73326678811169, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       37/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 10721.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.171109E+00 | moe loss: 9.481390E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 47.753 | TFLOPs: 2.57 |
time (ms) | forward-compute: 9604.99 | backward-compute: 1017.19 | backward-embedding-all-reduce: 0.01 | optimizer: 67.41 | batch-generator: 7686.25
[2023-01-05 02:51:23,564] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:51:23,591] [INFO] [timer.py:207:stop] 0/38, RunningAvgSamplesPerSec=85.91861307351661, CurrSamplesPerSec=108.422016132713, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       38/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 10816.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.155881E+00 | moe loss: 9.357119E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 47.335 | TFLOPs: 2.55 |
time (ms) | forward-compute: 9712.95 | backward-compute: 1011.12 | backward-embedding-all-reduce: 0.01 | optimizer: 61.41 | batch-generator: 6992.43
[2023-01-05 02:51:35,319] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:51:35,346] [INFO] [timer.py:207:stop] 0/39, RunningAvgSamplesPerSec=85.90753954893964, CurrSamplesPerSec=85.51078491335372, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       39/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 11758.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.158553E+00 | moe loss: 9.267805E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 43.545 | TFLOPs: 2.35 |
time (ms) | forward-compute: 10637.71 | backward-compute: 1025.17 | backward-embedding-all-reduce: 0.01 | optimizer: 61.72 | batch-generator: 8601.60
[2023-01-05 02:51:46,157] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:51:46,185] [INFO] [timer.py:207:stop] 0/40, RunningAvgSamplesPerSec=85.78499578481878, CurrSamplesPerSec=81.48432939119454, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       40/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 10838.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.139441E+00 | moe loss: 9.310564E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 47.239 | TFLOPs: 2.55 |
time (ms) | forward-compute: 9739.29 | backward-compute: 1003.66 | backward-embedding-all-reduce: 0.01 | optimizer: 61.65 | batch-generator: 6969.53
[2023-01-05 02:51:56,532] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:51:56,560] [INFO] [timer.py:207:stop] 0/41, RunningAvgSamplesPerSec=86.26670867310467, CurrSamplesPerSec=109.66804305577175, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       41/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 10374.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.129429E+00 | moe loss: 9.349945E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 49.354 | TFLOPs: 2.66 |
time (ms) | forward-compute: 9265.06 | backward-compute: 1016.12 | backward-embedding-all-reduce: 0.01 | optimizer: 61.44 | batch-generator: 6965.46
[2023-01-05 02:52:06,797] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:52:06,825] [INFO] [timer.py:207:stop] 0/42, RunningAvgSamplesPerSec=86.73553336010126, CurrSamplesPerSec=110.06338305075572, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       42/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 10264.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.127400E+00 | moe loss: 9.251650E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 49.883 | TFLOPs: 2.69 |
time (ms) | forward-compute: 9159.61 | backward-compute: 1013.28 | backward-embedding-all-reduce: 0.01 | optimizer: 61.24 | batch-generator: 7522.35
[2023-01-05 02:52:18,105] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:52:18,132] [INFO] [timer.py:207:stop] 0/43, RunningAvgSamplesPerSec=86.87273962706678, CurrSamplesPerSec=92.74098969619246, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       43/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 11308.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.114199E+00 | moe loss: 9.149932E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 45.277 | TFLOPs: 2.44 |
time (ms) | forward-compute: 10186.70 | backward-compute: 1025.27 | backward-embedding-all-reduce: 0.01 | optimizer: 62.67 | batch-generator: 7839.51
[2023-01-05 02:52:28,137] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:52:28,164] [INFO] [timer.py:207:stop] 0/44, RunningAvgSamplesPerSec=87.11063237119761, CurrSamplesPerSec=98.12790401404901, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       44/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 10030.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.102162E+00 | moe loss: 9.148979E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 51.044 | TFLOPs: 2.75 |
time (ms) | forward-compute: 8925.57 | backward-compute: 1012.88 | backward-embedding-all-reduce: 0.01 | optimizer: 62.42 | batch-generator: 6676.00
[2023-01-05 02:52:39,034] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:52:39,062] [INFO] [timer.py:207:stop] 0/45, RunningAvgSamplesPerSec=86.86241235279046, CurrSamplesPerSec=77.5780174556384, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       45/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 10897.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.095708E+00 | moe loss: 9.019472E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 46.984 | TFLOPs: 2.53 |
time (ms) | forward-compute: 9779.85 | backward-compute: 1024.01 | backward-embedding-all-reduce: 0.01 | optimizer: 61.84 | batch-generator: 7868.04
[2023-01-05 02:52:50,485] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:52:50,513] [INFO] [timer.py:207:stop] 0/46, RunningAvgSamplesPerSec=87.14304126464789, CurrSamplesPerSec=101.20218498477078, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       46/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 11452.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.076481E+00 | moe loss: 8.897910E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 44.707 | TFLOPs: 2.41 |
time (ms) | forward-compute: 10336.79 | backward-compute: 1020.54 | backward-embedding-all-reduce: 0.01 | optimizer: 61.70 | batch-generator: 6902.90
[2023-01-05 02:52:59,924] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:52:59,951] [INFO] [timer.py:207:stop] 0/47, RunningAvgSamplesPerSec=87.49809176424812, CurrSamplesPerSec=106.61021360927593, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       47/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 9438.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.061458E+00 | moe loss: 8.800837E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.246 | TFLOPs: 2.92 |
time (ms) | forward-compute: 8236.81 | backward-compute: 1102.60 | backward-embedding-all-reduce: 0.01 | optimizer: 65.67 | batch-generator: 5928.95
[2023-01-05 02:53:09,260] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:53:09,288] [INFO] [timer.py:207:stop] 0/48, RunningAvgSamplesPerSec=87.98741928423298, CurrSamplesPerSec=117.57673934962897, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       48/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 9337.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.057309E+00 | moe loss: 8.752709E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.833 | TFLOPs: 2.95 |
time (ms) | forward-compute: 8211.62 | backward-compute: 1028.62 | backward-embedding-all-reduce: 0.01 | optimizer: 63.08 | batch-generator: 6699.66
[2023-01-05 02:53:18,528] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:53:18,555] [INFO] [timer.py:207:stop] 0/49, RunningAvgSamplesPerSec=87.08211161288689, CurrSamplesPerSec=59.10697632223932, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       49/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 9267.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.043144E+00 | moe loss: 8.742891E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.244 | TFLOPs: 2.98 |
time (ms) | forward-compute: 8160.76 | backward-compute: 1013.52 | backward-embedding-all-reduce: 0.01 | optimizer: 60.96 | batch-generator: 5767.84
[2023-01-05 02:53:28,984] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:53:29,012] [INFO] [timer.py:207:stop] 0/50, RunningAvgSamplesPerSec=87.25056635765895, CurrSamplesPerSec=95.9766139746947, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       50/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 10456.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.034752E+00 | moe loss: 8.761093E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 48.965 | TFLOPs: 2.64 |
time (ms) | forward-compute: 9338.82 | backward-compute: 1015.22 | backward-embedding-all-reduce: 0.01 | optimizer: 68.58 | batch-generator: 7248.34
[2023-01-05 02:53:37,909] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:53:37,937] [INFO] [timer.py:207:stop] 0/51, RunningAvgSamplesPerSec=87.5105719979539, CurrSamplesPerSec=102.11738372081965, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       51/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 8924.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.029419E+00 | moe loss: 8.678508E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 57.373 | TFLOPs: 3.09 |
time (ms) | forward-compute: 7815.70 | backward-compute: 1012.59 | backward-embedding-all-reduce: 0.01 | optimizer: 63.02 | batch-generator: 5894.22
[2023-01-05 02:53:46,759] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:53:46,787] [INFO] [timer.py:207:stop] 0/52, RunningAvgSamplesPerSec=87.50874458418008, CurrSamplesPerSec=87.41929470675092, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       52/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 8851.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.016397E+00 | moe loss: 8.715008E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 57.845 | TFLOPs: 3.12 |
time (ms) | forward-compute: 7749.77 | backward-compute: 1006.59 | backward-embedding-all-reduce: 0.01 | optimizer: 62.07 | batch-generator: 5586.08
[2023-01-05 02:53:55,397] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:53:55,425] [INFO] [timer.py:207:stop] 0/53, RunningAvgSamplesPerSec=87.81995553619484, CurrSamplesPerSec=106.81315697430814, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       53/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 8637.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.006212E+00 | moe loss: 8.705617E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 59.274 | TFLOPs: 3.19 |
time (ms) | forward-compute: 7521.49 | backward-compute: 1022.95 | backward-embedding-all-reduce: 0.01 | optimizer: 61.10 | batch-generator: 5474.16
[2023-01-05 02:54:04,793] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:54:04,820] [INFO] [timer.py:207:stop] 0/54, RunningAvgSamplesPerSec=88.07186362939969, CurrSamplesPerSec=103.16385898702167, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       54/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 9394.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.002165E+00 | moe loss: 8.905225E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.501 | TFLOPs: 2.94 |
time (ms) | forward-compute: 8181.78 | backward-compute: 1119.81 | backward-embedding-all-reduce: 0.01 | optimizer: 61.71 | batch-generator: 6376.07
[2023-01-05 02:54:13,525] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:54:13,552] [INFO] [timer.py:207:stop] 0/55, RunningAvgSamplesPerSec=88.23375619664728, CurrSamplesPerSec=97.5590050441229, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       55/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 8730.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.980099E+00 | moe loss: 8.753712E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.642 | TFLOPs: 3.16 |
time (ms) | forward-compute: 7620.96 | backward-compute: 1015.18 | backward-embedding-all-reduce: 0.01 | optimizer: 61.92 | batch-generator: 5025.93
[2023-01-05 02:54:21,836] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:54:21,864] [INFO] [timer.py:207:stop] 0/56, RunningAvgSamplesPerSec=88.34220014365229, CurrSamplesPerSec=94.49776565537373, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       56/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 8313.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.965807E+00 | moe loss: 8.653060E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.588 | TFLOPs: 3.32 |
time (ms) | forward-compute: 7198.15 | backward-compute: 1021.02 | backward-embedding-all-reduce: 0.01 | optimizer: 61.37 | batch-generator: 4892.14
[2023-01-05 02:54:32,283] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:54:32,311] [INFO] [timer.py:207:stop] 0/57, RunningAvgSamplesPerSec=88.3215178775375, CurrSamplesPerSec=87.21887681863404, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       57/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 10447.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.953788E+00 | moe loss: 8.671504E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 49.006 | TFLOPs: 2.64 |
time (ms) | forward-compute: 9327.32 | backward-compute: 1020.13 | backward-embedding-all-reduce: 0.01 | optimizer: 62.54 | batch-generator: 7071.36
[2023-01-05 02:54:40,576] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:54:40,604] [INFO] [timer.py:207:stop] 0/58, RunningAvgSamplesPerSec=88.32599980167114, CurrSamplesPerSec=88.57320809776874, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       58/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 8291.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.939668E+00 | moe loss: 8.569898E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.749 | TFLOPs: 3.33 |
time (ms) | forward-compute: 7171.91 | backward-compute: 1019.04 | backward-embedding-all-reduce: 0.01 | optimizer: 69.68 | batch-generator: 5189.49
[2023-01-05 02:54:48,720] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:54:48,748] [INFO] [timer.py:207:stop] 0/59, RunningAvgSamplesPerSec=88.42206810408501, CurrSamplesPerSec=94.15705525173689, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       59/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 8142.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.934999E+00 | moe loss: 8.603545E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 62.878 | TFLOPs: 3.39 |
time (ms) | forward-compute: 7031.68 | backward-compute: 1014.00 | backward-embedding-all-reduce: 0.01 | optimizer: 62.46 | batch-generator: 5265.50
[2023-01-05 02:54:58,199] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:54:58,227] [INFO] [timer.py:207:stop] 0/60, RunningAvgSamplesPerSec=88.12229286863428, CurrSamplesPerSec=73.85091733338763, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       60/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 9480.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.919693E+00 | moe loss: 8.581449E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.007 | TFLOPs: 2.91 |
time (ms) | forward-compute: 8355.40 | backward-compute: 1021.32 | backward-embedding-all-reduce: 0.01 | optimizer: 69.54 | batch-generator: 6024.90
[2023-01-05 02:55:06,223] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:55:06,250] [INFO] [timer.py:207:stop] 0/61, RunningAvgSamplesPerSec=88.18968231251188, CurrSamplesPerSec=92.28280600309058, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       61/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 8015.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.905087E+00 | moe loss: 8.491562E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.879 | TFLOPs: 3.44 |
time (ms) | forward-compute: 6894.30 | backward-compute: 1025.54 | backward-embedding-all-reduce: 0.01 | optimizer: 66.57 | batch-generator: 4976.92
[2023-01-05 02:55:14,307] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:55:14,335] [INFO] [timer.py:207:stop] 0/62, RunningAvgSamplesPerSec=88.45104973310686, CurrSamplesPerSec=107.19495243952112, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       62/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 8092.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.891708E+00 | moe loss: 8.561699E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.267 | TFLOPs: 3.41 |
time (ms) | forward-compute: 6968.34 | backward-compute: 1022.49 | backward-embedding-all-reduce: 0.01 | optimizer: 66.23 | batch-generator: 4868.77
[2023-01-05 02:55:22,267] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:55:22,294] [INFO] [timer.py:207:stop] 0/63, RunningAvgSamplesPerSec=88.71628783023786, CurrSamplesPerSec=108.18029871389648, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       63/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 7958.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.891006E+00 | moe loss: 8.374143E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 64.332 | TFLOPs: 3.47 |
time (ms) | forward-compute: 6837.26 | backward-compute: 1022.07 | backward-embedding-all-reduce: 0.01 | optimizer: 66.05 | batch-generator: 5297.50
[2023-01-05 02:55:31,390] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:55:31,418] [INFO] [timer.py:207:stop] 0/64, RunningAvgSamplesPerSec=88.9016536451789, CurrSamplesPerSec=101.88773181711214, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       64/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 9125.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.879695E+00 | moe loss: 8.584320E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.104 | TFLOPs: 3.02 |
time (ms) | forward-compute: 8006.42 | backward-compute: 1023.62 | backward-embedding-all-reduce: 0.01 | optimizer: 61.09 | batch-generator: 5474.37
[2023-01-05 02:55:40,272] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:55:40,300] [INFO] [timer.py:207:stop] 0/65, RunningAvgSamplesPerSec=89.1256529066621, CurrSamplesPerSec=105.62627873942007, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       65/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 8879.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.872166E+00 | moe loss: 8.339394E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 57.663 | TFLOPs: 3.11 |
time (ms) | forward-compute: 7768.81 | backward-compute: 1015.17 | backward-embedding-all-reduce: 0.01 | optimizer: 65.07 | batch-generator: 5968.44
[2023-01-05 02:55:49,175] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:55:49,203] [INFO] [timer.py:207:stop] 0/66, RunningAvgSamplesPerSec=89.50235169480246, CurrSamplesPerSec=121.98367979423654, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       66/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 8902.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.844620E+00 | moe loss: 8.676701E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 57.512 | TFLOPs: 3.10 |
time (ms) | forward-compute: 7780.70 | backward-compute: 1021.04 | backward-embedding-all-reduce: 0.01 | optimizer: 64.99 | batch-generator: 5005.51
[2023-01-05 02:55:58,172] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:55:58,200] [INFO] [timer.py:207:stop] 0/67, RunningAvgSamplesPerSec=89.85393377495502, CurrSamplesPerSec=120.0299480773599, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       67/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 8998.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.845922E+00 | moe loss: 8.346822E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.897 | TFLOPs: 3.07 |
time (ms) | forward-compute: 7875.54 | backward-compute: 1027.92 | backward-embedding-all-reduce: 0.01 | optimizer: 61.81 | batch-generator: 5729.88
[2023-01-05 02:56:07,109] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:56:07,137] [INFO] [timer.py:207:stop] 0/68, RunningAvgSamplesPerSec=89.87275347159084, CurrSamplesPerSec=91.11317721683973, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       68/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 8938.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.831582E+00 | moe loss: 8.232655E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 57.283 | TFLOPs: 3.09 |
time (ms) | forward-compute: 7818.61 | backward-compute: 1025.14 | backward-embedding-all-reduce: 0.01 | optimizer: 61.50 | batch-generator: 5815.18
[2023-01-05 02:56:16,197] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:56:16,224] [INFO] [timer.py:207:stop] 0/69, RunningAvgSamplesPerSec=89.96975781027923, CurrSamplesPerSec=96.87054766491259, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       69/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 9087.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.820472E+00 | moe loss: 8.558235E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.340 | TFLOPs: 3.04 |
time (ms) | forward-compute: 7966.59 | backward-compute: 1019.78 | backward-embedding-all-reduce: 0.01 | optimizer: 69.00 | batch-generator: 5694.54
[2023-01-05 02:56:26,935] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:56:26,963] [INFO] [timer.py:207:stop] 0/70, RunningAvgSamplesPerSec=90.06576400400876, CurrSamplesPerSec=97.00087050757205, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       70/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 10736.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.820245E+00 | moe loss: 8.290331E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 47.690 | TFLOPs: 2.57 |
time (ms) | forward-compute: 9611.29 | backward-compute: 1031.54 | backward-embedding-all-reduce: 0.01 | optimizer: 63.01 | batch-generator: 6904.92
[2023-01-05 02:56:34,983] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:56:35,011] [INFO] [timer.py:207:stop] 0/71, RunningAvgSamplesPerSec=89.9921911928038, CurrSamplesPerSec=85.25638997042147, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       71/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 8049.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.787910E+00 | moe loss: 8.209885E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.607 | TFLOPs: 3.43 |
time (ms) | forward-compute: 6786.57 | backward-compute: 1166.95 | backward-embedding-all-reduce: 0.01 | optimizer: 61.83 | batch-generator: 4660.87
[2023-01-05 02:56:43,451] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:56:43,478] [INFO] [timer.py:207:stop] 0/72, RunningAvgSamplesPerSec=90.0311229465968, CurrSamplesPerSec=92.80126558122426, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       72/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 8467.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.791913E+00 | moe loss: 8.399555E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 60.466 | TFLOPs: 3.26 |
time (ms) | forward-compute: 7358.92 | backward-compute: 1013.67 | backward-embedding-all-reduce: 0.01 | optimizer: 62.32 | batch-generator: 5014.37
[2023-01-05 02:56:51,060] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:56:51,086] [INFO] [timer.py:207:stop] 0/73, RunningAvgSamplesPerSec=90.07775913202984, CurrSamplesPerSec=93.46687256374946, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       73/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 7607.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.781591E+00 | moe loss: 8.181085E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 67.306 | TFLOPs: 3.63 |
time (ms) | forward-compute: 6480.35 | backward-compute: 1025.81 | backward-embedding-all-reduce: 0.01 | optimizer: 69.55 | batch-generator: 4779.16
[2023-01-05 02:57:00,320] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:57:00,348] [INFO] [timer.py:207:stop] 0/74, RunningAvgSamplesPerSec=89.8513726897817, CurrSamplesPerSec=76.24605818191941, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       74/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 9261.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.766820E+00 | moe loss: 8.108826E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.282 | TFLOPs: 2.98 |
time (ms) | forward-compute: 8150.40 | backward-compute: 1017.43 | backward-embedding-all-reduce: 0.01 | optimizer: 62.47 | batch-generator: 6050.38
[2023-01-05 02:57:08,706] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:57:08,733] [INFO] [timer.py:207:stop] 0/75, RunningAvgSamplesPerSec=90.13086570914297, CurrSamplesPerSec=116.14267567071757, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       75/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 8387.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.761233E+00 | moe loss: 8.325271E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.045 | TFLOPs: 3.29 |
time (ms) | forward-compute: 7268.86 | backward-compute: 1021.93 | backward-embedding-all-reduce: 0.01 | optimizer: 61.78 | batch-generator: 5285.94
[2023-01-05 02:57:17,845] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:57:17,873] [INFO] [timer.py:207:stop] 0/76, RunningAvgSamplesPerSec=90.23428895701961, CurrSamplesPerSec=98.48388456601089, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       76/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 9137.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.745657E+00 | moe loss: 8.063538E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.034 | TFLOPs: 3.02 |
time (ms) | forward-compute: 8025.34 | backward-compute: 1018.77 | backward-embedding-all-reduce: 0.01 | optimizer: 63.29 | batch-generator: 6366.38
[2023-01-05 02:57:26,526] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:57:26,553] [INFO] [timer.py:207:stop] 0/77, RunningAvgSamplesPerSec=90.31240762761192, CurrSamplesPerSec=96.4942267480169, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       77/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 8680.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.729081E+00 | moe loss: 8.238387E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.982 | TFLOPs: 3.18 |
time (ms) | forward-compute: 7568.00 | backward-compute: 1018.51 | backward-embedding-all-reduce: 0.01 | optimizer: 61.36 | batch-generator: 5571.99
[2023-01-05 02:57:34,371] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:57:34,398] [INFO] [timer.py:207:stop] 0/78, RunningAvgSamplesPerSec=90.44326065198972, CurrSamplesPerSec=101.46966272597548, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       78/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 7844.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.726219E+00 | moe loss: 8.284684E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 65.268 | TFLOPs: 3.52 |
time (ms) | forward-compute: 6618.13 | backward-compute: 1125.94 | backward-embedding-all-reduce: 0.01 | optimizer: 67.52 | batch-generator: 4666.61
[2023-01-05 02:57:43,489] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:57:43,517] [INFO] [timer.py:207:stop] 0/79, RunningAvgSamplesPerSec=90.12796304158468, CurrSamplesPerSec=71.25041931066627, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       79/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 9118.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.716870E+00 | moe loss: 8.034957E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.151 | TFLOPs: 3.03 |
time (ms) | forward-compute: 8008.83 | backward-compute: 1014.78 | backward-embedding-all-reduce: 0.01 | optimizer: 61.83 | batch-generator: 6184.38
[2023-01-05 02:57:54,078] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:57:54,106] [INFO] [timer.py:207:stop] 0/80, RunningAvgSamplesPerSec=90.07878498464979, CurrSamplesPerSec=86.44674051240301, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       80/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 10590.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.703070E+00 | moe loss: 8.291703E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 48.345 | TFLOPs: 2.60 |
time (ms) | forward-compute: 9472.19 | backward-compute: 1022.96 | backward-embedding-all-reduce: 0.01 | optimizer: 62.41 | batch-generator: 6881.39
[2023-01-05 02:58:04,685] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:58:04,712] [INFO] [timer.py:207:stop] 0/81, RunningAvgSamplesPerSec=90.08561907356606, CurrSamplesPerSec=90.62189195138123, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       81/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 10606.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.702057E+00 | moe loss: 8.239547E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 48.271 | TFLOPs: 2.60 |
time (ms) | forward-compute: 9494.53 | backward-compute: 1013.06 | backward-embedding-all-reduce: 0.01 | optimizer: 67.44 | batch-generator: 7018.16
[2023-01-05 02:58:13,989] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:58:14,016] [INFO] [timer.py:207:stop] 0/82, RunningAvgSamplesPerSec=90.15570720786913, CurrSamplesPerSec=96.05986702260903, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       82/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 9302.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.698065E+00 | moe loss: 8.013882E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.037 | TFLOPs: 2.97 |
time (ms) | forward-compute: 8171.59 | backward-compute: 1030.24 | backward-embedding-all-reduce: 0.01 | optimizer: 70.11 | batch-generator: 6295.58
[2023-01-05 02:58:22,367] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:58:22,395] [INFO] [timer.py:207:stop] 0/83, RunningAvgSamplesPerSec=90.35158166874065, CurrSamplesPerSec=109.35931645603655, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       83/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 8377.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.682506E+00 | moe loss: 8.326364E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.113 | TFLOPs: 3.29 |
time (ms) | forward-compute: 7253.73 | backward-compute: 1019.95 | backward-embedding-all-reduce: 0.01 | optimizer: 71.61 | batch-generator: 5237.75
[2023-01-05 02:58:31,837] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:58:31,864] [INFO] [timer.py:207:stop] 0/84, RunningAvgSamplesPerSec=90.57854109543206, CurrSamplesPerSec=113.71622737360802, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       84/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 9469.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.654725E+00 | moe loss: 8.148974E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.067 | TFLOPs: 2.91 |
time (ms) | forward-compute: 8353.99 | backward-compute: 1021.25 | backward-embedding-all-reduce: 0.01 | optimizer: 61.90 | batch-generator: 6175.23
[2023-01-05 02:58:39,797] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:58:39,824] [INFO] [timer.py:207:stop] 0/85, RunningAvgSamplesPerSec=90.74394462677087, CurrSamplesPerSec=106.7247410060301, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       85/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 7959.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.650638E+00 | moe loss: 8.070835E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 64.328 | TFLOPs: 3.47 |
time (ms) | forward-compute: 6846.93 | backward-compute: 1007.84 | backward-embedding-all-reduce: 0.01 | optimizer: 72.24 | batch-generator: 4434.76
[2023-01-05 02:58:48,974] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:58:49,001] [INFO] [timer.py:207:stop] 0/86, RunningAvgSamplesPerSec=90.85482818029001, CurrSamplesPerSec=101.1094414102226, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       86/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 9178.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.648679E+00 | moe loss: 8.162300E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.781 | TFLOPs: 3.01 |
time (ms) | forward-compute: 8060.05 | backward-compute: 1022.43 | backward-embedding-all-reduce: 0.01 | optimizer: 62.34 | batch-generator: 6175.54
[2023-01-05 02:58:57,062] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:58:57,089] [INFO] [timer.py:207:stop] 0/87, RunningAvgSamplesPerSec=90.63928416229484, CurrSamplesPerSec=75.57798144473401, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       87/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 8083.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.615866E+00 | moe loss: 8.038779E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.342 | TFLOPs: 3.41 |
time (ms) | forward-compute: 6873.44 | backward-compute: 1106.89 | backward-embedding-all-reduce: 0.01 | optimizer: 74.94 | batch-generator: 5025.19
[2023-01-05 02:59:05,154] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:59:05,182] [INFO] [timer.py:207:stop] 0/88, RunningAvgSamplesPerSec=90.67939188476046, CurrSamplesPerSec=94.22335356434195, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       88/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 8097.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.615705E+00 | moe loss: 8.154254E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.233 | TFLOPs: 3.41 |
time (ms) | forward-compute: 6955.56 | backward-compute: 1035.03 | backward-embedding-all-reduce: 0.01 | optimizer: 70.27 | batch-generator: 4652.10
[2023-01-05 02:59:13,345] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:59:13,372] [INFO] [timer.py:207:stop] 0/89, RunningAvgSamplesPerSec=90.57822330593072, CurrSamplesPerSec=82.64829267917209, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       89/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 8189.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.598149E+00 | moe loss: 8.138123E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 62.517 | TFLOPs: 3.37 |
time (ms) | forward-compute: 7065.85 | backward-compute: 1025.40 | backward-embedding-all-reduce: 0.01 | optimizer: 62.24 | batch-generator: 4446.71
[2023-01-05 02:59:23,174] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:59:23,202] [INFO] [timer.py:207:stop] 0/90, RunningAvgSamplesPerSec=90.52050908777076, CurrSamplesPerSec=85.76612711008245, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       90/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 9831.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.582105E+00 | moe loss: 7.895005E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 52.079 | TFLOPs: 2.81 |
time (ms) | forward-compute: 8573.88 | backward-compute: 1161.44 | backward-embedding-all-reduce: 0.01 | optimizer: 61.68 | batch-generator: 6799.50
[2023-01-05 02:59:30,815] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:59:30,843] [INFO] [timer.py:207:stop] 0/91, RunningAvgSamplesPerSec=90.72153894832356, CurrSamplesPerSec=112.7581116099051, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       91/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 7641.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.580676E+00 | moe loss: 8.733778E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 67.007 | TFLOPs: 3.61 |
time (ms) | forward-compute: 6529.45 | backward-compute: 1017.79 | backward-embedding-all-reduce: 0.01 | optimizer: 61.90 | batch-generator: 4105.08
[2023-01-05 02:59:40,104] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:59:40,132] [INFO] [timer.py:207:stop] 0/92, RunningAvgSamplesPerSec=90.79090246349107, CurrSamplesPerSec=97.42007219891684, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       92/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 9287.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.562934E+00 | moe loss: 8.232383E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.130 | TFLOPs: 2.97 |
time (ms) | forward-compute: 8168.78 | backward-compute: 1026.32 | backward-embedding-all-reduce: 0.01 | optimizer: 61.89 | batch-generator: 6073.87
[2023-01-05 02:59:48,197] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:59:48,225] [INFO] [timer.py:207:stop] 0/93, RunningAvgSamplesPerSec=90.9549446221579, CurrSamplesPerSec=108.61759315945214, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       93/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 8094.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.543959E+00 | moe loss: 8.050662E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.252 | TFLOPs: 3.41 |
time (ms) | forward-compute: 6962.76 | backward-compute: 1033.88 | backward-embedding-all-reduce: 0.01 | optimizer: 63.82 | batch-generator: 4949.43
[2023-01-05 02:59:57,675] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 02:59:57,702] [INFO] [timer.py:207:stop] 0/94, RunningAvgSamplesPerSec=91.18556332633364, CurrSamplesPerSec=118.5356405576798, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       94/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 9475.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.547655E+00 | moe loss: 8.034463E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.032 | TFLOPs: 2.91 |
time (ms) | forward-compute: 8360.52 | backward-compute: 1018.85 | backward-embedding-all-reduce: 0.01 | optimizer: 66.60 | batch-generator: 5126.67
[2023-01-05 03:00:05,452] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:00:05,479] [INFO] [timer.py:207:stop] 0/95, RunningAvgSamplesPerSec=91.36158747703375, CurrSamplesPerSec=111.09092940698356, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       95/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 7776.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.533000E+00 | moe loss: 7.965124E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 65.840 | TFLOPs: 3.55 |
time (ms) | forward-compute: 6663.51 | backward-compute: 1019.90 | backward-embedding-all-reduce: 0.01 | optimizer: 61.21 | batch-generator: 4390.63
[2023-01-05 03:00:14,418] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:00:14,444] [INFO] [timer.py:207:stop] 0/96, RunningAvgSamplesPerSec=91.24501734271065, CurrSamplesPerSec=81.56630869134663, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       96/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 8965.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.553053E+00 | moe loss: 8.369019E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 57.110 | TFLOPs: 3.08 |
time (ms) | forward-compute: 7840.26 | backward-compute: 1017.69 | backward-embedding-all-reduce: 0.01 | optimizer: 74.80 | batch-generator: 5840.44
[2023-01-05 03:00:22,242] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:00:22,270] [INFO] [timer.py:207:stop] 0/97, RunningAvgSamplesPerSec=91.36142212423262, CurrSamplesPerSec=103.81029202649674, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       97/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 7827.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.507609E+00 | moe loss: 8.154097E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 65.412 | TFLOPs: 3.52 |
time (ms) | forward-compute: 6718.83 | backward-compute: 1011.40 | backward-embedding-all-reduce: 0.01 | optimizer: 62.34 | batch-generator: 4590.07
[2023-01-05 03:00:30,576] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:00:30,603] [INFO] [timer.py:207:stop] 0/98, RunningAvgSamplesPerSec=91.56896376072568, CurrSamplesPerSec=116.76843666671017, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       98/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 8335.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.520675E+00 | moe loss: 8.084203E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.428 | TFLOPs: 3.31 |
time (ms) | forward-compute: 7218.48 | backward-compute: 1018.14 | backward-embedding-all-reduce: 0.01 | optimizer: 62.82 | batch-generator: 4906.63
[2023-01-05 03:00:38,500] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:00:38,528] [INFO] [timer.py:207:stop] 0/99, RunningAvgSamplesPerSec=91.7485833266928, CurrSamplesPerSec=113.03421233476615, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration       99/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 7922.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.498850E+00 | moe loss: 8.157234E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 64.625 | TFLOPs: 3.48 |
time (ms) | forward-compute: 6803.56 | backward-compute: 1019.53 | backward-embedding-all-reduce: 0.01 | optimizer: 67.83 | batch-generator: 5124.55
[2023-01-05 03:00:47,896] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:00:47,924] [INFO] [timer.py:207:stop] 0/100, RunningAvgSamplesPerSec=91.78829148213033, CurrSamplesPerSec=95.81050585476918, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      100/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 9398.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.497397E+00 | moe loss: 8.201222E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 54.479 | TFLOPs: 2.94 |
time (ms) | forward-compute: 8285.80 | backward-compute: 1013.86 | backward-embedding-all-reduce: 0.01 | optimizer: 62.26 | batch-generator: 6132.95
[2023-01-05 03:00:55,785] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:00:55,813] [INFO] [timer.py:207:stop] 0/101, RunningAvgSamplesPerSec=91.9076770304504, CurrSamplesPerSec=105.33407209172742, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      101/     200 | consumed samples:        51712 | consumed tokens:     52953088 | elapsed time per iteration (ms): 7888.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.483754E+00 | moe loss: 8.122608E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 64.908 | TFLOPs: 3.50 |
time (ms) | forward-compute: 6767.67 | backward-compute: 1021.24 | backward-embedding-all-reduce: 0.01 | optimizer: 62.70 | batch-generator: 4721.97
[2023-01-05 03:01:04,081] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:01:04,109] [INFO] [timer.py:207:stop] 0/102, RunningAvgSamplesPerSec=91.87141820880373, CurrSamplesPerSec=88.41808721338323, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      102/     200 | consumed samples:        52224 | consumed tokens:     53477376 | elapsed time per iteration (ms): 8288.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.478307E+00 | moe loss: 8.028922E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.771 | TFLOPs: 3.33 |
time (ms) | forward-compute: 7183.77 | backward-compute: 1018.39 | backward-embedding-all-reduce: 0.01 | optimizer: 62.00 | batch-generator: 4700.91
[2023-01-05 03:01:16,375] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:01:16,402] [INFO] [timer.py:207:stop] 0/103, RunningAvgSamplesPerSec=91.88396913280752, CurrSamplesPerSec=93.1566191806493, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      103/     200 | consumed samples:        52736 | consumed tokens:     54001664 | elapsed time per iteration (ms): 12299.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.453692E+00 | moe loss: 8.044145E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 41.628 | TFLOPs: 2.24 |
time (ms) | forward-compute: 11184.98 | backward-compute: 1024.60 | backward-embedding-all-reduce: 0.01 | optimizer: 60.15 | batch-generator: 8272.80
[2023-01-05 03:01:24,034] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:01:24,062] [INFO] [timer.py:207:stop] 0/104, RunningAvgSamplesPerSec=92.11272008600143, CurrSamplesPerSec=123.05419032648602, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      104/     200 | consumed samples:        53248 | consumed tokens:     54525952 | elapsed time per iteration (ms): 7658.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.460456E+00 | moe loss: 8.118058E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 66.857 | TFLOPs: 3.60 |
time (ms) | forward-compute: 6533.53 | backward-compute: 1023.96 | backward-embedding-all-reduce: 0.01 | optimizer: 68.66 | batch-generator: 4795.49
[2023-01-05 03:01:31,997] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:01:32,025] [INFO] [timer.py:207:stop] 0/105, RunningAvgSamplesPerSec=92.23173053982545, CurrSamplesPerSec=106.23142966480971, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      105/     200 | consumed samples:        53760 | consumed tokens:     55050240 | elapsed time per iteration (ms): 7964.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.448822E+00 | moe loss: 8.050142E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 64.286 | TFLOPs: 3.46 |
time (ms) | forward-compute: 6844.62 | backward-compute: 1011.70 | backward-embedding-all-reduce: 0.01 | optimizer: 74.09 | batch-generator: 4604.09
[2023-01-05 03:01:39,821] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:01:39,849] [INFO] [timer.py:207:stop] 0/106, RunningAvgSamplesPerSec=92.4626118127986, CurrSamplesPerSec=124.58534941713917, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      106/     200 | consumed samples:        54272 | consumed tokens:     55574528 | elapsed time per iteration (ms): 7824.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.441534E+00 | moe loss: 8.074582E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 65.436 | TFLOPs: 3.53 |
time (ms) | forward-compute: 6716.73 | backward-compute: 1010.36 | backward-embedding-all-reduce: 0.01 | optimizer: 63.17 | batch-generator: 4371.56
[2023-01-05 03:01:47,614] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:01:47,641] [INFO] [timer.py:207:stop] 0/107, RunningAvgSamplesPerSec=92.41593128347279, CurrSamplesPerSec=87.80567024101501, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      107/     200 | consumed samples:        54784 | consumed tokens:     56098816 | elapsed time per iteration (ms): 7789.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.442265E+00 | moe loss: 8.120437E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 65.728 | TFLOPs: 3.54 |
time (ms) | forward-compute: 6661.48 | backward-compute: 1031.00 | backward-embedding-all-reduce: 0.01 | optimizer: 67.68 | batch-generator: 4813.95
[2023-01-05 03:01:55,927] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:01:55,955] [INFO] [timer.py:207:stop] 0/108, RunningAvgSamplesPerSec=92.45484069995145, CurrSamplesPerSec=96.7310924521156, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      108/     200 | consumed samples:        55296 | consumed tokens:     56623104 | elapsed time per iteration (ms): 8316.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.416512E+00 | moe loss: 8.152302E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.568 | TFLOPs: 3.32 |
time (ms) | forward-compute: 7193.72 | backward-compute: 1025.87 | backward-embedding-all-reduce: 0.01 | optimizer: 63.22 | batch-generator: 5050.11
[2023-01-05 03:02:04,690] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:02:04,717] [INFO] [timer.py:207:stop] 0/109, RunningAvgSamplesPerSec=92.49297785686859, CurrSamplesPerSec=96.72210013450668, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      109/     200 | consumed samples:        55808 | consumed tokens:     57147392 | elapsed time per iteration (ms): 8761.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.417212E+00 | moe loss: 8.043150E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.435 | TFLOPs: 3.15 |
time (ms) | forward-compute: 7635.31 | backward-compute: 1022.00 | backward-embedding-all-reduce: 0.01 | optimizer: 71.00 | batch-generator: 4910.37
[2023-01-05 03:02:12,905] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:02:12,933] [INFO] [timer.py:207:stop] 0/110, RunningAvgSamplesPerSec=92.72592465318569, CurrSamplesPerSec=126.93196040472897, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      110/     200 | consumed samples:        56320 | consumed tokens:     57671680 | elapsed time per iteration (ms): 8215.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.404518E+00 | moe loss: 8.021137E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 62.319 | TFLOPs: 3.36 |
time (ms) | forward-compute: 7107.53 | backward-compute: 1012.78 | backward-embedding-all-reduce: 0.01 | optimizer: 61.61 | batch-generator: 4832.16
[2023-01-05 03:02:20,128] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:02:20,155] [INFO] [timer.py:207:stop] 0/111, RunningAvgSamplesPerSec=92.74478654140123, CurrSamplesPerSec=94.8280518306038, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      111/     200 | consumed samples:        56832 | consumed tokens:     58195968 | elapsed time per iteration (ms): 7224.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.406250E+00 | moe loss: 8.105503E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.869 | TFLOPs: 3.82 |
time (ms) | forward-compute: 6097.77 | backward-compute: 1028.44 | backward-embedding-all-reduce: 0.01 | optimizer: 62.95 | batch-generator: 4156.19
[2023-01-05 03:02:29,335] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:02:29,362] [INFO] [timer.py:207:stop] 0/112, RunningAvgSamplesPerSec=92.83650143367295, CurrSamplesPerSec=104.05225182900783, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      112/     200 | consumed samples:        57344 | consumed tokens:     58720256 | elapsed time per iteration (ms): 9205.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.388675E+00 | moe loss: 8.223876E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.620 | TFLOPs: 3.00 |
time (ms) | forward-compute: 8074.43 | backward-compute: 1023.04 | backward-embedding-all-reduce: 0.01 | optimizer: 74.70 | batch-generator: 5923.29
[2023-01-05 03:02:36,730] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:02:36,756] [INFO] [timer.py:207:stop] 0/113, RunningAvgSamplesPerSec=92.78217378698825, CurrSamplesPerSec=87.1708407237991, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      113/     200 | consumed samples:        57856 | consumed tokens:     59244544 | elapsed time per iteration (ms): 7393.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.385916E+00 | moe loss: 8.167897E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 69.254 | TFLOPs: 3.73 |
time (ms) | forward-compute: 6277.89 | backward-compute: 1015.29 | backward-embedding-all-reduce: 0.01 | optimizer: 66.01 | batch-generator: 4315.51
[2023-01-05 03:02:44,998] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:02:45,026] [INFO] [timer.py:207:stop] 0/114, RunningAvgSamplesPerSec=92.80440089012276, CurrSamplesPerSec=95.33961530360982, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      114/     200 | consumed samples:        58368 | consumed tokens:     59768832 | elapsed time per iteration (ms): 8270.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.388936E+00 | moe loss: 8.035883E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.905 | TFLOPs: 3.34 |
time (ms) | forward-compute: 7149.83 | backward-compute: 1025.32 | backward-embedding-all-reduce: 0.01 | optimizer: 60.88 | batch-generator: 4821.40
[2023-01-05 03:02:52,281] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:02:52,308] [INFO] [timer.py:207:stop] 0/115, RunningAvgSamplesPerSec=92.81980301925498, CurrSamplesPerSec=94.57780533779618, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      115/     200 | consumed samples:        58880 | consumed tokens:     60293120 | elapsed time per iteration (ms): 7282.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.388961E+00 | moe loss: 8.023625E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.303 | TFLOPs: 3.79 |
time (ms) | forward-compute: 6157.24 | backward-compute: 1020.97 | backward-embedding-all-reduce: 0.01 | optimizer: 69.40 | batch-generator: 4095.33
[2023-01-05 03:02:59,626] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:02:59,654] [INFO] [timer.py:207:stop] 0/116, RunningAvgSamplesPerSec=92.87455251435436, CurrSamplesPerSec=99.5069628992588, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      116/     200 | consumed samples:        59392 | consumed tokens:     60817408 | elapsed time per iteration (ms): 7343.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.367221E+00 | moe loss: 8.121065E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 69.726 | TFLOPs: 3.76 |
time (ms) | forward-compute: 6128.64 | backward-compute: 1110.35 | backward-embedding-all-reduce: 0.01 | optimizer: 73.51 | batch-generator: 4398.17
[2023-01-05 03:03:07,046] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:03:07,073] [INFO] [timer.py:207:stop] 0/117, RunningAvgSamplesPerSec=93.00437556319928, CurrSamplesPerSec=110.63426439580404, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      117/     200 | consumed samples:        59904 | consumed tokens:     61341696 | elapsed time per iteration (ms): 7419.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.366245E+00 | moe loss: 8.131285E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 69.004 | TFLOPs: 3.72 |
time (ms) | forward-compute: 6290.56 | backward-compute: 1035.77 | backward-embedding-all-reduce: 0.01 | optimizer: 61.50 | batch-generator: 4913.78
[2023-01-05 03:03:14,943] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:03:14,971] [INFO] [timer.py:207:stop] 0/118, RunningAvgSamplesPerSec=92.91346378328316, CurrSamplesPerSec=83.52428932053999, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      118/     200 | consumed samples:        60416 | consumed tokens:     61865984 | elapsed time per iteration (ms): 7898.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.364999E+00 | moe loss: 8.117753E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 64.822 | TFLOPs: 3.49 |
time (ms) | forward-compute: 6790.27 | backward-compute: 1014.32 | backward-embedding-all-reduce: 0.01 | optimizer: 61.05 | batch-generator: 4502.56
[2023-01-05 03:03:25,600] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:03:25,628] [INFO] [timer.py:207:stop] 0/119, RunningAvgSamplesPerSec=93.05168239730159, CurrSamplesPerSec=112.45762700503396, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      119/     200 | consumed samples:        60928 | consumed tokens:     62390272 | elapsed time per iteration (ms): 10657.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.354130E+00 | moe loss: 8.057159E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 48.040 | TFLOPs: 2.59 |
time (ms) | forward-compute: 9542.08 | backward-compute: 1019.92 | backward-embedding-all-reduce: 0.01 | optimizer: 60.82 | batch-generator: 7015.64
[2023-01-05 03:03:35,901] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:03:35,928] [INFO] [timer.py:207:stop] 0/120, RunningAvgSamplesPerSec=93.23125098036954, CurrSamplesPerSec=120.42011198847278, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      120/     200 | consumed samples:        61440 | consumed tokens:     62914560 | elapsed time per iteration (ms): 10298.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.349845E+00 | moe loss: 8.162502E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 49.718 | TFLOPs: 2.68 |
time (ms) | forward-compute: 9085.92 | backward-compute: 1110.82 | backward-embedding-all-reduce: 0.01 | optimizer: 70.97 | batch-generator: 6951.09
[2023-01-05 03:03:44,938] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:03:44,966] [INFO] [timer.py:207:stop] 0/121, RunningAvgSamplesPerSec=93.18760195769985, CurrSamplesPerSec=88.30895102231085, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      121/     200 | consumed samples:        61952 | consumed tokens:     63438848 | elapsed time per iteration (ms): 9040.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.326515E+00 | moe loss: 8.155046E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.636 | TFLOPs: 3.05 |
time (ms) | forward-compute: 7932.96 | backward-compute: 1010.99 | backward-embedding-all-reduce: 0.01 | optimizer: 61.81 | batch-generator: 5724.45
[2023-01-05 03:03:55,474] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:03:55,500] [INFO] [timer.py:207:stop] 0/122, RunningAvgSamplesPerSec=92.82335588219162, CurrSamplesPerSec=63.35459727469606, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      122/     200 | consumed samples:        62464 | consumed tokens:     63963136 | elapsed time per iteration (ms): 10532.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.341628E+00 | moe loss: 8.127943E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 48.614 | TFLOPs: 2.62 |
time (ms) | forward-compute: 9418.71 | backward-compute: 1016.37 | backward-embedding-all-reduce: 0.01 | optimizer: 66.05 | batch-generator: 6783.57
[2023-01-05 03:04:05,216] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:04:05,243] [INFO] [timer.py:207:stop] 0/123, RunningAvgSamplesPerSec=92.97732557356329, CurrSamplesPerSec=116.08361067482201, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      123/     200 | consumed samples:        62976 | consumed tokens:     64487424 | elapsed time per iteration (ms): 9743.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.325315E+00 | moe loss: 8.050648E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 52.549 | TFLOPs: 2.83 |
time (ms) | forward-compute: 8638.27 | backward-compute: 1010.48 | backward-embedding-all-reduce: 0.01 | optimizer: 62.02 | batch-generator: 6447.26
[2023-01-05 03:04:13,470] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:04:13,497] [INFO] [timer.py:207:stop] 0/124, RunningAvgSamplesPerSec=93.11752190569997, CurrSamplesPerSec=113.89832395550066, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      124/     200 | consumed samples:        63488 | consumed tokens:     65011712 | elapsed time per iteration (ms): 8253.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.336383E+00 | moe loss: 8.154678E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 62.034 | TFLOPs: 3.34 |
time (ms) | forward-compute: 7131.77 | backward-compute: 1024.83 | backward-embedding-all-reduce: 0.01 | optimizer: 61.61 | batch-generator: 4885.54
[2023-01-05 03:04:21,494] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:04:21,521] [INFO] [timer.py:207:stop] 0/125, RunningAvgSamplesPerSec=93.18377974572513, CurrSamplesPerSec=102.04195940300414, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      125/     200 | consumed samples:        64000 | consumed tokens:     65536000 | elapsed time per iteration (ms): 8023.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.316339E+00 | moe loss: 8.068763E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.815 | TFLOPs: 3.44 |
time (ms) | forward-compute: 6911.00 | backward-compute: 1015.53 | backward-embedding-all-reduce: 0.01 | optimizer: 63.95 | batch-generator: 4669.67
[2023-01-05 03:04:30,458] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:04:30,486] [INFO] [timer.py:207:stop] 0/126, RunningAvgSamplesPerSec=93.26158460313671, CurrSamplesPerSec=103.9358192034395, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      126/     200 | consumed samples:        64512 | consumed tokens:     66060288 | elapsed time per iteration (ms): 8966.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.311964E+00 | moe loss: 8.063697E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 57.104 | TFLOPs: 3.08 |
time (ms) | forward-compute: 7854.29 | backward-compute: 1010.13 | backward-embedding-all-reduce: 0.01 | optimizer: 69.50 | batch-generator: 6290.02
[2023-01-05 03:04:40,683] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:04:40,709] [INFO] [timer.py:207:stop] 0/127, RunningAvgSamplesPerSec=93.37106242966146, CurrSamplesPerSec=109.27762376173133, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      127/     200 | consumed samples:        65024 | consumed tokens:     66584576 | elapsed time per iteration (ms): 10221.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.305809E+00 | moe loss: 8.027062E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 50.090 | TFLOPs: 2.70 |
time (ms) | forward-compute: 9099.77 | backward-compute: 1028.19 | backward-embedding-all-reduce: 0.01 | optimizer: 61.46 | batch-generator: 6815.30
[2023-01-05 03:04:48,794] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:04:48,821] [INFO] [timer.py:207:stop] 0/128, RunningAvgSamplesPerSec=93.49287859716802, CurrSamplesPerSec=111.71074899155246, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      128/     200 | consumed samples:        65536 | consumed tokens:     67108864 | elapsed time per iteration (ms): 8114.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.305912E+00 | moe loss: 8.100910E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.100 | TFLOPs: 3.40 |
time (ms) | forward-compute: 6992.54 | backward-compute: 1022.94 | backward-embedding-all-reduce: 0.01 | optimizer: 61.86 | batch-generator: 4955.13
[2023-01-05 03:04:56,354] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:04:56,381] [INFO] [timer.py:207:stop] 0/129, RunningAvgSamplesPerSec=93.5750838788691, CurrSamplesPerSec=105.23368711069365, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      129/     200 | consumed samples:        66048 | consumed tokens:     67633152 | elapsed time per iteration (ms): 7559.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.297501E+00 | moe loss: 8.030404E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 67.731 | TFLOPs: 3.65 |
time (ms) | forward-compute: 6422.74 | backward-compute: 1036.62 | backward-embedding-all-reduce: 0.01 | optimizer: 69.15 | batch-generator: 4520.06
[2023-01-05 03:05:03,705] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:05:03,733] [INFO] [timer.py:207:stop] 0/130, RunningAvgSamplesPerSec=93.5632560789315, CurrSamplesPerSec=92.08504460594934, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      130/     200 | consumed samples:        66560 | consumed tokens:     68157440 | elapsed time per iteration (ms): 7347.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.291316E+00 | moe loss: 8.065233E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 69.679 | TFLOPs: 3.75 |
time (ms) | forward-compute: 6221.96 | backward-compute: 1033.88 | backward-embedding-all-reduce: 0.01 | optimizer: 63.09 | batch-generator: 4056.97
[2023-01-05 03:05:11,420] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:05:11,447] [INFO] [timer.py:207:stop] 0/131, RunningAvgSamplesPerSec=93.62664707636877, CurrSamplesPerSec=102.51720430805635, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      131/     200 | consumed samples:        67072 | consumed tokens:     68681728 | elapsed time per iteration (ms): 7717.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.293539E+00 | moe loss: 8.012631E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 66.345 | TFLOPs: 3.57 |
time (ms) | forward-compute: 6602.65 | backward-compute: 1019.96 | backward-embedding-all-reduce: 0.01 | optimizer: 62.28 | batch-generator: 4467.73
[2023-01-05 03:05:18,640] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:05:18,668] [INFO] [timer.py:207:stop] 0/132, RunningAvgSamplesPerSec=93.61775249183968, CurrSamplesPerSec=92.48435002149874, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      132/     200 | consumed samples:        67584 | consumed tokens:     69206016 | elapsed time per iteration (ms): 7220.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.278661E+00 | moe loss: 8.080593E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.905 | TFLOPs: 3.82 |
time (ms) | forward-compute: 6109.35 | backward-compute: 1011.82 | backward-embedding-all-reduce: 0.01 | optimizer: 68.07 | batch-generator: 4452.65
[2023-01-05 03:05:26,324] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:05:26,352] [INFO] [timer.py:207:stop] 0/133, RunningAvgSamplesPerSec=93.45318457844712, CurrSamplesPerSec=76.06953151721952, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      133/     200 | consumed samples:        68096 | consumed tokens:     69730304 | elapsed time per iteration (ms): 7684.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.269608E+00 | moe loss: 8.054147E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 66.626 | TFLOPs: 3.59 |
time (ms) | forward-compute: 6497.89 | backward-compute: 1088.31 | backward-embedding-all-reduce: 0.01 | optimizer: 65.62 | batch-generator: 4025.73
[2023-01-05 03:05:35,000] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:05:35,028] [INFO] [timer.py:207:stop] 0/134, RunningAvgSamplesPerSec=93.46822859782516, CurrSamplesPerSec=95.4817746024282, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      134/     200 | consumed samples:        68608 | consumed tokens:     70254592 | elapsed time per iteration (ms): 8675.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.278375E+00 | moe loss: 8.099714E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 59.018 | TFLOPs: 3.18 |
time (ms) | forward-compute: 7566.83 | backward-compute: 1013.32 | backward-embedding-all-reduce: 0.01 | optimizer: 62.42 | batch-generator: 5334.60
[2023-01-05 03:05:42,248] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:05:42,276] [INFO] [timer.py:207:stop] 0/135, RunningAvgSamplesPerSec=93.53215216458032, CurrSamplesPerSec=102.81373523444678, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      135/     200 | consumed samples:        69120 | consumed tokens:     70778880 | elapsed time per iteration (ms): 7248.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.274246E+00 | moe loss: 8.030274E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.634 | TFLOPs: 3.81 |
time (ms) | forward-compute: 6133.15 | backward-compute: 1018.83 | backward-embedding-all-reduce: 0.01 | optimizer: 63.77 | batch-generator: 4480.53
[2023-01-05 03:05:51,292] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:05:51,319] [INFO] [timer.py:207:stop] 0/136, RunningAvgSamplesPerSec=93.55215212778678, CurrSamplesPerSec=96.29059561713258, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      136/     200 | consumed samples:        69632 | consumed tokens:     71303168 | elapsed time per iteration (ms): 9042.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.277231E+00 | moe loss: 8.142459E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.621 | TFLOPs: 3.05 |
time (ms) | forward-compute: 7934.97 | backward-compute: 1015.16 | backward-embedding-all-reduce: 0.01 | optimizer: 60.81 | batch-generator: 5072.54
[2023-01-05 03:05:58,597] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:05:58,624] [INFO] [timer.py:207:stop] 0/137, RunningAvgSamplesPerSec=93.65257335949755, CurrSamplesPerSec=109.38663393651034, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      137/     200 | consumed samples:        70144 | consumed tokens:     71827456 | elapsed time per iteration (ms): 7306.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.261183E+00 | moe loss: 8.083284E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.078 | TFLOPs: 3.78 |
time (ms) | forward-compute: 6194.53 | backward-compute: 1016.25 | backward-embedding-all-reduce: 0.01 | optimizer: 62.51 | batch-generator: 4655.99
[2023-01-05 03:06:07,338] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:06:07,366] [INFO] [timer.py:207:stop] 0/138, RunningAvgSamplesPerSec=93.69880513742494, CurrSamplesPerSec=100.38903316140305, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      138/     200 | consumed samples:        70656 | consumed tokens:     72351744 | elapsed time per iteration (ms): 8740.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.256536E+00 | moe loss: 8.010484E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.577 | TFLOPs: 3.16 |
time (ms) | forward-compute: 7503.17 | backward-compute: 1145.11 | backward-embedding-all-reduce: 0.01 | optimizer: 60.28 | batch-generator: 5329.09
[2023-01-05 03:06:16,469] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:06:16,496] [INFO] [timer.py:207:stop] 0/139, RunningAvgSamplesPerSec=93.33963916116771, CurrSamplesPerSec=61.35458609523927, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      139/     200 | consumed samples:        71168 | consumed tokens:     72876032 | elapsed time per iteration (ms): 9132.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.251309E+00 | moe loss: 8.142285E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 56.065 | TFLOPs: 3.02 |
time (ms) | forward-compute: 8020.84 | backward-compute: 1016.01 | backward-embedding-all-reduce: 0.01 | optimizer: 61.39 | batch-generator: 5334.47
[2023-01-05 03:06:26,004] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:06:26,029] [INFO] [timer.py:207:stop] 0/140, RunningAvgSamplesPerSec=93.37442024710484, CurrSamplesPerSec=98.39764111451767, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      140/     200 | consumed samples:        71680 | consumed tokens:     73400320 | elapsed time per iteration (ms): 9530.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.255489E+00 | moe loss: 8.106419E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 53.722 | TFLOPs: 2.89 |
time (ms) | forward-compute: 8418.37 | backward-compute: 1015.84 | backward-embedding-all-reduce: 0.01 | optimizer: 63.36 | batch-generator: 5407.85
[2023-01-05 03:06:34,139] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:06:34,167] [INFO] [timer.py:207:stop] 0/141, RunningAvgSamplesPerSec=93.3137978942191, CurrSamplesPerSec=85.64080660176606, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      141/     200 | consumed samples:        72192 | consumed tokens:     73924608 | elapsed time per iteration (ms): 8138.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.233466E+00 | moe loss: 8.064172E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 62.913 | TFLOPs: 3.39 |
time (ms) | forward-compute: 7013.56 | backward-compute: 1022.78 | backward-embedding-all-reduce: 0.01 | optimizer: 69.59 | batch-generator: 4783.91
[2023-01-05 03:06:41,188] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:06:41,215] [INFO] [timer.py:207:stop] 0/142, RunningAvgSamplesPerSec=93.24031189280292, CurrSamplesPerSec=84.04082016011982, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      142/     200 | consumed samples:        72704 | consumed tokens:     74448896 | elapsed time per iteration (ms): 7048.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.239210E+00 | moe loss: 8.044950E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 72.642 | TFLOPs: 3.91 |
time (ms) | forward-compute: 5915.43 | backward-compute: 1037.31 | backward-embedding-all-reduce: 0.01 | optimizer: 64.02 | batch-generator: 4019.58
[2023-01-05 03:06:48,956] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:06:48,984] [INFO] [timer.py:207:stop] 0/143, RunningAvgSamplesPerSec=93.37975415160517, CurrSamplesPerSec=118.10834084979254, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      143/     200 | consumed samples:        73216 | consumed tokens:     74973184 | elapsed time per iteration (ms): 7770.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.226352E+00 | moe loss: 8.020483E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 65.888 | TFLOPs: 3.55 |
time (ms) | forward-compute: 6657.54 | backward-compute: 1016.26 | backward-embedding-all-reduce: 0.01 | optimizer: 62.97 | batch-generator: 4985.92
[2023-01-05 03:06:59,247] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:06:59,275] [INFO] [timer.py:207:stop] 0/144, RunningAvgSamplesPerSec=93.42656979732033, CurrSamplesPerSec=100.53325623306253, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      144/     200 | consumed samples:        73728 | consumed tokens:     75497472 | elapsed time per iteration (ms): 10288.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.223324E+00 | moe loss: 8.114855E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 49.764 | TFLOPs: 2.68 |
time (ms) | forward-compute: 9159.83 | backward-compute: 1035.45 | backward-embedding-all-reduce: 0.01 | optimizer: 63.08 | batch-generator: 7067.98
[2023-01-05 03:07:06,193] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:07:06,221] [INFO] [timer.py:207:stop] 0/145, RunningAvgSamplesPerSec=93.42510202378408, CurrSamplesPerSec=93.21714538321805, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      145/     200 | consumed samples:        74240 | consumed tokens:     76021760 | elapsed time per iteration (ms): 6942.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.225135E+00 | moe loss: 8.074455E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 73.751 | TFLOPs: 3.97 |
time (ms) | forward-compute: 5830.07 | backward-compute: 1021.50 | backward-embedding-all-reduce: 0.01 | optimizer: 61.42 | batch-generator: 3998.75
[2023-01-05 03:07:13,118] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:07:13,145] [INFO] [timer.py:207:stop] 0/146, RunningAvgSamplesPerSec=93.56125212818978, CurrSamplesPerSec=118.19206579839555, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      146/     200 | consumed samples:        74752 | consumed tokens:     76546048 | elapsed time per iteration (ms): 6928.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.220106E+00 | moe loss: 8.037764E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 73.900 | TFLOPs: 3.98 |
time (ms) | forward-compute: 5810.26 | backward-compute: 1018.16 | backward-embedding-all-reduce: 0.01 | optimizer: 68.41 | batch-generator: 3572.60
[2023-01-05 03:07:20,389] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:07:20,416] [INFO] [timer.py:207:stop] 0/147, RunningAvgSamplesPerSec=93.63985963892304, CurrSamplesPerSec=106.52813594433013, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      147/     200 | consumed samples:        75264 | consumed tokens:     77070336 | elapsed time per iteration (ms): 7273.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.229500E+00 | moe loss: 8.048267E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.395 | TFLOPs: 3.79 |
time (ms) | forward-compute: 6151.92 | backward-compute: 1023.83 | backward-embedding-all-reduce: 0.01 | optimizer: 63.11 | batch-generator: 4418.92
[2023-01-05 03:07:29,118] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:07:29,145] [INFO] [timer.py:207:stop] 0/148, RunningAvgSamplesPerSec=93.69827417758191, CurrSamplesPerSec=103.0165414884231, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      148/     200 | consumed samples:        75776 | consumed tokens:     77594624 | elapsed time per iteration (ms): 8724.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.204498E+00 | moe loss: 8.101532E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.683 | TFLOPs: 3.16 |
time (ms) | forward-compute: 7610.67 | backward-compute: 1018.49 | backward-embedding-all-reduce: 0.01 | optimizer: 64.31 | batch-generator: 4863.23
[2023-01-05 03:07:37,193] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:07:37,221] [INFO] [timer.py:207:stop] 0/149, RunningAvgSamplesPerSec=93.61035424524275, CurrSamplesPerSec=82.33127951488622, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      149/     200 | consumed samples:        76288 | consumed tokens:     78118912 | elapsed time per iteration (ms): 8078.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.216640E+00 | moe loss: 8.074258E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.377 | TFLOPs: 3.41 |
time (ms) | forward-compute: 6964.28 | backward-compute: 1013.42 | backward-embedding-all-reduce: 0.01 | optimizer: 67.89 | batch-generator: 4556.00
[2023-01-05 03:07:45,377] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:07:45,404] [INFO] [timer.py:207:stop] 0/150, RunningAvgSamplesPerSec=93.72286125680144, CurrSamplesPerSec=113.83447075022868, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      150/     200 | consumed samples:        76800 | consumed tokens:     78643200 | elapsed time per iteration (ms): 8183.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.204242E+00 | moe loss: 8.041038E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 62.567 | TFLOPs: 3.37 |
time (ms) | forward-compute: 7069.19 | backward-compute: 1016.32 | backward-embedding-all-reduce: 0.01 | optimizer: 64.09 | batch-generator: 5097.27
[2023-01-05 03:07:53,254] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:07:53,281] [INFO] [timer.py:207:stop] 0/151, RunningAvgSamplesPerSec=93.69612479077547, CurrSamplesPerSec=89.90050828606421, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      151/     200 | consumed samples:        77312 | consumed tokens:     79167488 | elapsed time per iteration (ms): 7876.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.200758E+00 | moe loss: 8.071418E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 65.006 | TFLOPs: 3.50 |
time (ms) | forward-compute: 6765.67 | backward-compute: 1016.66 | backward-embedding-all-reduce: 0.01 | optimizer: 61.47 | batch-generator: 4841.91
[2023-01-05 03:08:00,658] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:08:00,686] [INFO] [timer.py:207:stop] 0/152, RunningAvgSamplesPerSec=93.70178730353499, CurrSamplesPerSec=94.55321966841706, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      152/     200 | consumed samples:        77824 | consumed tokens:     79691776 | elapsed time per iteration (ms): 7405.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.201297E+00 | moe loss: 8.071205E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 69.134 | TFLOPs: 3.73 |
time (ms) | forward-compute: 6288.48 | backward-compute: 1023.03 | backward-embedding-all-reduce: 0.01 | optimizer: 60.84 | batch-generator: 3691.34
[2023-01-05 03:08:07,847] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:08:07,875] [INFO] [timer.py:207:stop] 0/153, RunningAvgSamplesPerSec=93.84561788590261, CurrSamplesPerSec=121.91658809444654, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      153/     200 | consumed samples:        78336 | consumed tokens:     80216064 | elapsed time per iteration (ms): 7187.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.203198E+00 | moe loss: 8.156601E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 71.233 | TFLOPs: 3.84 |
time (ms) | forward-compute: 6067.72 | backward-compute: 1024.63 | backward-embedding-all-reduce: 0.01 | optimizer: 61.80 | batch-generator: 3699.38
[2023-01-05 03:08:14,943] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:08:14,970] [INFO] [timer.py:207:stop] 0/154, RunningAvgSamplesPerSec=93.9008388268782, CurrSamplesPerSec=103.0577131427299, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      154/     200 | consumed samples:        78848 | consumed tokens:     80740352 | elapsed time per iteration (ms): 7095.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.192482E+00 | moe loss: 8.066559E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 72.155 | TFLOPs: 3.89 |
time (ms) | forward-compute: 5988.57 | backward-compute: 1008.89 | backward-embedding-all-reduce: 0.01 | optimizer: 64.62 | batch-generator: 4104.36
[2023-01-05 03:08:21,986] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:08:22,013] [INFO] [timer.py:207:stop] 0/155, RunningAvgSamplesPerSec=94.01460071923165, CurrSamplesPerSec=115.23510652044101, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      155/     200 | consumed samples:        79360 | consumed tokens:     81264640 | elapsed time per iteration (ms): 7043.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.187270E+00 | moe loss: 8.124496E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 72.688 | TFLOPs: 3.92 |
time (ms) | forward-compute: 5930.25 | backward-compute: 1017.16 | backward-embedding-all-reduce: 0.01 | optimizer: 62.62 | batch-generator: 3978.56
[2023-01-05 03:08:28,722] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:08:28,749] [INFO] [timer.py:207:stop] 0/156, RunningAvgSamplesPerSec=94.11952437713987, CurrSamplesPerSec=113.5000526411703, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      156/     200 | consumed samples:        79872 | consumed tokens:     81788928 | elapsed time per iteration (ms): 6736.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.188184E+00 | moe loss: 8.076081E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 76.008 | TFLOPs: 4.10 |
time (ms) | forward-compute: 5617.84 | backward-compute: 1023.61 | backward-embedding-all-reduce: 0.01 | optimizer: 62.02 | batch-generator: 3849.18
[2023-01-05 03:08:37,680] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:08:37,708] [INFO] [timer.py:207:stop] 0/157, RunningAvgSamplesPerSec=94.12618344699288, CurrSamplesPerSec=95.16305013302987, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      157/     200 | consumed samples:        80384 | consumed tokens:     82313216 | elapsed time per iteration (ms): 8959.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.177413E+00 | moe loss: 8.159395E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 57.147 | TFLOPs: 3.08 |
time (ms) | forward-compute: 7782.56 | backward-compute: 1080.54 | backward-embedding-all-reduce: 0.01 | optimizer: 62.23 | batch-generator: 5356.37
[2023-01-05 03:08:46,372] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:08:46,400] [INFO] [timer.py:207:stop] 0/158, RunningAvgSamplesPerSec=94.25702189070037, CurrSamplesPerSec=120.14227908086289, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      158/     200 | consumed samples:        80896 | consumed tokens:     82837504 | elapsed time per iteration (ms): 8693.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.179938E+00 | moe loss: 8.081447E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.895 | TFLOPs: 3.17 |
time (ms) | forward-compute: 7571.19 | backward-compute: 1026.92 | backward-embedding-all-reduce: 0.01 | optimizer: 60.96 | batch-generator: 5296.82
[2023-01-05 03:08:54,813] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:08:54,840] [INFO] [timer.py:207:stop] 0/159, RunningAvgSamplesPerSec=94.21138803465915, CurrSamplesPerSec=87.59561753443074, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      159/     200 | consumed samples:        81408 | consumed tokens:     83361792 | elapsed time per iteration (ms): 8437.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.183613E+00 | moe loss: 8.090987E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 60.680 | TFLOPs: 3.27 |
time (ms) | forward-compute: 7329.51 | backward-compute: 1018.00 | backward-embedding-all-reduce: 0.01 | optimizer: 60.63 | batch-generator: 5016.49
[2023-01-05 03:09:01,949] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:09:01,976] [INFO] [timer.py:207:stop] 0/160, RunningAvgSamplesPerSec=94.3572475191679, CurrSamplesPerSec=124.65778481168434, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      160/     200 | consumed samples:        81920 | consumed tokens:     83886080 | elapsed time per iteration (ms): 7136.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.180151E+00 | moe loss: 8.035590E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 71.740 | TFLOPs: 3.87 |
time (ms) | forward-compute: 6025.87 | backward-compute: 1014.36 | backward-embedding-all-reduce: 0.01 | optimizer: 63.57 | batch-generator: 4152.52
[2023-01-05 03:09:09,053] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:09:09,081] [INFO] [timer.py:207:stop] 0/161, RunningAvgSamplesPerSec=94.50665840110184, CurrSamplesPerSec=126.04023739764105, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      161/     200 | consumed samples:        82432 | consumed tokens:     84410368 | elapsed time per iteration (ms): 7103.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.174348E+00 | moe loss: 8.052365E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 72.076 | TFLOPs: 3.88 |
time (ms) | forward-compute: 5995.28 | backward-compute: 1014.36 | backward-embedding-all-reduce: 0.01 | optimizer: 62.64 | batch-generator: 3933.13
[2023-01-05 03:09:17,492] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:09:17,519] [INFO] [timer.py:207:stop] 0/162, RunningAvgSamplesPerSec=94.31163664107139, CurrSamplesPerSec=71.01198446725428, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      162/     200 | consumed samples:        82944 | consumed tokens:     84934656 | elapsed time per iteration (ms): 8437.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.174089E+00 | moe loss: 8.070186E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 60.678 | TFLOPs: 3.27 |
time (ms) | forward-compute: 7313.17 | backward-compute: 1029.91 | backward-embedding-all-reduce: 0.01 | optimizer: 61.70 | batch-generator: 4764.26
[2023-01-05 03:09:25,510] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:09:25,538] [INFO] [timer.py:207:stop] 0/163, RunningAvgSamplesPerSec=94.35523136397923, CurrSamplesPerSec=101.89094174801872, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      163/     200 | consumed samples:        83456 | consumed tokens:     85458944 | elapsed time per iteration (ms): 8019.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.155623E+00 | moe loss: 8.027075E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.845 | TFLOPs: 3.44 |
time (ms) | forward-compute: 6902.42 | backward-compute: 1021.88 | backward-embedding-all-reduce: 0.01 | optimizer: 61.05 | batch-generator: 4494.60
[2023-01-05 03:09:34,043] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:09:34,070] [INFO] [timer.py:207:stop] 0/164, RunningAvgSamplesPerSec=94.34120060682115, CurrSamplesPerSec=92.13539368090973, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      164/     200 | consumed samples:        83968 | consumed tokens:     85983232 | elapsed time per iteration (ms): 8534.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.161901E+00 | moe loss: 8.064637E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 59.993 | TFLOPs: 3.23 |
time (ms) | forward-compute: 7367.46 | backward-compute: 1069.39 | backward-embedding-all-reduce: 0.01 | optimizer: 64.26 | batch-generator: 4831.75
[2023-01-05 03:09:40,772] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:09:40,800] [INFO] [timer.py:207:stop] 0/165, RunningAvgSamplesPerSec=94.46115049599273, CurrSamplesPerSec=118.96488340410897, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      165/     200 | consumed samples:        84480 | consumed tokens:     86507520 | elapsed time per iteration (ms): 6727.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.153789E+00 | moe loss: 8.122654E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 76.105 | TFLOPs: 4.10 |
time (ms) | forward-compute: 5613.62 | backward-compute: 1019.14 | backward-embedding-all-reduce: 0.01 | optimizer: 63.96 | batch-generator: 4215.75
[2023-01-05 03:09:47,143] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:09:47,170] [INFO] [timer.py:207:stop] 0/166, RunningAvgSamplesPerSec=94.38144040710208, CurrSamplesPerSec=82.96934650488339, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      166/     200 | consumed samples:        84992 | consumed tokens:     87031808 | elapsed time per iteration (ms): 6369.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.156398E+00 | moe loss: 8.135914E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 80.382 | TFLOPs: 4.33 |
time (ms) | forward-compute: 5254.47 | backward-compute: 1020.51 | backward-embedding-all-reduce: 0.01 | optimizer: 61.70 | batch-generator: 3842.10
[2023-01-05 03:09:53,796] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:09:53,823] [INFO] [timer.py:207:stop] 0/167, RunningAvgSamplesPerSec=94.49395934889917, CurrSamplesPerSec=117.45912927198961, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      167/     200 | consumed samples:        85504 | consumed tokens:     87556096 | elapsed time per iteration (ms): 6653.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.152557E+00 | moe loss: 8.151845E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 76.956 | TFLOPs: 4.15 |
time (ms) | forward-compute: 5536.84 | backward-compute: 1022.20 | backward-embedding-all-reduce: 0.01 | optimizer: 61.30 | batch-generator: 3663.75
[2023-01-05 03:10:02,284] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:02,311] [INFO] [timer.py:207:stop] 0/168, RunningAvgSamplesPerSec=94.45194065105424, CurrSamplesPerSec=87.99564144742234, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      168/     200 | consumed samples:        86016 | consumed tokens:     88080384 | elapsed time per iteration (ms): 8487.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.145244E+00 | moe loss: 8.112894E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 60.323 | TFLOPs: 3.25 |
time (ms) | forward-compute: 7379.94 | backward-compute: 1013.04 | backward-embedding-all-reduce: 0.01 | optimizer: 61.67 | batch-generator: 5514.24
[2023-01-05 03:10:08,969] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:08,997] [INFO] [timer.py:207:stop] 0/169, RunningAvgSamplesPerSec=94.365802703205, CurrSamplesPerSec=81.9582928913625, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      169/     200 | consumed samples:        86528 | consumed tokens:     88604672 | elapsed time per iteration (ms): 6680.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.135491E+00 | moe loss: 8.094732E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 76.641 | TFLOPs: 4.13 |
time (ms) | forward-compute: 5576.42 | backward-compute: 1014.56 | backward-embedding-all-reduce: 0.01 | optimizer: 62.00 | batch-generator: 3823.51
[2023-01-05 03:10:15,561] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:15,588] [INFO] [timer.py:207:stop] 0/170, RunningAvgSamplesPerSec=94.28851416986359, CurrSamplesPerSec=82.94363297037413, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      170/     200 | consumed samples:        87040 | consumed tokens:     89128960 | elapsed time per iteration (ms): 6597.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.131419E+00 | moe loss: 8.143546E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.608 | TFLOPs: 4.18 |
time (ms) | forward-compute: 5489.24 | backward-compute: 1010.89 | backward-embedding-all-reduce: 0.01 | optimizer: 61.05 | batch-generator: 3567.89
[2023-01-05 03:10:24,012] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:24,039] [INFO] [timer.py:207:stop] 0/171, RunningAvgSamplesPerSec=94.20695813874117, CurrSamplesPerSec=82.25428353694504, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      171/     200 | consumed samples:        87552 | consumed tokens:     89653248 | elapsed time per iteration (ms): 8452.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.135130E+00 | moe loss: 8.075396E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 60.573 | TFLOPs: 3.26 |
time (ms) | forward-compute: 7341.91 | backward-compute: 1014.86 | backward-embedding-all-reduce: 0.01 | optimizer: 61.04 | batch-generator: 5246.90
[2023-01-05 03:10:30,608] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:30,635] [INFO] [timer.py:207:stop] 0/172, RunningAvgSamplesPerSec=94.2572234328893, CurrSamplesPerSec=103.59895364983701, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      172/     200 | consumed samples:        88064 | consumed tokens:     90177536 | elapsed time per iteration (ms): 6593.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.149152E+00 | moe loss: 7.991492E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.651 | TFLOPs: 4.18 |
time (ms) | forward-compute: 5485.54 | backward-compute: 1015.05 | backward-embedding-all-reduce: 0.01 | optimizer: 62.49 | batch-generator: 3825.48
[2023-01-05 03:10:36,653] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:36,680] [INFO] [timer.py:207:stop] 0/173, RunningAvgSamplesPerSec=94.38723924703471, CurrSamplesPerSec=123.30034550771076, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      173/     200 | consumed samples:        88576 | consumed tokens:     90701824 | elapsed time per iteration (ms): 6046.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.144910E+00 | moe loss: 8.113046E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 84.671 | TFLOPs: 4.56 |
time (ms) | forward-compute: 4925.59 | backward-compute: 1024.54 | backward-embedding-all-reduce: 0.01 | optimizer: 62.12 | batch-generator: 3326.78
[2023-01-05 03:10:43,636] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:43,664] [INFO] [timer.py:207:stop] 0/174, RunningAvgSamplesPerSec=94.1299181578612, CurrSamplesPerSec=64.20057782454798, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      174/     200 | consumed samples:        89088 | consumed tokens:     91226112 | elapsed time per iteration (ms): 6983.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.118729E+00 | moe loss: 8.047554E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 73.312 | TFLOPs: 3.95 |
time (ms) | forward-compute: 5867.55 | backward-compute: 1021.45 | backward-embedding-all-reduce: 0.01 | optimizer: 60.79 | batch-generator: 3346.29
[2023-01-05 03:10:50,262] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:50,290] [INFO] [timer.py:207:stop] 0/175, RunningAvgSamplesPerSec=94.21792355098343, CurrSamplesPerSec=112.27230887369444, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      175/     200 | consumed samples:        89600 | consumed tokens:     91750400 | elapsed time per iteration (ms): 6625.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.133056E+00 | moe loss: 8.057934E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.276 | TFLOPs: 4.16 |
time (ms) | forward-compute: 5512.02 | backward-compute: 1015.25 | backward-embedding-all-reduce: 0.01 | optimizer: 65.67 | batch-generator: 3751.19
[2023-01-05 03:10:56,462] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:10:56,490] [INFO] [timer.py:207:stop] 0/176, RunningAvgSamplesPerSec=94.1985847708016, CurrSamplesPerSec=90.96836521888828, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      176/     200 | consumed samples:        90112 | consumed tokens:     92274688 | elapsed time per iteration (ms): 6203.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.129357E+00 | moe loss: 8.127005E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 82.538 | TFLOPs: 4.45 |
time (ms) | forward-compute: 5069.97 | backward-compute: 1020.16 | backward-embedding-all-reduce: 0.02 | optimizer: 78.26 | batch-generator: 3440.43
[2023-01-05 03:11:04,149] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:11:04,177] [INFO] [timer.py:207:stop] 0/177, RunningAvgSamplesPerSec=94.29184572366094, CurrSamplesPerSec=113.91591790140615, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      177/     200 | consumed samples:        90624 | consumed tokens:     92798976 | elapsed time per iteration (ms): 7684.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.128040E+00 | moe loss: 8.095920E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 66.632 | TFLOPs: 3.59 |
time (ms) | forward-compute: 6567.37 | backward-compute: 1021.69 | backward-embedding-all-reduce: 0.01 | optimizer: 62.50 | batch-generator: 4702.16
[2023-01-05 03:11:10,441] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:11:10,469] [INFO] [timer.py:207:stop] 0/178, RunningAvgSamplesPerSec=94.32989110633784, CurrSamplesPerSec=101.49655736180105, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      178/     200 | consumed samples:        91136 | consumed tokens:     93323264 | elapsed time per iteration (ms): 6293.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.119227E+00 | moe loss: 8.102334E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 81.350 | TFLOPs: 4.38 |
time (ms) | forward-compute: 5172.24 | backward-compute: 1015.08 | backward-embedding-all-reduce: 0.01 | optimizer: 71.25 | batch-generator: 3203.86
[2023-01-05 03:11:16,765] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:11:16,793] [INFO] [timer.py:207:stop] 0/179, RunningAvgSamplesPerSec=94.27159161194925, CurrSamplesPerSec=85.02321232534376, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      179/     200 | consumed samples:        91648 | consumed tokens:     93847552 | elapsed time per iteration (ms): 6321.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.124161E+00 | moe loss: 8.106899E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 80.988 | TFLOPs: 4.36 |
time (ms) | forward-compute: 5198.97 | backward-compute: 1025.54 | backward-embedding-all-reduce: 0.01 | optimizer: 61.87 | batch-generator: 3375.32
[2023-01-05 03:11:24,069] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:11:24,097] [INFO] [timer.py:207:stop] 0/180, RunningAvgSamplesPerSec=94.37763695083423, CurrSamplesPerSec=117.84033689807411, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      180/     200 | consumed samples:        92160 | consumed tokens:     94371840 | elapsed time per iteration (ms): 7305.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.100307E+00 | moe loss: 8.110366E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.084 | TFLOPs: 3.78 |
time (ms) | forward-compute: 6191.33 | backward-compute: 1017.30 | backward-embedding-all-reduce: 0.01 | optimizer: 62.20 | batch-generator: 4424.32
[2023-01-05 03:11:31,318] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:11:31,346] [INFO] [timer.py:207:stop] 0/181, RunningAvgSamplesPerSec=94.47050991622261, CurrSamplesPerSec=114.53221689127064, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      181/     200 | consumed samples:        92672 | consumed tokens:     94896128 | elapsed time per iteration (ms): 7249.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.106756E+00 | moe loss: 8.153716E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.625 | TFLOPs: 3.81 |
time (ms) | forward-compute: 6119.99 | backward-compute: 1030.81 | backward-embedding-all-reduce: 0.01 | optimizer: 61.54 | batch-generator: 4108.29
[2023-01-05 03:11:39,899] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:11:39,926] [INFO] [timer.py:207:stop] 0/182, RunningAvgSamplesPerSec=94.51149845988044, CurrSamplesPerSec=102.46969584186022, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      182/     200 | consumed samples:        93184 | consumed tokens:     95420416 | elapsed time per iteration (ms): 8579.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.111763E+00 | moe loss: 8.144811E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 59.679 | TFLOPs: 3.22 |
time (ms) | forward-compute: 7472.61 | backward-compute: 1008.15 | backward-embedding-all-reduce: 0.01 | optimizer: 62.09 | batch-generator: 5356.01
[2023-01-05 03:11:46,652] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:11:46,680] [INFO] [timer.py:207:stop] 0/183, RunningAvgSamplesPerSec=94.54334026442315, CurrSamplesPerSec=100.64694087210829, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      183/     200 | consumed samples:        93696 | consumed tokens:     95944704 | elapsed time per iteration (ms): 6753.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.108157E+00 | moe loss: 8.119467E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 75.808 | TFLOPs: 4.08 |
time (ms) | forward-compute: 5629.96 | backward-compute: 1026.54 | backward-embedding-all-reduce: 0.01 | optimizer: 62.10 | batch-generator: 3429.66
[2023-01-05 03:11:55,816] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:11:55,840] [INFO] [timer.py:207:stop] 0/184, RunningAvgSamplesPerSec=94.65619020620693, CurrSamplesPerSec=120.74223094434132, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      184/     200 | consumed samples:        94208 | consumed tokens:     96468992 | elapsed time per iteration (ms): 9159.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.098337E+00 | moe loss: 8.129093E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 55.900 | TFLOPs: 3.01 |
time (ms) | forward-compute: 8037.71 | backward-compute: 1026.32 | backward-embedding-all-reduce: 0.01 | optimizer: 62.52 | batch-generator: 5077.73
[2023-01-05 03:12:02,402] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:12:02,429] [INFO] [timer.py:207:stop] 0/185, RunningAvgSamplesPerSec=94.62980365651387, CurrSamplesPerSec=90.0606070772785, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      185/     200 | consumed samples:        94720 | consumed tokens:     96993280 | elapsed time per iteration (ms): 6587.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.101812E+00 | moe loss: 8.123474E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.726 | TFLOPs: 4.19 |
time (ms) | forward-compute: 5407.87 | backward-compute: 1086.48 | backward-embedding-all-reduce: 0.01 | optimizer: 62.19 | batch-generator: 3647.15
[2023-01-05 03:12:11,211] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:12:11,238] [INFO] [timer.py:207:stop] 0/186, RunningAvgSamplesPerSec=94.54497588852664, CurrSamplesPerSec=81.22111373205898, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      186/     200 | consumed samples:        95232 | consumed tokens:     97517568 | elapsed time per iteration (ms): 8809.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.092772E+00 | moe loss: 8.086372E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 58.122 | TFLOPs: 3.13 |
time (ms) | forward-compute: 7693.37 | backward-compute: 1017.88 | backward-embedding-all-reduce: 0.01 | optimizer: 66.11 | batch-generator: 4907.42
[2023-01-05 03:12:18,551] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:12:18,578] [INFO] [timer.py:207:stop] 0/187, RunningAvgSamplesPerSec=94.55348719138549, CurrSamplesPerSec=96.1460883667854, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      187/     200 | consumed samples:        95744 | consumed tokens:     98041856 | elapsed time per iteration (ms): 7341.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.096980E+00 | moe loss: 8.104070E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 69.741 | TFLOPs: 3.76 |
time (ms) | forward-compute: 6213.99 | backward-compute: 1022.08 | backward-embedding-all-reduce: 0.01 | optimizer: 71.50 | batch-generator: 4093.87
[2023-01-05 03:12:26,164] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:12:26,192] [INFO] [timer.py:207:stop] 0/188, RunningAvgSamplesPerSec=94.40810824998299, CurrSamplesPerSec=73.50118889720065, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      188/     200 | consumed samples:        96256 | consumed tokens:     98566144 | elapsed time per iteration (ms): 7612.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.090395E+00 | moe loss: 8.087148E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 67.255 | TFLOPs: 3.62 |
time (ms) | forward-compute: 6489.35 | backward-compute: 1018.92 | backward-embedding-all-reduce: 0.01 | optimizer: 68.90 | batch-generator: 4263.58
[2023-01-05 03:12:33,441] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:12:33,468] [INFO] [timer.py:207:stop] 0/189, RunningAvgSamplesPerSec=94.47794724866952, CurrSamplesPerSec=109.55167531931687, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      189/     200 | consumed samples:        96768 | consumed tokens:     99090432 | elapsed time per iteration (ms): 7270.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.099244E+00 | moe loss: 8.151767E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 70.425 | TFLOPs: 3.79 |
time (ms) | forward-compute: 6076.60 | backward-compute: 1075.57 | backward-embedding-all-reduce: 0.01 | optimizer: 90.89 | batch-generator: 3963.04
[2023-01-05 03:12:41,553] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:12:41,581] [INFO] [timer.py:207:stop] 0/190, RunningAvgSamplesPerSec=94.48863284268178, CurrSamplesPerSec=96.5302449652857, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      190/     200 | consumed samples:        97280 | consumed tokens:     99614720 | elapsed time per iteration (ms): 8121.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.096121E+00 | moe loss: 8.104789E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 63.041 | TFLOPs: 3.40 |
time (ms) | forward-compute: 7009.59 | backward-compute: 1013.87 | backward-embedding-all-reduce: 0.01 | optimizer: 62.57 | batch-generator: 4707.88
[2023-01-05 03:12:48,195] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:12:48,223] [INFO] [timer.py:207:stop] 0/191, RunningAvgSamplesPerSec=94.55959074097606, CurrSamplesPerSec=110.1043293505769, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      191/     200 | consumed samples:        97792 | consumed tokens:    100139008 | elapsed time per iteration (ms): 6638.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.090617E+00 | moe loss: 8.188729E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.127 | TFLOPs: 4.16 |
time (ms) | forward-compute: 5523.49 | backward-compute: 1018.31 | backward-embedding-all-reduce: 0.01 | optimizer: 67.99 | batch-generator: 3268.85
[2023-01-05 03:12:54,216] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:12:54,243] [INFO] [timer.py:207:stop] 0/192, RunningAvgSamplesPerSec=94.63580375224002, CurrSamplesPerSec=111.64226623828982, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      192/     200 | consumed samples:        98304 | consumed tokens:    100663296 | elapsed time per iteration (ms): 6024.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.080033E+00 | moe loss: 8.108032E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 84.989 | TFLOPs: 4.58 |
time (ms) | forward-compute: 4909.75 | backward-compute: 1016.17 | backward-embedding-all-reduce: 0.01 | optimizer: 61.61 | batch-generator: 3138.03
[2023-01-05 03:13:00,467] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:13:00,494] [INFO] [timer.py:207:stop] 0/193, RunningAvgSamplesPerSec=94.65370419698517, CurrSamplesPerSec=98.18224299236336, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      193/     200 | consumed samples:        98816 | consumed tokens:    101187584 | elapsed time per iteration (ms): 6247.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.073963E+00 | moe loss: 8.066822E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 81.956 | TFLOPs: 4.42 |
time (ms) | forward-compute: 5018.37 | backward-compute: 1133.49 | backward-embedding-all-reduce: 0.01 | optimizer: 61.71 | batch-generator: 2822.07
[2023-01-05 03:13:06,565] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:13:06,593] [INFO] [timer.py:207:stop] 0/194, RunningAvgSamplesPerSec=94.67230530442569, CurrSamplesPerSec=98.36439708463571, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      194/     200 | consumed samples:        99328 | consumed tokens:    101711872 | elapsed time per iteration (ms): 6098.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.066941E+00 | moe loss: 8.137438E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 83.958 | TFLOPs: 4.52 |
time (ms) | forward-compute: 4981.39 | backward-compute: 1020.47 | backward-embedding-all-reduce: 0.01 | optimizer: 62.68 | batch-generator: 3384.36
[2023-01-05 03:13:13,344] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:13:13,372] [INFO] [timer.py:207:stop] 0/195, RunningAvgSamplesPerSec=94.60945061651735, CurrSamplesPerSec=83.9128796759465, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      195/     200 | consumed samples:        99840 | consumed tokens:    102236160 | elapsed time per iteration (ms): 6779.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.074800E+00 | moe loss: 8.098315E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 75.521 | TFLOPs: 4.07 |
time (ms) | forward-compute: 5672.76 | backward-compute: 1011.21 | backward-embedding-all-reduce: 0.01 | optimizer: 61.48 | batch-generator: 3559.08
[2023-01-05 03:13:19,695] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:13:19,723] [INFO] [timer.py:207:stop] 0/196, RunningAvgSamplesPerSec=94.6662680670965, CurrSamplesPerSec=107.07710767247349, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      196/     200 | consumed samples:       100352 | consumed tokens:    102760448 | elapsed time per iteration (ms): 6356.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.061320E+00 | moe loss: 8.159311E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 80.548 | TFLOPs: 4.34 |
time (ms) | forward-compute: 5245.75 | backward-compute: 1010.30 | backward-embedding-all-reduce: 0.01 | optimizer: 61.17 | batch-generator: 3395.58
[2023-01-05 03:13:25,863] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:13:25,891] [INFO] [timer.py:207:stop] 0/197, RunningAvgSamplesPerSec=94.68564758291681, CurrSamplesPerSec=98.6015619891964, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      197/     200 | consumed samples:       100864 | consumed tokens:    103284736 | elapsed time per iteration (ms): 6161.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.080128E+00 | moe loss: 8.156885E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 83.096 | TFLOPs: 4.48 |
time (ms) | forward-compute: 4976.46 | backward-compute: 1093.41 | backward-embedding-all-reduce: 0.01 | optimizer: 62.03 | batch-generator: 3351.68
[2023-01-05 03:13:34,187] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:13:34,214] [INFO] [timer.py:207:stop] 0/198, RunningAvgSamplesPerSec=94.76540659220088, CurrSamplesPerSec=113.3909320356771, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      198/     200 | consumed samples:       101376 | consumed tokens:    103809024 | elapsed time per iteration (ms): 8324.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.058865E+00 | moe loss: 8.146071E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 61.508 | TFLOPs: 3.31 |
time (ms) | forward-compute: 7212.86 | backward-compute: 1016.28 | backward-embedding-all-reduce: 0.01 | optimizer: 61.47 | batch-generator: 5304.49
[2023-01-05 03:13:40,399] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:13:40,427] [INFO] [timer.py:207:stop] 0/199, RunningAvgSamplesPerSec=94.87110689401028, CurrSamplesPerSec=121.41423289818744, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      199/     200 | consumed samples:       101888 | consumed tokens:    104333312 | elapsed time per iteration (ms): 6212.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.063523E+00 | moe loss: 8.166561E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 82.418 | TFLOPs: 4.44 |
time (ms) | forward-compute: 5083.42 | backward-compute: 1033.54 | backward-embedding-all-reduce: 0.01 | optimizer: 61.40 | batch-generator: 3191.58
[2023-01-05 03:13:46,253] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-01-05 03:13:46,277] [INFO] [timer.py:207:stop] 0/200, RunningAvgSamplesPerSec=94.71023536595185, CurrSamplesPerSec=70.9945182227373, MemAllocated=6.95GB, MaxMemAllocated=10.8GB
 iteration      200/     200 | consumed samples:       102400 | consumed tokens:    104857600 | elapsed time per iteration (ms): 5849.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.059884E+00 | moe loss: 8.180275E-02 | loss scale: 2048.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 87.526 | TFLOPs: 4.72 |
time (ms) | forward-compute: 4740.56 | backward-compute: 1012.87 | backward-embedding-all-reduce: 0.01 | optimizer: 63.59 | batch-generator: 2756.74
[after training is done] datetime: 2023-01-05 03:13:46 
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.6328980922698975 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5006442070007324 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4952874183654785 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.525980234146118 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4664063453674316 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.52016544342041 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4972455501556396 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5045924186706543 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.517716407775879 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5320370197296143 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.502152919769287 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4998326301574707 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.524651288986206 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.446092367172241 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5021026134490967 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5306951999664307 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5207359790802 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4699583053588867 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5709989070892334 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5675289630889893 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4994404315948486 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.6357603073120117 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4948925971984863 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.552570104598999 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.44382381439209 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.496757984161377 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.524744749069214 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5281195640563965 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5157546997070312 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.48835825920105 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5059518814086914 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.490687370300293 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.6133265495300293 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.512455701828003 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.462735414505005 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4952006340026855 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.53499436378479 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.506807327270508 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.540681838989258 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.469599723815918 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.551891803741455 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5256102085113525 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.445094347000122 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5072295665740967 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.515310049057007 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4476375579833984 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5676066875457764 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.6188342571258545 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.62498140335083 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.6453418731689453 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4729316234588623 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5757856369018555 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4752376079559326 seconds
Loading extension module utils...
Time to load utils op: 2.6511154174804688 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5893588066101074 seconds
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.507126808166504 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4989333152770996 seconds
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5214953422546387 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5361216068267822 seconds
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.5972402095794678 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.4996137619018555 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/torch']
torch version .................... 1.9.0a0+gitd69c22d
torch cuda version ............... 11.0
torch hip version ................ None
nvcc version ..................... 11.0
deepspeed install path ........... ['/GPUFS/thu_wgchen_2/laekov/.local/lib/python3.6/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.9, cuda 11.0
**** Git info for Megatron: git_hash=c240204 git_branch=moe ****
No existing process group found, creating a new group named: ep_size_64
Using /GPUFS/thu_wgchen_2/laekov/.cache/torch_extensions as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 2.615443468093872 seconds
